# 体系结构基础 & 并行性

## *计算机分类 & PC发展史*

<img src="主流计算类别及其系统特性汇总.png">

### 分类

* 个人移动设备 PMD

  个人移动设备 Personal Mobile Device PMD 是指一类带有带有多媒体用户界面的无线设备，比如智能手机、平板电脑等

  PMD上的应用程序大部分是基于Web应用、面向媒体的。能耗与尺寸要求决定了要采用闪存而不是磁盘来作为存储方式

* 桌面计算机/PC

* 服务器 Server

  服务器是专门用于提供服务、存储数据、执行计算和处理网络请求的计算机。它们通常用于支持网络应用程序、托管网站、存储大量数据、进行数据分析、提供网络服务等任务

  服务器已经取代了传统的大型机，称为大规模企业计算的中枢

  因为服务器一般都是用来提供商业服务的，因此服务器的要求集中于高可用性、可扩展性和很高的吞吐能力

* 仓库级计算机 Warehouse-Scale Computer/集群 Cluster

  互联网的爆炸式增长，特别是一些SaaS应用，比如社交网络、互联网商业推动了一类被称为集群 cluster的大型计算机的发展

  集群是一群使用局域网连接到一起的PC或Server，其中最大规模的集群被称为仓库级计算机 WSC

* 嵌入式设备 & IoT

  嵌入式设备存在于各种工业产品中，价格和能耗是使用嵌入式设备时最关心的要素

  嵌入式设备在硬件和软件复杂性方面有很大的限制，我们以**能否运行第三方软件作为区分是否为嵌入式计算机的标准**

  随着嵌入式设备的蓬勃发展与智能互联的需求，物联网 Internet of Things IoT 也迅速崛起。IoT指的是嵌入式计算机连接到互联网，通常是无线连接。当加装传感器和执行器时，物联网设备收集有用的数据并与物理世界互动，从而产生各种“智能”应用，例如智能手表、智能恒温器、智能音响、智能汽车、智能家居、智能电网和智能城市等

### 补充：PC机的发展简史

个人计算机（PC）的发展历史可以追溯到上世纪70年代以及之前，以下是个人计算机发展历史的简要概述：

1. 早期计算器和机械计算机：在20世纪初期，机械计算机和早期电子计算器开始出现，用于执行基本的数学运算和数据处理任务。这些设备通常是巨大的、昂贵的，且使用范围有限
2. 第一台个人计算机：**Altair 8800**：在1975年，Altair 8800被认为是第一台真正的个人计算机。它是一台基于微处理器的计算机，由一家名为MITS的公司制造。Altair 8800需要通过切换开关来编程，而没有图形界面，但它启发了很多人
3. **Apple I和Apple II**：在1976年，史蒂夫·乔布斯和史蒂夫·沃兹尼亚克成立了苹果公司，并推出了Apple I计算机。随后，他们发布了Apple II，这是一台具有图形界面和键盘的计算机，大受欢迎，被认为是个人计算机的重要里程碑
4. **IBM PC和MS-DOS**：1981年，IBM发布了IBM PC（个人计算机）并使用了微软的操作系统MS-DOS。IBM PC的成功导致了PC市场的标准化，并为后来的兼容机打下了基础
5. **IBM兼容机的崛起**：1980年代末和1990年代初，许多公司开始生产IBM兼容机，它们使用与IBM PC兼容的硬件和操作系统，使得PC市场多样化和竞争激烈
6. Windows操作系统：微软推出了Windows操作系统，它具有图形用户界面（GUI），并成为世界上最流行的PC操作系统之一。Windows 3.0和Windows 95版本的发布进一步推动了个人计算机的普及
7. 互联网的普及：1990年代中期，互联网的广泛普及促使许多人购买个人计算机，并开始使用电子邮件、浏览网页等在线服务
8. 移动计算：21世纪初，便携式计算机如笔记本电脑和智能手机开始流行，改变了计算机的使用方式。智能手机特别改变了人们与计算的互动方式

### **IBM PC兼容机 IBM PC compatible**

1980年代初期，市场上已经存在了大量不同标准的个人电脑，比如Apple机、TRS-80、PC-9801等。1981年，IBM推出了IBM PC。然后在1982年IBM公开了IBM PC上除BIOS之外的全部技术资料，从而在IBM PC机的基础上形成了**PC机的开放标准**，使不同厂商的标准部件可以互换。开放标准聚拢了大量板卡生产商和整机生产商，大大促进了PC机的产业化发展速度

到1990年代初，个人电脑市场上仅剩下IBM PC兼容机和苹果的麦金塔电脑  Macintosh 两个主要系列，并且IBM兼容机数量占据了绝对主导地位。随着IBM兼容机的发展以及计算能力的大大提高，它甚至蚕食了小型机的市场份额

在IBM PC兼容机逐步成为事实上的PC标准过程中，为微软、Intel，以及大量兼容机部件商、兼容机厂商提供了市场机会，甚至IBM自己在PC市场上的份额都不是第一位。随着技术的发展，IBM容机经历了IBM PC XT/AT (8086)、80286、80386、80486、奔腾 Pentium等阶段，很多新的内容加入进来，因此到了1990年代/BM对个人电脑架构的影响力逐渐下降，计算机技术人员更倾向称之为Wintel标准架构

Wintel标准架构在字面上指由Microsoft Windows操作系统与Intel CPU所组成的个人计算机。但该词实际上是指Microsoft与Intel的商业联盟，该联盟意图并成功地取代了BM公司在个人计算机市场上的主导地位。所以也称为Winte联盟。这是广受媒体使用的一种通俗用法

在奔腾电脑之后，由于大量新的PC的技术标准应用，PC的技术标准开始由IEEE等组织而不是某个厂家来确定，IBM PC兼容机的说法逐渐被**标准PC**以及后续的**ACPI PC**所取代

### Macintosh PC的三次ISA迁移

虽然早期的Macintosh PC都比较昂贵，市场由更便宜的Commodore 64、IBM PC及IBM PC兼容机主导。但后来，麦金塔操作系统被教育和出版领域广泛应用，使得苹果公司成为未来十年的世界第二大个人电脑制造商。20世纪90年代初，苹果公司推出了麦金塔LC I和Color Classic，这两款产品在价格上与当时的Wintel机型竞争

但是由于早期Macintosh PC搭载的都是摩托罗拉的CPU，而摩托罗拉在与Intel的竞争中逐渐被打败，其各类产品无法在benchmarking中击败intel的产品，Macintosh PC的市场逐渐萎缩。直到乔布斯重回苹果之后精简产品线Macintosh PC才重焕生机

1. 1994年，Macintosh 从摩托罗拉68000系处理器迁移至PowerPC处理器
2. 2005年至2006年，Macintosh 从PowerPC处理器迁移至Intel平台处理器，并且改名Mac
3. 2020年开始，Mac从intel平台处理器迁移至苹果处理器 Apple silicon（基于ARM架构）

### ACPI PC

ACPI Advanced Configuration and Power Interface 是一种电脑标准，旨在管理计算机硬件和软件之间的电源管理、配置和通信。ACPI标准最初由英特尔、微软和东芝等公司于1996年共同开发，旨在取代之前的计算机电源管理（APM）标准，以提供更高级、更灵活的电源管理和配置功能。以下是ACPI标准的主要特点和组成部分：

1. **电源管理**：ACPI允许操作系统和硬件协同管理电源，以实现更有效的电源管理和降低能耗。它允许计算机在不同的电源模式之间切换，例如休眠模式、待机模式和正常运行模式，以节省电能
2. **硬件配置**：ACPI标准提供了一种描述计算机硬件配置的方式，包括中央处理器、内存、外围设备等。这使操作系统能够更好地理解和管理计算机的硬件资源
3. **高级电源管理**：ACPI允许操作系统动态控制硬件组件的电源状态，以实现更精确的电源管理。这包括在不需要时关闭设备、降低处理器频率、调整显示器亮度等
4. **设备热插拔**：ACPI支持设备的热插拔，这意味着用户可以在计算机运行时插入或移除设备，而无需重启计算机
5. **事件通知**：ACPI标准引入了一种事件通知机制，可以通过特定的事件触发操作系统或应用程序的响应。这些事件可以是硬件状态更改、电源事件、按键触发事件等
6. **操作系统支持**：ACPI标准通常需要操作系统提供ACPI驱动程序以实现功能。大多数现代操作系统，如Windows、Linux和macOS，都提供了ACPI支持
7. **ACPI表**：计算机的ACPI信息通常存储在特殊的数据结构中，称为ACPI表。这些表包含了计算机的硬件配置、电源管理信息和其他相关信息，操作系统通过解析这些表来了解和控制计算机的状态

## *计算机体系结构*

### 什么是Computer Architecture？

**Computer Architecture 计算机体系结构**是指计算机系统中各个组件的结构、功能和互联方式，以及它们如何共同协作以实现数据处理和运算任务的方法和规范。其中属于计算机体系结构的范围是硬件和软件之间的接口 Interface between SW and HW 以及微结构的设计

下图是一张计算机整个体系的抽象模型

<img src="计算机体系模型.drawio.png" width="50%">

计算机体系结构包括以下几个关键方面：

* **指令集架构 Instruction Set Architecture，ISA**：指令集架构定义了处理器能够理解和执行的指令集合，包括操作码、寄存器、地址模式等。ISA 是硬件和软件之间的重要接口，不同的计算机体系结构有不同的指令集架构

* **微结构设计**
  * **内存层次结构**：计算机的内存层次结构包括多级缓存和主存，这些层次的设计影响了数据访问速度和性能。高速缓存被用来存储最常用的数据，以减少处理器访问主存的延迟
  * **处理器结构**：处理器结构涉及到处理器的核心数、流水线设计、并行处理能力等。现代处理器常常具备多核心、超线程等功能，以实现更高的并行处理能力
  * **总线结构**：总线是连接计算机内部各个组件的通信通道，包括数据总线、地址总线和控制总线。总线的设计影响了各个组件之间数据传输的速度和效率
  * **输入输出（I/O）系统**：I/O 系统包括与计算机外部设备进行通信的方式和接口，如网络接口、磁盘控制器等
  * **中断和异常处理机制**：这些机制允许计算机在运行过程中响应外部事件，如硬件错误、输入输出请求等，以保持系统的稳定性和可靠性

计算机体系结构的设计旨在平衡性能、功耗、成本和可扩展性等方面的需求。不同的计算机体系结构在不同的应用场景下可能表现出更适合的特点，例如高性能计算、嵌入式系统、移动设备等

### Computer Architecture的8个伟大思想

1. 面向摩尔定律的设计

2. 使用抽象简化设计 abstraction

3. 加速经常性事件 make the common case fast

   通过试验来确定系统中哪些是经常性事件，哪些是罕见事件。经常性事件往往比罕见事件简单，因此通过优化经常性事件可以有效地改善系统性能

4. 通过并行提高性能 parallellism

5. 通过流水线提高性能 pipelining

6. 通过预测提高性能 prediction

7. 存储层次

8. 通过冗余提高可靠性

### 架构师面对的最重要的功能需求

## *发展趋势*

### 性能趋势：带宽胜过延迟

带宽的增长速度至少是延迟改进速度的平方

### 成本趋势

## *计算机性能*

### 性能的定义

不同的场景下性能的定义是不同的

* 对于PC等个人设备来说更关心响应时间 response time。响应时间也叫执行时间 execution time，它是指从开始一个任务到该任务完成的时间，其中更包括了硬盘访问、内存访问、IO活动、OS的开销和CPU执行时间
* 对于Data Center、Server或HPC而言则吞吐量 throughput 或带宽 bandwidth 是更重要的，即在给定时间内完成的任务数

为了减少“增加”和“减少”带来的误解，当我们想表达增加性能 increase performance 和减少执行时间 decrease execution time 的时候，实际会说改善性能 improve performance 和改善执行实现 improve execution time

### 存储器的延迟 & 带宽

* 延迟 latency

  * 延迟为处理器发出内存项请求到内存项实际到达之间的间隔
  * 延迟的成因很多，比如从内存到缓存的传输、缓存到寄存器的传输，或者将它们总结为内存和处理器之间的延迟
  * CPU通过构建合理高效的层次化存储器来降低延迟，而GPU则通过快速的硬件级线程切换来规避延迟

* 带宽 bandwidth 或吞吐量 throughput

  内存带宽（字节/秒） = 内存频率（Hz） × 内存通道数量 × 数据传输宽度（总线宽度）

  * 内存频率是内存模块的工作频率，通常以赫兹（Hz）表示
  * 内存通道数量是系统中同时可访问的内存通道数
  * 数据传输宽度（总线宽度）是每个内存访问的宽度，通常以字节为单位

### CPU相关的时间度量

计算机中对时间最直接的定义是挂钟时间 wall clock time/响应时间 response time/运行时间 elapsed time等。这些术语均表示完成某项任务所需的总时间，包括了磁盘访问、内存访问、IO活动和OS开销等一切时间

CPU执行时间 CPU execution time，简称为**CPU时间**，它只是在CPU上花费的时间，不包括IO等待等其他程序运行的时间。CPU时间还可以进一步划分为

* 用户CPU时间 user CPU time：CPU时间中属于运行用户程序的时间
* 系统CPU时间 system CPU time：CPU时间中属于OS为用户执行相关任务所花去的时间（即陷入内核的时间）

用术语**系统性能 system performance** 表示空载系统的响应时间，并于术语CPU性能 CPU performance 来表示用户CPU时间

### 时钟周期

时钟周期 clock cycle，也称为滴答数 ticks、时钟滴答数 clock ticks、时钟数、周期数

时钟周期是指计算机内部的主时钟发生一次脉冲或周期性振荡的时间间隔。时钟周期用于同步计算机的各个组件，确保它们在正确的时间执行操作

### CPU周期 & CPU性能

CPU性能度量的基本指标应该是**CPU执行时间**，也称为CPU周期或者机器周期
$$
程序的CPU执行时间=程序的CPU时钟周期数\times 时钟周期长度=\frac{程序的CPU时钟周期数}{时钟频率}
$$
从上式中可以看出，只要硬件设计者可以减少程序执行所需的CPU时钟周期数或缩短时钟周期长度就可以改善性能。不过这两者往往是此消彼长的，设计者需要做出抉择

### 指令周期 & 指令性能

CPI取决于指令周期/程序的CPU时钟周期数是一条指令所需要的CPU执行时间和程序指令数 Instruction Count或指令路径长度 Instruciton Path Length
$$
CPI=\frac{程序的CPU时钟周期数}{IC}
$$
**指令平均时钟周期数 clock cycle per instruction CPI 表示执行每条指令所需的时钟周期平均数**。CPI提供了一种相同指令系统 ISA在不同的实现下（不同的硬件实现方式或者不同的处理器设计来执行这些指令）比较性能的方法，因为在相同ISA中程序的指令数是相同的，所以可以比较CPU的性能

也可以使用每时钟周期指令数 IPC instruction per clock cycle

### CPU性能公式

结合上面两个公式，可以得到基本的CPU性能公式
$$
CPU时间=IC\times CPI\times 时钟周期=\frac{IC\times CPI}{时钟频率}
$$
其中时钟周期取决于硬件技术与组成；CPl取决于组成与指令集体系结构；IC取决于指令集体系结构和编译器技术

指令数和CPI是很难测量的。CPI与计算机的各种设计细节密切相关。CPI对于不同应用程序是不同的，对于相同ISA的不同实现方式也是不同的

## *Roofline Model*

Roofline模型是在 *Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM 52, 4 (April 2009), 65–76. https://doi.org/10.1145/1498765.1498785* 论文中提出的关于系统性能的可视化模型，它将浮点性能、算术强度和内存性能结合到一张二维图中

### 算术强度

$$
算术强度=\frac{浮点操作数/秒}{浮点操作数/字节}=字节/秒
$$

存储器每访问一字节所包含的浮点运算比例称为算术强度 arithmetic intensity。下面是几种Berkeley设计模式的算术强度

<img src="Berkeley设计模式的算术强度.png" width="50%">

算术强度指标通常用于评估程序对计算资源（如处理器）与内存资源的依赖程度。算术强度高的程序通常被认为是计算密集型，因为它们执行大量的计算而相对较少地访问内存

### Operational Intensity

Operational Intensity是在 中给出的概念。它测量的不是Processor和Cache之间的流量，而是LLC和memory之间的bandwidth

### Roofline模型解析

$$
FLOPS=\frac{\#FLOP}{time(Sec)}=\frac{\#FLOP}{Byte}\cdot\frac{Byte}{time(Sec)}=Arithmetic\ Intensity\times Bandwidth=AI\times BW\\\xrightarrow{Log}\log{(FLOPs)}=\log{(AI\times BW)}=\log{(AI)}+\log{(BW)}\sim y=x+b
$$

<img src="Roofline模型.png" width="50%">

计算密集型（高OI）程序主要受限于浮点计算峰值性能 peak GFLOP/s，而内存密集型程序则主要受限于DRAM的峰值理论带宽 peak Gigabytes/s。二者的交叉点 ridge point 称为 machine balance，表征了硬件架构的特点

优化过程是这样的：理论计算得到峰值DRAM bandwidth，然后测量实际的GLOPs，比较二者？

## *Benchmarking*

### 桌面基准测试

* CPU密集型基准测试
* 图形密集型基准测试

### 服务器基准测试

### 并行基准测试

* Linpack：Linpack 线性代数包是一个用于解决线性代数问题的benchmarking，主要用于测量计算机系统的浮点运算性能。它通常用于评估超级计算机和高性能计算群集的性能。Linpack通常使用在TOP500超级计算机排名中，以确定超级计算机的性能排名
* SPECrate
* SPLASH & SPLASH2：Stanford Parallel Applications for Shared Memory 是20世纪90年代斯坦福大学的研究成果，目的是提供类似于SPEC CPU的并行benchmarking。这两个基准测试可用于评估计算机系统的内存子系统性能，特别是对于并行应用程序来说，内存性能非常关键
* NAS (NASA Advanced Supercomputing)：NAS Parallel Benchmarks是一组由NASA开发的基准测试，旨在评估超级计算机和并行计算系统的性能。这些基准测试包括一系列科学应用程序，涵盖了不同的计算和通信模式。允许用C或Fortran重写
* PARSEC (Princeton Application Repository for Shared Memory Computer) 由Pthread和OpenMP的多线程程序组成，它们主要专注于新兴的计算领域，由9个应用程序和3个核心程序组成
* YCSB (Yahoo Cloud Serving Benchmark) 是云端的并行benchmarking，目的是比较云数据服务的性能，它通过使用Cassandra和HBase作为具有代表性的例子，提供了一个易于让用户评测新数据服务的框架

## *单核向多核的跨越*

### 功耗墙 Power wall

$$
功耗\propto\frac{1}{2}\times负载电压\times电压^2\times开关频率
$$

### 多核时代

为了避免处理器 Processor 和微处理器 Microprocessor的混淆，又将处理器称之为一个核 core。例：在一个微处理器中有4个处理器/核心，那么将这个微处理器称为4核芯片/4核微处理器（其中一个处理器是一个数据通路/运算器+一个控制器）

多核时代的到来也宣告着处理器性能的提高从单纯依靠指令级并行 ILP 转向数据级并行 DLP 、线程级并行 TLP 和请求级并行 RLP。原来的ILP是依靠编译器和硬件自动进行的，但新时代的DLP、TLP和RLP则更多的是需要程序员显式调整应用程序的结构，对程序员的开发能力提出了更多的要求

## *并行度 & 并行体系结构分类*

并行 parallelism 就是在程序的执行过程中寻找独立的操作，这些独立的操作往往其执行逻辑相同，只是用于不同的数据项

多种级别的并行度已经成为各种不同的计算机设计的推动力量，其中能耗和成本则是主要的约束条件

### 应用并行

* 数据级并行 DLP Data Level Parallelism
  * 同一操作被并行地应用于许多数据项，一般都是在SIMD中使用
  * 数据并行性在科学计算中十分常见，并行性往往源于一个数据集（向量、矩阵、图等）被分散到许多处理器上，没个处理器都可以处理其数据的一部分

* 任务级并行 TLP Task Level Parallelism：创建了一些能够单独处理但大量采用并行方式执行的工作任务/子程序

### 硬件并行

* 指令级并行 ILP Instruction Level Parallelism
  * 主要的ILP技术有流水线 pipelining、多发射 multiple-issue、分支预测和推理执行 branch prediction and speculative execution、无序 out-of-order execution、预取 prefetching
  * ILP并不在用户的控制范围内，而是由编译器和CPU共同决定的

* 3种SIMD形式将单条指令并行引用于一个数据集，以开发数据级并行 DLP
* 线程级并行 TLP Thread Level Parallelism：软件级TLP和硬件级TLP，可以由程序员来制定
* 请求级并行 RLP Request Level Parallelism：有大量工作可以自然地并行开发，几乎不需要通信或同步

## *创建并形处理程序的挑战*

提高并行性的困难并不在于硬件上，而是在于只有很少的应用程序在被重写后能够在多处理器上取得更高的速度，开发多处理器的并行处理程序是非常困难的

### Amdahl定律的加速比定义

加速比 speedup 是一种用于衡量并行计算系统性能改善程度的度量。它通常用于比较串行计算和并行计算之间的性能差异。加速比 speedup 取决于两个因素

1. 原计算机计算时间中可升级部分所占的比例，这个比例称为**升级比例** $Frachtion_{enhanced}<1$。比如60秒中有20秒是可以改善的，则升级比例为 $1/3$ 
2. 可升级部分的改善幅度称为**升级加速比** $Speed_{enhanced}>1$

$$
改进后的执行时间=\frac{受改进影响的执行时间}{改进量}+不受影响的执行时间=原执行时间\times\left(\left(1-升级比例\right)+\frac{升级比例}{升级加速比}\right)
$$

Amdahl定律定义了使用某一特定功能所获的的加速比 speedup
$$
Amdahl's\ law:\ 加速比=\frac{整个任务在采用该升级时的性能}{整个任务在未采用该升级时的性能}=\frac{1}{\left(1-升级比例\right)+\frac{升级比例}{升级加速比}}
$$

### Amdahl定律在并行处理器上的应用

Amdahl定律可以用作评估并行计算性能的数学模型，它对于理解如何利用多核CPU的性能提供了重要的洞察。Amdahl定律的核心观点是，**对于一个并行计算任务，其性能提升受到了任务中无法并行化部分的限制**。所谓无法并行化的部分就是指部分代码必须依赖故有顺序执行

我们可以将并行任务套用到上面的Amdahl定律中，此时

* 升级比例 $Frachtion_{enhanced}=f$ 是程序中并行执行的的比例
* 升级加速比 $Speed_{enhanced}=p$ 是CPU的核心数，因为每多一个核心就提高一倍的改善幅度
* 原执行时间就是纯顺序执行的时间 T

根据Amdahl定律可以写出加速比为

$$
T(p)=\left(1-f\right)\cdot T+\frac{f\cdot T}{p}
$$

从上式可以得到下面的关系，即在固定的处理器核心数为了达到某个加速比程序中需要并行化的部分的比例。从而引出了一个构建并序的难点，即在一定的核心数下要向得到至少等比例的加速比需要极高的程序并行度

<img src="Amdahl定律限制下处理器数量的限制.png" width="50%">

### 大规模问题

想要在多处理器上获得良好的加速比，保持问题规模不变的情况相比于问题规模增长的情况要困难得多

* 强比例缩放 strong scaling 意味着在保持问题规模不变的情况下在多处理器上获得的加速比
* 弱比例缩放 weak scaling 意味着在问题规模与处理器数量成比例增加的情况下在多处理器上获得的加速比

假设问题规模M是主存中的工作集，处理器数量是P，那么每个处理器所占用的内存对于强比例缩放大约为 $M/P$，对于弱比例缩放大约为M

### 负载均衡 & 高度并行

调度并将工作划分为可并行的部分，以及在处理器之间同步时间、维持通信、均衡负载都是困难的

若各个处理器并未分配到完全相同的工作量，则会产生一部分的闲置，从而造成负载不均衡 load unbalance

处理器之间的通信是效率损失的一个重要来源，一个不需要通信就可以解决的问题无疑是具有吸引力的。这类问题实际上由许多完全独立的计算组成，被称为高度并行 embarrassingly parallel 或便捷并行 conveniently parallel

# 流水线数据依赖

## *数据依赖*

### 数据依赖的种类

因无法提供指令所需数据而导致指令不能在预期的时钟周期内执行的情况称为数据依赖。计算机流水线中，数据依赖源于一条指令依赖于一条尚在流水线中的指令

* Dataflow/Flow dependence 是 true dependence，因为是语义上的依赖，也就是说第二步的r3用到的就是第一步的r3

  <img src="RAW.png" width="70%">

  比如说下面的指令就对应RAW，若不进行修正，它会严重干扰流水线的运行，第二条指令需要等到第一条指令WB，会白白浪费3个时钟周期

  ```assembly
  add x1, x2, x3
  sub x4, x1, x5
  ```

* Anti dependence 和 Output dependence 是 false dependence，因为没有语义上的依赖，只是因为寄存器标号不是无限多造成的name dependency。为什么说没有语义上的依赖？举个列子，先做 `x+y=z`，再做 `m+n=x`，这种对于x的WAR实际上第二步并不关心第一步x是多少，反正都是要写覆盖的，只要第二步不比第一步先执行就行。In-Order和OoO通过reorder和保留站提供的寄存器重命名就可以解决它

  * Anti dependence 反依赖

    <img src="WAR.png" width="70%">

    第二步不一定要写到r1里，如果可以的话写到r6、r7都行，这是可以从结构上避免的

  * Output dependence 输出依赖

    <img src="WAW.png" width="70%">

    如果交换第一步和第三步的顺序，则会影响到第二步的结果，所以这两步无法在流水线上并行执行。但是如果将其中任意一个r3换成另一个寄存器就可以消除output dependence了

### 如何消除数据依赖

Anti和Output dependence 是比较容易消除的，只要是按照指令顺序，后面的指令在最后阶段写入就可以完全消除这两种数据依赖

但是Flow dependence是不能这么简单就消除的。消除Flow dependence 主要有6种方法

* Detect and wait until value is available in register file，最保守的方法，让流水线停顿
* Detect and forward/bypass data to dependent instruction 前递
* Detect and eliminate the dependence at the software level 利用编译器重排指令来消除Flow dependency，静态调度和动态调度
* Detect and move it out of the way for independent instructions
* Predict the needed value(s), execute “speculatively”, and verify 预测
* GPU的fine-grained multithreading，线程快速切换

### 静态调度和动态调度

* Static scheduling 静态调度：software based instruction scheduling，也称为编译时，就是任何在程序真正被执行之前的行为
* Hardware scheduling 动态调度：hardware based instruction scheduling，也称为运行时，就是任何在程序真正被执行时的行为

一般来说任何只靠一方的努力都无法达到相对最佳的效果，实际中都是二者的结合。有一个很好的例子是MIPS的设计

MIPS是一种有大语义鸿沟的ISA，所以编译器要做很多，包括防止Interlocking。一开始MIPS没有在微结构层面设置任何的anti-interlocking。这也是MIPS, Microprocessor without Interlocked Pipeline Stages 名字的由来。不过后来发现这是一个极其糟糕的设计，因为compiler和软件要做大量的静态调度，所以后来MIPS实际上加入了静态期防止interlocking的内容

另一方面为了进行静态调度，编译器要知道很多信息，但是不幸的是，任何在运行时才能知道的信息编译器都是无法知晓的，比如说依赖于长度的操作的延迟（比方说如果乘法有一个操作数在运行时确定为0，那么就能极大的缩短操作时间）、**特别是涉及到缓存或内存的访问**、分支方向等

一种方法是对程序进行动态或静态的做实验来进行性能分析 profiling

## *检测数据依赖*

### 计分牌 score-boarding

每一个寄存器都多带一个有效位 Valid bit，每一个要写这个寄存器的指令都要重置这个有效位。另外的指令在译码阶段要检查它的所有dest和src寄存器是否都有效，如果是则说明没有依赖不需要停顿流水线；否则要停顿流水线

好处是这种实现方法比较简单，坏处是它无法分辨三种数据依赖

### 组合逻辑检查

这种方法可以只识别出Flow dependence，但是需要硬件支持，而且随着流水线的加深硬件会越来越复杂

## *前递*

当检测出数据依赖后，最终的解决方法是令流水线停顿 stall pipeline。但是流水线停顿的损耗很大，所以如果可以避免停顿就要必要它。前递 forwarding（将结果从前面一条指令直接传递给后面一条指令） 或旁路 bypassing（将结果绕过寄存器堆，直接传递给需要它的单元） 就是避免不必要的流水线停顿的一种方法

前递的思路是提前从内部缓冲中取到数据，而不是等到数据到达程序员可见的寄存器或存储器之后再去取

<img src="前递.png" width="80%">

`and $t0, $s0, $s1` 要用到 `$s0` 寄存器的值至少要等到 `add $s0, $s2, $s3` 的第5步 WB之后才能取到

这种情况就可以用forwarding来修正，上图中 `add` 第3步EX之后连接到 `and` 的ID之后的通路就是这种效果

前递实际上和dafaflow的一旦数据准备好，就立刻trigger需要它的指令的思想是一样的

### 前递失效

<img src="bypass的load失效.png" width="80%">

load指令要直到写入内存后才可以bypass给其他指令。此时必须要插入bubble，stall pipeline才能解决数据冒险

不过注意，上图中因为三条指令都需要用到 `$s0`，所以无法通过静态调度的方式调换指令顺序来实现bypass的目标·

## *插入bubble*

### 微结构上实现stall

实现stall需要硬件支持

* IF/ID流水线寄存器需要使能EN控制，来保存stall之前的指令
* EX流水线寄存器需要synchronous clear/reset CLR，或者在每个流水线寄存器添加INV invalid位，用来插入bubble/nop

stall的步骤是

* 不使能PC和IF/ID流水线寄存器的锁存功能，让之后被stalled的指令都留在原地
* 在开始stall的指令后插入nops 指令

### 例子

载入-使用型数据冒险 load-use data harzard：一种特定形式的数据冒险，指当载入指令要取的数据还没取回时，其他指令就需要该数据的情况

```assembly
ld x1, 0(x2)
sub x4, x1, x5
```

上面的汇编代码就是典型的载入-使用型数据冒险，`ld` 要到第四阶段才能memory中把数据取到。这种情况下没有任何的前递方法可以帮助提前取到数据

<img src="插入bubble.png" width="60%">

解决方法是使用硬件检测停顿，或由软件对代码重新排序以尽量避免载入-使用型流水线停顿。这种情况称为**流水线停顿 pipeline stall，通常俗称为气泡 bubble**。具体做法是插入一个从未无意义的控制信号 `nop`，令CPU空转一个时钟周期

对于当今普遍有20-30级pipeline的处理器来说stall的操作带来的损耗是非常大的

# Precise Exception

## *多周期执行*

### 不同的FU

<img src="按序提交.drawio.png" width="70%">

* Decode, D 解码：取指令后放入register，在ROB中预留一个位置，如果指令可以执行，就**dispatch指令，也就是送到functional unit**。如果因为任何原因指令无法执行，都不可以dispatch
* Execute, E 执行：指令在执行的时候可以乱序。这个阶段在不同的functional unit中的执行时间是不同的
* Completion, R：将结果写入reorder buffer
* Ertirement/Commit, W 提交
  * Retire/graduate/commit：完成执行后更新architectural state的步骤
  * 检查最早的指令是否发生了异常，如果没有就将结果写入architectural register或者内存；如果发生异常，就要冲刷pipeline并跳转到exception handler

### 交付的时间点

<img src="乱序提交违反冯诺依曼语义.drawio.png" width="70%">

上图中后执行的指令在先执行的指令写入结果之前就写入了，没有遵守Von Neumann的顺序执行语意。如果不保留Von Neumann规定的顺序语义的话会引发很多问题

比如说在语言层面如果我们catch了一个exception，如果这个exception的结果是已经写入了寄存器或内存的话会很麻烦

### 异常和中断

* 异常 exception 是内部错误 internal：比如除0错误、溢出、页错误等
* 中断 interrupt 是外部错误 external：比如没有内存了，这是系统的问题，程序员无力解决

处理异常和中断都需要

* 停止现有指令
* 将architectural state保存下来
* 跳转到handler来处理异常或中断
* Optional：回到程序停止执行的地方

### Precise Exception

Precise exception的本质就是说要遵循Von Neumann语义

保留Von Neumann语意有助于debugging，如果没有Von Neumann这层语义上的约定，程序员需要自己重构程序，努力去找出程序到底是如何执行的

traps 软件层面的opcode，比如说浮点运算可能在硬件层面消耗过大，可以通过在软件层面模拟

Dataflow model就很难遵循precise exception，dataflow中还是有异常，但根本不知道有没有执行了

支持Precise Exception的方法有多种

* Reorder Buffer
* History Buffer
* Future Register File
* Checkpointing

## *Reorder Buffer*

### ROB 结构

<img src="ROB结构.drawio.png" width="70%">

ROB在硬件上被设计为一个先进先出的环形队列，它里面存储的指令是已经译码了但还没有被retired/commited的指令

* 仍然乱序执行指令，但在写入体系寄存器之前把结果放到 Reorder Buffer, ROB 重排序缓冲区 里面
* 当指令被解码的时候，会在ROB中为这条指令按序预留一个位置
* 当指令完成的时候，它的结果会写入ROB entry
* 当ROB中最早的指令无异常完成的时候，它的结果才会被写入寄存器堆或内存中

### ROB Entry

<img src="ROB_entry.drawio.png" width="80%">

* Valid bit有效位：为一条尚未retire的指令保留了ROB entry或者已经被写入entry了
* DesRegID：要写入的寄存器ID
* DesRegVal：要写入寄存器中的数据
* StoreAddr & StoreData：写内存要用的
* PC & Exception：若有异常发生，需要处理
* Valid bits for reg/data + control bits：数据是否已写完成等控制位，从而可以给其他后面的指令提供数据

### Example

具体的例子可以直接看课件笔记

其中有一个要注意的点是为了防止pipeline中的RAW数据依赖，在写的时候要把Valid bit设为0，直到数据写入后再改为1

### 考虑数据依赖性

stall是一个保守的坏主意，既然结果已经在ROB里去了，那就想办法去ROB中取

ROB的访问不是按照序号来的，而是按照指令的先后顺序。因此这里用到了 content adressable memory，它的作用就是一个 KV map，通过Key，在这里是Register ID来搜索某个ROB entry。而且必须是R3的最新定义，因为很有可能有多个写R3的entry，要用一个 priority comparison logic 的硬件逻辑或者软件来实现

现在的处理器的ROB一般都多达上千个entry，这导致用来搜索的硬件逻辑变得非常昂贵

ROB实际上提供了renaming of registers的方式来解决anti-dependency和output-dependency两种false-dependency的数据依赖形势。ROB给用户营造了一种有很多很多寄存器的假设，实际上一个CPU上的寄存器可能只有数十个

<img src="按序提交遵守冯诺依曼语义.drawio.png" width="70%">

ROB达到的效果就是上图，即In-order dispatch, out-of-order completion, in-order retirement 按序分发、乱序执行、按序提交

# 乱序执行

Distributed: load imbalance

## *OoO with Precise Exceptions*

### In-Order dispatch的问题

Dispatch 派送 指令的意思是把一个指令送到functional unit。In-Order dispatch的问题在于如果发生了stall，那会停止后面所有指令的dispatch，即使是根本没有数据依赖性的指令

<img src="InOrderDispatch的问题.drawio.png" width="80%">

就比如上图中的例子，因为第二句指令 `ADD R3 <- R3, R1` 对第一句指令存在RAW的真数据依赖，所以要stall流水线，单后面三句指令是无辜的，他们对前面的指令并不存在数据依赖。我们可以优化因为In-Order dispatch造成的损失

### Tomasulo Algorithm

Tomasulo Algorithm 是IBM的 Robert Tomasulo 1967年所研发用来改善处理器乱序执行指令级并行性的硬件算法。首先应用在 IBM 360/91 的浮点运算单元上

Tomasulo Algorithm 在提出后还加入了一些改进，主要是加入了对precise exception的支持。OoO变种被使用在当今大部分高性能的处理器上

Tomasulo Algorithm 过程如下

* If 保留站在renaming之前就可用了
  * 将指令 + renamed 操作数（source value/tag）插入到保留站中
  * 只有当保留站可用时才rename
* Else stall
* While 在保留站中的时候，每一条指令
  * 通过 Common Data Bus, CDB 监控各自的tag of its sources
  * 若tag可见，就把tag对应的值取过来放到保留站中
* 若指令需要的source register都准备好了，就把指令dispatch到functional unit中（前提是若FU的流水线没满）
* 在FU中的指令执行完毕后
  * CDB仲裁
  * 将tagged value放到CDB中广播

### 保留站

核心思想：增加硬件缓冲保留站 reservation station，把没有准备好的指令移到保留站里

具体过程看pdf

<img src="OutOfOrder.drawio.png" width="80%">

在支持 precise exception 的OoO中，主要有以下几部分组

* Reservations Stations 保留站/Scheduling Window 发射窗口：保留站 + 中间配合保留站使用的用于renaming的寄存器堆/表称为Register Alias Table, RAT，或者Frontend Register File
* Reorder Buffer/Instruction Window/Active Window：ROB用于按序retire
* architectural state register总是在最后确定retire的时候在修改的

执行完毕后需要deallocate 清空保留站，否则如果没有足够的保留站 entry可以使用的话还是会stall pipeline

### OoO的改进效果

<img src="OoO对同一个例子的改进效果.drawio.png" width="80%">

对于上面在In-Order Dispatch中的同一个例子，现在用OoO改进。后三个不存在数据依赖的指令现在不会被stall了，将原来5条指令需要20个cycle来执行缩短到了16个cycle

## *Dataflow Model*

指令是按照dataflow order来取指、执行。没有Pragramm Counter，只有当操作数准备好了的时候才会取指、执行

执行顺序是由Dataflow dependence指定的。每条指令会声明哪些其他的指令会接受自己的执行结果，当当前指令执行完毕后会 "fire" 其他需要自己结果的指令。一条指令所需要的所有input到位后就会被执行

因此在同一时间可能会有多条指令在执行，相比于一般按序执行的Von  Neumann模型有更高的并行度

### Example

* Sequential

  ```
  v = a + b;
  w = b * 2;
  x = v - w;
  y = v + w;
  z = x * y;
  ```

  从结果来看，`a` 和 `b` 是唯一的输入，`z` 是唯一的输出

* Dataflow

  <img src="DataflowExample.drawio.png" width="20%">

### Dataflow Nodes

下图是一个典型的Dataflow Node及其ISA表示

<img src="DataflowNode及其ISA表示.drawio.png">

* Conditional

  <img src="Dataflow条件节点drawio.png">

* Relational

  <img src="Dataflow关系节点.drawio.png">

* Barrier Synchronization：等待所有的dataflow到位后向外发射

  <img src="Dataflow同步节点drawio.png">

### OoO 与Dataflow的关系

OoO对于让没有数据依赖的指令OoO dispatch，让数据准备好的指令 fire 需要这些数据的指令实际上和 dataflow model 的思想基本是一样的

实际上在pdf的例子中，我们可以根据在某一个cycle中的部分寄存器信息来部分还原出dataflow model。要注意的是这种逆向工程的能力可以用来对系统进行攻击

因此在程序员的视角中，仍然是一个Von Neumann模型，但在微架构层面实际上则是一个dataflow模型

### FPGA

FPGA很适合dataflow，把（部分）程序变成了一个dataflow graph 然后映射到了硬件上。中间没有ISA，相当于直接写了一个硬件逻辑，比如说用在视频压缩的时候

FPGA的应用很多，比如说最近很多ML的应用可以在FPGA上加速

### Dataflow Model at ISA Level

* Pros：相比于Von Neumann可以获得更多的ILP，只有真正的数据依赖才会限制指令执行
* Cons
  * 没有precise state语义，这对于debugging和interrupt/exception的处理会造成很大的麻烦
  * 硬件的消耗比较大，比如tag matching的硬件
  * 过多的ILP也不是好事，调度不过来

## *Value Replication*

### 问题

在OoO的各个环节的寄存器中都存在着大量重复的值

优化方法就是只保留一个physical register file，所以其他地方没有寄存器，而是一个软件的map，其中保存 `<Register, Pointers to Register File>` KV对

### 解决方法

Frontend register file 在执行结束后进行广播的时候更新，用于renaming



# 分支预测

## *控制依赖*

### 控制依赖的场景

控制依赖也称为分支依赖 branch dependency，由于取到的指令并不是所需要的，或者指令地址的流向不是流水线所预期的。或者通俗的说，当需要根据一条指令的结果做出决定，而其他指令正在执行

若是一条分支指令的话，跳转到的PC是需要先decode才能得到的，这在指令周期的第一步就卡住了

### 分支类型

conditional和indirect比较麻烦

间接分支（也称为计算跳转、间接跳转、寄存器间接跳转）是存在于某些机器语言指令集中的一类程序控制指令。参数指定地址位置，而不是像直接分支那样指定要执行的下一条指令的地址。一个例子是“r1 寄存器上的间接跳转”。这意味着下一条要执行的指令位于寄存器r1中的地址处。在执行指令之前，跳转目标地址是未知的。间接分支还可以取决于内存位置的值。 间接分支对于创建条件分支非常有用，尤其是多路分支。例如，根据程序输入，可以在跳转表中查找值，以查找指向处理数据值所暗示的各种情况的代码的指针。可以将数据值添加到表的地址并将结果存储在寄存器中。然后可以根据该寄存器的值执行间接跳转，从而有效地将程序控制分派给适合输入的代码。 类似地，可以使用存储器中指定的被调用子例程的地址间接完成子例程调用指令。函数指针通常通过间接子程序调用来实现。 间接分支是 Spectre 的攻击面之一。为了减轻这种攻击，GCC 8.1 引入了新选项 -mindirect-branch=、-mfunction-return= 和 -mindirect-branch-register。

### 应对控制依赖的方法

* Stall：采用停顿，这种保守的方法是有效的，不过对性能影响很大，只有在所有方法都失效时才会使用
* Branch prediction 预测分支是否运行，预测的方法很多
* Delayed branching, branch delay slot
* 和在应对数据依赖时一样，FGMP通过快速线程切换也可以应对控制依赖
* Predicated Execution 判定式执行：Eliminate control-flow instructions
* Multipath execution：取出所有可能的分支并执行

### 硬件的提前分支决议

Early branch resolution

Equality checker

## *分支预测介绍*

分支预测 branch prediction 可以说是解决分支冒险的最重要的方法，它预测分支的结果并直接沿预测方向执行，而不是等分支结果确定后才开始执行。若预测对了则流水线全速运行；若预测错误就要 flush 清空或刷新预测分支流水线的各个阶段的数据，然后回头做实际发生的那个分支方向

### 分支预测方法

* 静态编译时分支预测

  * Always not taken
  * Always taken
  * BTFN
  * Profile based
  * Program analysis based

* 动态运行时分支预测

  动态分支预测 dynamic branch prediction 是借助硬件实现的，比如一种实现方案是采用分支预测缓存 branch prediction buffer或分支历史表 branch history table。分支预测缓存是一块按照分支指令的低位地址定位的小容量存储器

  动态硬件检测器根据每个条件分支指令的行为进行预测，并在程序生命周期内可能改变条件分支的预测结果

  动态预测的一种常用实现方法是保存每个条件分支是否发生分支的历史记录，然后根据最近的过去行为来预测未来。当历史记录的数量和类型足够多时。动态分支预测器的正确率超过90%

  * Last Time Predictability
    * Last time prediction (single-bit)
    * Two-bit counter based prediction
  * Two-level prediction (global vs. local) 两级预测算法
    * Global：1个分支的输出可能与其他分支的输出有联系
    * Local：1个分支的下一个输出可能与该分支的之前输出有联系
  * Hybrid
  * 现代处理器还会采用ML来预测分支

## *静态分支预测*

### Alway staken/not taken & BTFN

* Always taken/not taken
  * 不需要复杂的硬件支持
  * always taken（概率为60%～70%）的效果一般比always not taken（概率为30%～40%）的效果好
* Backward taken, forward not taken (BTFN)
  * backward branch通常指的是控制流程中的一个向后的跳转。程序中当一个程序执行到某一点，需要回到之前的某个点重新执行时，会使用到向后分支。这常常发生在循环结构中，比如while循环或for循环。每当循环的一次迭代完成，程序就会通过一个向后分支返回到循环的开始，继续下一次迭代
  * BTFN指的是把loop分支设为taken，其他分支设为not taken

### Profile-based

利用 Profiling 的方式来预测分支

* 静态/编译时Profiling

  * 这种分析在编译过程中进行，无需运行程序

  * 编译器可以分析源代码中的静态特性，如函数调用图、循环嵌套层次、分支预测等

  * 静态profiling通常不会测量运行时间或内存使用，但它可以帮助优化代码的结构和大小

* 动态/运行时Profiling

  * 动态profiling在程序运行时收集数据，它更常用且功能更强大

  * 编译器可能会在编译时插入额外的指令，用于在运行时收集信息。这种方法称为 Instrumentation

  * 另一种动态profiling方法是采样，它定期检查程序的状态而不是记录每个事件。这通常通过操作系统的定时器中断来实现

Profile-based是一种per-branch的预测，当profiling的结果准确时比较有效，不过需要在分支指令中额外插入命中位 hint bits

如果分支的真实概率是五五开的话，效果就会很差。特别是若profiling本身就出错了，那么run-time会变成一场灾难。比如说profiling得到的结果是分支A的概率是99%，分支B的概率是1%，所以编译时预测为分支A。但是实际上run-time的结果是分支B的概率是99%，分支A的概率是1%，意味着99%的情况下流水线都是错误的

* Accuracy depends on representativeness of profile input set

  ```
  TTTTTTTTTTTTTTTTTTNN -> 90% accuracy (or 10%?)
  ```

* Accuracy depends on dynamic branch behavior

  ```
  TTTTTTTTTTNNNNNNNNNN -> 50% accuracy
  TNTNTNTNTNTNTNTNTNTN -> 50% accuracy
  ```

### Programm-based

* 思路：使用基于程序分析的启发式方法来静态预测分支方向
* 操作码启发式 opcode heuristic：预测 BLEZ 为not taken（在许多程序中使用负整数作为错误值） 
* 循环启发式 loop heuristic：预测作为循环执行守卫的分支为已取（即执行循环）
* 指针和浮点数比较：预测不相等

Programm-based 不需要性能分析，但是启发式方法可能不具代表性或者效果不佳，启发式需要编译器分析和ISA支持（对于其他静态方法同理）

另外也可以通过编译程序指令 `#pragma` 为编译器提供建议

```c++
if (likely(x)) { /**/ }
if (unlikely(error)) { /**/ }
```

举一个OpenMP的例子：`#pragma omp parallel` 只是说建议编译器采用并行的方式来执行代码，而不是强制性的（虽然大部分情况下这么写都可以产生并行区）

## *Last Time Predictability*

### Last Time Predictor 

Last Time Predictor 假设分支会采取和上回同样的选择

对于一个拥有K个迭代的循环，它的正确率是 $(K-2)/K$

* 当K很大时，接近100%正确率

  ```
  TTTTTTTTTTNNNNNNNNNN -> 90% accuracy，中间从T到N的时候预测N出错
  ```

* 当K很小时，接近0%正确率

  ```
  TNTNTNTNTNTNTNTNTNTN -> 0% accuracy
  ```

### implementation

<img src="LastTimePredictor.drawio.png" width=90%>

### 问题 & 改善措施

Last Time Predictor的问题在于它从 `Not Taken -> Taken` 和 `Taken -> Not Taken` 的预测变换的太快了，换句话说动态性能太强

需要增加一些迟滞 hysteresis，从而来预测器不会因为一次不同的分支输出就发生变换

### 2BC Prediction

Two-Bit Counter Based Prediction, 2BC Prediction 就是上面增加迟滞的实现方法。2BC Prediction用两个bit来表示一个Taken or Not Taken的状态，因此2BC Prediction也被称为bimodal prediction

2BC Prediction采用saturating arithmetic，其状态图变化如下图

<img src="2BC的FSM.png" width="50%">

2BC Prediction提供了 $(K-1)/K$ 的正确率

```
TTTTTTTTTTTTTTTTTTTN -> 95% accuracy
TNTNTNTNTNTNTNTNTNTN -> 50% accuracy
```

虽然可以采用更多bits来进一步增加Taken or Not Taken的状态，但效果并不显著，因此用的最多的还是2BC Prediction。对于大部分程序而言，使用了2BC Prediction后可以达到80%-90%的正确率。不过这个正确率对于分支预测还是不够的，需要进一步优化

## *Global Branch Correlation*

### 设计

核心思想是**1个分支的输出可能与其他分支的输出有联系**，比如说下面几种情况

* 若第一个分支not taken，第二个分支肯定也not taken

  ```c
  if (cond1) {}
  else if (cond1 && cond2) {}
  else {}
  ```

* 若第一个分支taken，第二个分支肯定not taken

  ```c++
  if (x<1）{}
  else if (x>1) {}
  else {}
  ```

* 若Y和Z都taken，则X肯定不taken；若Y和Z都not taken，则X肯定也not taken

  ```c++
  //branch Y
  if (cond1) {}
  //branch Z
  else if (cond2) {}
  //branch X
  else if (cond1 && cond2) {}
  else {}
  ```

### Implementation

将当前分支的结果与所有其他分支的上一次的taken/not taken结果联系起来

<img src="2LevelGlobalHistoryBranchPredictor.drawio.png" width="80%">

* 将Global History Register, GHR向左移动一位，然后将分支的输出结果放到最右边LSR位。16位的GHR比较常见
* 用GHR的值作为索引，来索引一个Pattern History Table, PHT。PHT里面放的是上一次出现这种GHR值的情况时的这个分支的输出状态（BHT entry）

### Gshare Branch Predictor

计算机架构师意识到除了GHR的结果外，PC的值同样也有参考意义，可以进一步提升Global Predictor的准确率。所以可以将GHR和PC的值Hash后，用来索引PHT

比如说XOR就是一种hash，当然还可以使用其他的hash

<img src="Gshare.drawio.png" width=70%>

## *Local Branch Correlation*



<img src="2LevelLocalHistoryBranchPredictor.drawio.png" width="70%">

warmup：需要一段时间来累计数据，从而判断分支





Limit：会执行多少次loop

# 其他应对控制依赖方式

## *Predicated Execution*

### Predicate Combining

合并分支条件，需要执行一些无用的指令

### Predication

## *Delayed Branching*

## *TAGE*

# 多发射

## *Superscalar*

### 动态多发射处理器

动态多发射处理器也称为**超标量处理器 superscalar processors**，或者直接叫superscalars。超标量是一种处理器的硬件处理方式，处理器能够在动态执行时选择指令，并在同一个cycle执行一条以上的指令。同一个周期最多同时执行N条指令的被称为N-wide superscalar

Superscalar和Out-of-Order是完全正交的概念，可以排列组合。但出于历史原因很少有out-order scalar，因为superscalar和OoO基本上是同一时间发展出来的

### 多发射技术

流水线技术充分实现了指令级并行性 ILP，我们可以进一步增加流水线级数以进一步提高ILP，但是随着流水线级数的增加会带来更多的问题

另一种提高ILP的方法是增加流水线内部的功能部件数量，这样可以每周期发出多条指令，这种技术被称为**多发射技术 multiple issue**。同一周期发射的指令组合称为**发射指令包 issue packet**，可能是由编译器静态打包，也可能是由处理器在动态执行过程中进行调度的

<img src="多发射.drawio.png" width="50%">

实现多发射CPU主要有两种方式，区别在于编译器和硬件的不同分工

* 静态多发射 static multiple issue：指令发射与否的判断是在编译时完成的
* 动态多发射 dynamic multiple issue：指令发射与否的判断是在动态执行过程中完成的
* 虽然这两种方法听起来差别很大，但实际中这两种方法往往是互相集合的

多发射技术的缺点在于需要在相邻流水阶段之间传递负载，并保证所有机器都满负荷工作，这会增加额外的工作量

### 超标量处理器

补充：发射槽 issue slots 是指令发射时所处的位置

1. 将指令打包并放入发射槽。处理器该如何判断本周期发射多少条指令？发射哪些指令？

   * 在大多数静态多发射CPU中，编译器会完成这部分工作
   * 在动态多发射CPU中则是由硬件来完成，编译器可以通过指令调度来提高发射效率

2. CPU除了要处理原来pipeline中的数据和控制冒险之外，现在还需要处理在同一cycle内同时执行的指令之间的依赖。和上面一样，静态由编译器完成，动态由硬件完成。比如说把下面这种情况编译器把OR命令提前

   <img src="超标量纵向数据依赖.png" width="70%">

### 推测的概念

以预测 prediction 思想为基础，**推测 speculation** 方法允许编译器或处理器猜测指令的行为，并允许**其他与被推测指令相关的指令提前开始执行**，以求今早消除掉该指令与其他指令之间的依赖关系

推测的难点在于预测结果可能出错，从而导致所在以预测指令为基础的推测全部失效。因此所有推测机制都必须包括预测结果正确性的检查机制，以及预测出错后的回退 & 恢复机制，这进一步复杂化了结构设计的复杂度

## *VLIW*

### 静态多发射处理器

超长指令字 Very Long Instruction Word VLIW ISA：一种ISA类型，编译器支持在单个指令中使用不同的编码位来**把多个无逻辑关系的可同时被发射的独立操作指令叠加到一起**

<img src="VLIW.png" width="60%">

将VLIW的思路套到静态多发射上：可以将发射指令包看成是一条需要进行多种操作的大指令，同时由编译器等软件来实现之前硬件上的OoO等调度

VLIW并不像超标量一样应用在几乎所有现代处理器中，但是它也对处理器设计产生了重大影响，特别是编译器优化及其软件栈

VLIW是以Von Neumann为基础的，但对Von Neumann进行了微调。可以认为VLIW是Von Neumann的变种

编译器了解底层硬件，Instructions in a bundle **statically aligned** to be directly supplied into the functional units

### VLIW 特点

* 缺点
  * 静态调度无法应对长延迟操作，特别是涉及到缓存、内存的操作。编译器此时只能假设一些情况，如果假设的不对，就只能依靠硬件来stall pipeline
  * 如果没有很多的并行性，会插入很多的nop来stall pipeline
  * 为了方便编译器调度，一条VLIW中的所有指令在同一个lock step内完成，如果有早完成的不能继续加载新的指令
  * 静态调度和微结构紧密相关，导致一代的代码优化可能无法给下一代使用
* 优点
  * 大部分给VLIW开发的编译器优化都可以给其他CPU使用，比如说给超标量处理器使用
  * 当有很多并行性的时候VLIW的效果很好，比如说在一些嵌入式场景、DSP、GPU等 

## *脉动阵列*

### 脉动阵列的架构

脉动阵列 Systolic Arrays 实际上是一种execution model（Von Neumann、Dataflow）

Systolic的意思是 adj. 血压收缩期的。之所以起这个名字是因为把数据流动的过程看作是血液在心脏中的流动。其中memory是心脏，它推动数据行进；PE是细胞，处理使用血液

核心思想是把一个PE换成一个PE的阵列，并且特殊设计数据流后将其送入PE阵列。在这种情况下都不需要指令了

<img src="SystolicArrays.drawio.png" width="70%">

### PE阵列和pipeline的PE的区别

* 每一个PE都是独立的PE
* Array结构可以是非线性的和多维的
* PE之间的连接可以是multi-directional，之间可以是不同的速度
* PE可以有本地内存和execute kernels，而不是只是作为指令的加工站。从而可以实现stream processing以及pipeline parallelism

### 二维矩阵运算

Systolic Arrays最重要的设计就是如何设计PE阵列的组织方式以及组织数据输入PE阵列
$$
\left[\begin{matrix}c_{00}&c_{01}&c_{02}\\c_{10}&c_{11}&c_{12}\\c_{20}&c_{21}&c_{22}\end{matrix}\right]=\left[\begin{matrix}a_{00}&a_{01}&a_{02}\\a_{10}&a_{11}&a_{12}\\a_{20}&a_{21}&a_{22}\end{matrix}\right]\times\left[\begin{matrix}b_{00}&b_{01}&b_{02}\\b_{10}&b_{11}&b_{12}\\b_{20}&b_{21}&b_{22}\end{matrix}\right]
$$
<img src="2维矩阵乘法运算的脉动阵列.drawio.png" width="60%">

### TPU & NPU

Tensor Processing Unit, TPU 特别擅长加速神经网络的前向和反向传播算法，这些算法大量依赖于矩阵和向量的乘法和加法运算。Systolic Array架构使得TPU在这些任务上比传统的CPU和GPU更加高效

Neural Processing Unit, NPU 是一个通用术语/樊城，用来描述专门设计用于处理神经网络计算的处理器。这类处理器优化了机器学习算法中常用的操作，如矩阵乘法、向量运算和激活函数等。NPU可以采用不同的架构设计，其中systolic array只是众多可能的设计之一

各家公司设计的NPU各不相同，它们可能采用不同的架构和技术以优化特定类型的机器学习任务。有的NPU设计中可能会包括systolic array架构以提高矩阵运算效率，而其他的设计可能采用不同的并行处理架构来优化运算速度或者能效。关键点在于NPU被优化用于执行与人工智能和机器学习相关的操作

<img src="TPU的处理单元.png" width="40%">

比如说上图是一个有256个元素的TPU矩阵乘法处理单元，source: *Jouppi et al., “In-Datacenter Performance Analysis of a Tensor Processing Unit”, ISCA 2017.*

# SIMD

## *SIMD体系*

关于具体的SIMD编程可以看*并行编程.md*

### 费林分类法

费林分类法 Flynn's Taxonomy 是CS领域中用于描述并行计算体系结构的一种分类方法。它是由计算机科学家Michael J. Flynn于1966年首次提出的，用于区分不同类型的并行计算架构和计算模型。Flynn's Taxonomy为研究和描述并行计算提供了一个基本的框架，有助于理解不同类型的并行计算架构和其适用性

Flynn's Taxonomy**根据指令流和数据流的并行性**来将计算机系统划分为四种基本类别，这些类别分别是：

<img src="费林分类法.drawio.png">

* SISD Single Instruction stream, Single Data stream 单指令流单数据流：这是传统的、串行的计算模型，其中每个时钟周期只执行一条指令，并处理单个数据元素。它们适用于串行计算任务，用ILP来实现并行加速
* SIMD Single Instruction stream, Multiple Data streams 单指令流多数据流
  * 利用向量寄存器，可以同时对多个数据流执行相同的指令
  * 大多数计算机都会采用SIMD架构，它是最重要的一种架构，下面会重点介绍

* MISD Multiple Instruction streams, Single Data stream 多指令单数据流：这是一种相对不常见的体系结构，其中不同的指令流并行操作相同的数据流。Systolic Arrays 和 Streaming Processor 非常接近 MISD
* MIMD Multiple Instruction stream, Multiple Data streams 多指令流多数据流
  * 大部分并行计算机都是MIMD，其中多个处理器同时执行不同的指令流，每个指令流可以操作不同的数据流。MIMD适用于通用并行计算任务，可以并行执行多个独立的任务或线程
  * 在MIMD处理器上经常会编写**SPMD Single Program Multiple Data 多程序多数据流编程风格的程序**，这是一个Nvidia的术语。SPMD即不同的处理器通过条件语句执行不同的代码段（并不意味着同一cycle执行同一指令）

### SIMD简介

SIMD有三种变体：向量结构体系、多媒体SIMD指令集扩展和GPU。因为GPU的重要性以及内容比较丰富单独开一章

SIMD与MIMD相比最大的优势是，由于数据是并行的，所以程序员可以采用顺序的编程方式但却能获得并行的加速比，减轻了程序员编程的压力

向量结构体系是多媒体SIMD ISA的超集，因此向量结构体系相比于多媒体SIMD ISA更具有一般性

### SIMD -- 子字并行

SIMD是对多个不同的数据并行执行同一个操作：这种体系结构中，同一条指令同时应用于多个数据元素，通常通过向量寄存器或向量处理器来实现。SIMD适用于数据并行任务，如多媒体处理和科学计算中的向量操作

* 在基于SIMD架构的计算机上**有多个核心**，在任意时间点上所有核心只有一个指令流处理不同的数据流，**现在大多数计算机都采用了SIMD架构**
* SIMD本质上是采用一个控制器来控制多个处理器，同时对一组数据中的每一条分别执行相同的操作，从而实现空间上的并行性的技术

所有的微处理器都对字节和半字有特殊支持，然后在整数程序中却很少会用到。然而计算机架构师发现通过在128位的加法器内划分进位链，CPU可以同时对16个8位操作数、8个16位操作数、4个32位操作数或者2个64位操作数的短向量进行并行操作。这种分割加法器的开销很小，但带来的加速却可能会很大

**将这种在一个宽字内部进行的并行操作称为子字并行 subword parallelism 或者数据级并行 data level parallelism**。对于单指令多数据，它们也被称为向量或SIMD

## *SIMD操作*

### 标量 & 向量

<img src="标量和向量操作.drawio.png" width="50%">

* 标量浮点数扩展
  * 标量意味着一次只处理一个数据元素。在标量浮点数扩展中，浮点数操作是针对单个数据元素执行的，每个操作都只影响一个数据。这是传统的浮点数处理方式，通常用于处理单个数据的任务，如标量数学运算、科学计算中的某个标量数据，或者处理循环迭代中的单个元素
  * 比如如果我们有一个包含1000个单精度浮点数的数组，并且要对每个元素执行某种数学运算，那么标量浮点数扩展会将运算应用于数组中的每个元素，逐个处理
  * 标量处理器通常具有更高的时钟频率，因为它们不需要考虑同时处理多个元素
* 向量浮点数扩展
  * 向量意味着一次处理多个数据元素。在向量浮点数扩展中，浮点数操作是并行处理多个数据元素的，每个操作同时影响多个数据。这对于需要高吞吐量和并行性的任务非常有用，如图形处理、向量运算、多媒体处理等
  * 例如，一个向量浮点数扩展处理器可以一次性处理4个、8个或更多单精度浮点数，而不是仅处理一个
  * 向量处理器通常在特定应用领域中具有出色的性能，但在一些通用用途的计算中可能效率较低，因为它们不太适合标量计算

### Time-Space Duality

一条指令可以在时间维度上也可以在空间维度上作用在多条数据流上

<img src="ArrayAndVectorProcessor.png" width="70%">

注意：上图中的LD0、LD1中的数字只是说明他们的操作数据不同，并不是指令不同，不需要重复的IF和ID

* Array Processor 阵列处理器：单条指令在同一cycle使用**具有相同功能**的不同的Processing Elements, PEs 来处理不同的数据。Array Processsor中的PE很强大，每个都可以做任何的操作
* Vector Processor 向量处理器：单条指令在连续的cycle使用相同的PEs来处理不同的数据

其实大部分现代的SIMD处理器都不是单独的阵列或者向量处理器，通常是二者的结合，最典型的就是GPU

### SIMD vs. VLIW

<img src="SIMD.drawio.png">

## *向量结构体系*

向量体系结构就是以流水线形式来执行许多数据操作

### 向量寄存器

下面是一个典型的向量处理器，其以Cray-1作为基础，将其称为RV64V。RISC-V的向量扩展集是RVV，RV64V是RVV的一个核心子集，就像计算机组成原理中实现的RISC-V的标量核心子集一样

<img src="向量体系结构.png" width="50%">

* 向量寄存器 vector register

  * 向量寄存器是SIMD的核心所在，它用于存储向量数据，允许在单个指令中同时操作多个数据元素。如上图所示，向量寄存器和标量寄存器的不同之处就在于一个向量寄存器中可以存放多个数（实际上就是**把多个标量寄存器堆叠起来就可以形成一个矢量寄存器**）。注意：上图中的都是寄存器堆，每一个小框都是一个单独的寄存器

    <img src="向量寄存器.png" width="50%">

  * 这些寄存器用于执行向量化操作，如SIMD指令集中的操作

  * RV64V中有32个向量寄存器，每个为64bit宽

* 向量功能单元 vector functional units

  * **每个功能单元都是完全流水线化的**，RV64V有5个功能单元：浮点的加、减、乘、除、整数和逻辑运算

  * 因为数据并行不存在数据或控制依赖，因此也不需要stall或flush这些操作，所以向量功能单元完全不需要像普通的标量处理器一样只能拥有10-20级的流水线，向量功能单元的流水线可以做到非常深，即上千级的向量流水线

    <img src="VectorFunctionalUnit.drawio.png" width="40%">

* 向量载入/存储单元 vector load/store unit：从存储单元中载入或者向存储单元中存储向量，这两个操作是完全流水线化的

* 标量寄存器集合 a set of scalar register：标量寄存器通常就是RISC-V的32个通用寄存器和32个浮点寄存器，标量寄存器可以i用来提供数据、作为向量功能单元的输入，还可以计算传送存储单元的地址

RV64V的具体指令可以看书，用后缀 `.vv`、`.vs`、`\sv` 来表示矢量和标量的操作数组合，当然也可以用 `i` 来表示纯标量的操作数

另外RV64V采用了dynamic register typing 的类型，在指令中删去了操作数类型，此举可以避免大量的指令重复

### 向量控制寄存器

一个Vector操作还需要其他的辅助寄存器

* 要可以在不同长度的Vector上操作：需要一个 **Vector Length Register, VLEN** 来存放向量长度

* 一个Vector的元素可能会存放在内存的不同地方：需要一个 **Vector Stride Register, VSTR** 来存放Stride的长度。Stride就是逻辑上一个vector的两个相邻元素之间在内存中的物理距离

  Row-major order：矩阵中同一行的元素在内存中连续存储；Column-major order：矩阵中同一列的元素在内存中连续存储

  假设下图中A和B矩阵都是row-major order存储的。因此行列相乘，A每次取要一行一行的元素取，所以它的stride是1；而B每次取要一列一列的元素取，所以它的stride是10
  $$
  A_{4\times6}B_{6\times10}\rightarrow C_{4\times10}
  $$
  <img src="VSTR.drawio.png" width="90%">

* 有时候不是要在所有的vector元素上都进行操作的，有几个元素不需要操作：需要一个 **Vector Mask Register, VMASK** 来存放哪些元素需要操作

## *向量计算优化*

相比于标量计算，向量计算一次的数据量很大，这很容易让Memory Bandwidth成为限制计算的Bottleneck（暂时不考虑缓存等）

因此需要对向量的计算做出一些优化

### Vector Memory System

Vector从内存中load或store的时候要尽量将其分散到不同的banks里去，否则可能会发生bank conflict。当满足下面的三个条件时，我们可以获得1 element/cycle 的throughput

* stride == 1
* 连续的元素分散在不同的banks里
* banks的数量要大于每一个bank的延迟

<img src="向量分散到不同的banks.drawio.png" width="60%">

分成多个bank后，虽然单个bank仍然需要多个cycle来读取，但因为现在每个cycle都可以读写一个bank，所以相当于**也组成了流水线**，平均的读写时间下降了。并且单个bank的读取时间也下降了

### Scalar Code Example

下面我们考虑来看一下如何通过上面的数据分散到不同的banks是如何优化向量运算的，首先看一下scalar版本的代码

```c
for (int i = 0; i < 49; i++) { C[i] = (A[i] + B[i]) /2; }
```

Assembly code of the scalar code (instruction and its latency in clock cycles)，总共是需要 `4 + 50*6 = 304` 条指令 

```assembly
    MOVI RO = 50       # 1 cycle
    MOVA R1 = A        # 1
    MOVA R2 = B        # 1
    MOVA R3 = C        # 1
X:  LD R4 = MEM[R1++]  # 11 jautoincrement addressing
	LD R5 = MEM[R2++]  # 11
	ADD R6 = R4 +R5    # 4
	SHFR R7 = R6 >> 1  # 1 
	ST MEM[R3++]= R7   # 11
	DECBNZ RO, X       # 2 ;decrement and branch if NZ
```

* 上面的指令如果是只有1个bank、1个port的情况下需要 `4 + 50*40=2004` 个cycles完成
* 如果采用2个banks、1个port或者1个bank、2个port的话，两条LD指令就可以流水线化了，只需要付出第一次load的11个cycles，第二个load平均下来是1个cycles，因此两个ld总共只需要消耗12个cycles，节省了10个cycles。即最终需要`4 + 50*30=1504` 个cycles完成

### 循环展开

循环展开 Loop unwinding/loop unrolling：一个循环是一个Vectorizable Loop如果每一次迭代之间没有依赖性，编译器展开后可以形成vector functional unit和memory banking的流水线

上面的scalar代码中的for循环里每一次迭代之间没有依赖性，我们可以把它展开

```assembly
# Vectorized Loop
MOVI VLEN = 50     # 1
MOVI VSTR = 1      # 1
VLD VO = A         # 11 + VLEN – 1
VLD V1 = B         # 11 + VLEN – 1
VADD V2 = VO +V1   # 4 + VLEN – 1
VSHFR V3 = V2 >> 1 # 1 + VLEN – 1
VST C = V3         # 11 + VLEN – 1
```

假设没有 vector chaining，所有的向量元素都准备好了才能往后面的向量功能单元送；1个port、16个banks

<img src="VectorizedLoop.png" width="80%">

其中 `VLEN=50`，总共需要285个cycles

可以看到向量比标量效率高多了，标量一个数据流程就需要一个完整的指令流程，而向量一个指令流程就可以操作很多数据，不过也需要特殊的ISA向量指令支持

### Vector Chaining

所谓的Chaining其实和应对数据依赖的前递 forwarding 是一个思路，也就是顺序靠前的向量运算的单个元素准备好之后直接给顺序靠后的向量功能单元，不需要等一个向量功能单元全部元素全运算好了才给

因为memory只有一个port，不能同时读写，所以VLD和VSD不能piepline。现在总共需要182个cycles

可以进一步优化，为每一个bank提供2个load ports、1个store port，现在可以完全pipeline load和store，总共需要79个cycles

### 其他优化

* Vector Stripmining：如果要进行的向量操作大于向量寄存器的容量，此时可以将一个loop打散成多个loop

* Gather/Scatter Operations：如果存储的向量的stride不规则

  特别试用于稀疏矩阵，稀疏矩阵又在ML中很常见。假设有一个有10亿数据的矩阵，但是里面90%都是0，这时候再按照同样的stride去存储、寻址、加载它们就没有道理了。scatter的作用就是把他们取出来后pack到一个vector后进行运算

* VMASK去除了分支操作

## *车道*

### 多车道

实际中都是把Array Processor和Vector Processor组合起来使用的。其中一个Vector Processor称为一个车道 Lane，比如说下图中一个车道包含了1个浮点加法功能单元、1个浮点乘法功能单元和1个存取单元

Array Processor则体现在把多个车道组合起来。然后把同一个向量寄存器中的不同元素分配给不同的车道以优化memory banking性质以提高memory的流水线程度

多车道是GPU

<img src="4车道向量单元结构.png" width="50%">

### Automatic Code Vectorization

对于SIMD指令，编译器自动把循环做了automatic code vectorization。自动向量化是指把一个没有依赖的循环展开为向量操作，比如说把下面迭代1和迭代2自动向量化

<img src="AutomaticCodeVectorization.drawio.png" width="70%">

自动向量化是一个编译时完成的reordering，需要循环依赖分析

### Example

这个例子中既有纵向的车道数据并行，也有横向的CPU流水线

<img src="车道例子.drawio.png">

### 单车道测量向量执行时间

向量运算序列的执行时间主要取决于3个因素：操作数向量的长度、操作之间的结构冒险、数据相关

为了简化对向量指令执行时间的测量，只使用单车道 lane，此时一条向量指令的执行时间大约就是向量长度。注意：车道 lane 和流水线 pipeline 是不同的，车道是完全并行的任务，而流水线则是交错的进行任务

护航指令组 convoy 是一组可以一直执行的向量指令，可以通过计算convoy的数目来估计一段代码的性能

钟鸣 chime 是指执行convoy所花费的时间单位，用来将convoy转换为执行时间所定义的定时度量

## *向量处理器的例子*

### ILLIAC IV 

### Cray-1

[Cray-1 - Chessprogramming wiki](https://www.chessprogramming.org/Cray-1)

Cray-1既有vector操作也有scalar操作，体现了heterogeneity 

设计者希望大部分的计算都发生在vector部分，而此时scalar的部分就成为了bottleneck，因此Cray-1的scalar操作部分被优化了当时计算机中最快的

## *多媒体SIMD扩展*

### 多媒体SIMD扩展起源

SIMD ISA多媒体扩展源于这样一个事实：许多多媒体应用程序操作的数据类型要比对32或64位CPU进行针对性优化的数据类型要更窄一些，使用一个完整的32位或者64位寄存器来存储单元来进行操作实际上浪费了运算能力。**此时通过划分这个加法器中的进位链，CPU可以同时对一些短向量进行操作**

* 图像、视频中大量使用8位（0-255）数据来表示RGB通道和透明度
* 音频采样则通常采用16位、24位、32位数据来表示
  * 16位：这是最常见的音频采样位数之一，通常用于CD音质（44.1 kHz，16位，立体声）。它提供了65536个不同的音量级别，可以捕捉大部分听觉范围的细微差异
  * 24位：24位音频采样通常被认为是高分辨率音频，它提供了更大的动态范围和更高的精度。它用于专业音频录制和音乐制作
  * 32位浮点：32位浮点音频采样具有非常高的精度，它们使用浮点数格式来表示音频样本，允许超过24位的动态范围和更高的精度。这在音频处理中非常有用，因为它可以避免损失音频质量

### 多媒体SIMD扩展相比于向量结构体系的简化

* **多媒体SIMD扩展固定了操作代码中数据操作数的数目，从而在x86体系结构的MMX、SSE和AVX扩展中添加了数百条指令**。向量体系结构有一个VLR，用于指定当前操作的操作数个数。一些程序的向量长度小于体系结构的最大支持长度，由于这些向量寄存器的长度可以变化，所以也能够很轻松地适应此类程序。此外，向量体系结构有一个隐含的MVL，它与向量长度寄存器相结合，可以避免使用大量操作码
* **多媒体SIMD扩展没有提供向量体系结构的更复杂寻址模式**，也就是步幅访问和集中一分散访问。这些功能增加了向量编译器成功向量化的程序数目
* **多媒体SIMD扩展通常不会像向量体系结构那样，为了支持元素的条件执行而提供遮罩寄存器**。这些省略增大了编译器生成 SIMD 代码的难度，也加大了 SIMD 汇编语言编程的难度

这些省略增大了编译器生成SIMD代码的难度，也加大了SIMD汇编语言编程的难度，那么为什么多媒体SIMD扩展还是如此流行呢，因为相比于向量结构体系，多媒体SIMD扩展具有如下优点

### x86-64的多媒体SIMD扩展演进

* 1997年集成在Pentium CPU上的MMX SIMD多媒体指令集

  引入MMX Multi Media eXtensions 的目的旨在提高多媒体和图像处理性能。MMX引入了8个64位寄存器，用于存储多媒体数据，以及一组指令，用于执行各种多媒体操作。它最初被广泛应用于多媒体应用程序，如音频编解码和图像处理

  然而MMX没有提供独立的矢量寄存器，它的8个寄存器MM0-MM8实际上就是浮点数寄存器st0-st7用来存放尾数的部分，从而导致MMX指令和浮点数操作不能同时工作

* SSE于1999年集成在Pentium III CPU上发布

  SSE Streaming SIMD Extensions 流式SIMD扩展是对MMX的进一步扩展，引入了128位XMM寄存器和一组新的指令，支持单精度浮点数运算和更广泛的SIMD操作。SSE提供了更多的并行性和性能，适用于一系列应用，包括3D图形渲染和数字信号处理。SSE的不同版本（如SSE2、SSE3、SSSE4等）在后续的处理器中陆续发布，增加了功能和性能，比如双精度SIMD浮点数据

* AVX于2011年第一季度发布的Sandy Bridge系列处理器中首次支持

  AVX Advanced VEctor Extension 高级向量扩展引入了256位和512位YMM和ZMM寄存器，并提供了更多的SIMD指令，以支持更大规模的并行计算。AVX广泛用于高性能计算、虚拟化、人工智能等领域，它加速了复杂的浮点数运算任务

* AVX2于2013年发布的Core i7 Haswell CPU中引入

* AVX-512：AVX-512是AVX的进一步扩展，引入了512位ZMM寄存器和广泛的指令集，支持更大规模的数据并行性。使用新的EVEX前缀编码将AVX指令进一步扩展到512位

* AMX, Advanced Matrix Extensions：Designed for AI/ML workloads, 2-dimensional registers, Tiled matrix multiply unit (TMUL)

# GPU

## *GPU简介*

https://zhidx.com/p/259964.html

在GPU的体系结构中融合了TLD（多核）、ILP（流水线、多发射）、DLP（SIMD）的多种并行处理形式，因此GPU的设计让它称为处理并行问题的好手。可以说**GPU是由多线程SIMD处理器组成的MIMD**

和CPU依赖于多级cache来消除内存的长延迟不同，GPU依赖于单个多线程SIMD处理器中的硬件多线程来隐藏存储器延迟（快速、大量的线程/warp调度）

异构系统 heterogeneous system：一个结合了多种处理器的系统，比如PC是一个CPU-GPU系统

这种统一且可扩展的处理器阵列为GPU引入了一种新的编程模型。GPU处理器阵列中大量的浮点处理能力非常适合解决非图形问题

GPU计算是指通过并行编程语言和API在GPU上进行计算，而不使用传统的图形API和图形管线模型。这与早期的通用GPU计算（GPGPU）方法形成对比，后者涉及使用图形API和图形管线来执行非图形任务

### GPU的发展历程

图形处理单元(GPU)的演进 - 2know的文章 - 知乎 <https://zhuanlan.zhihu.com/p/493201665>

1. 早期计算机图形（1950s - 1960s）：在计算机图形的早期阶段，计算机的图形渲染能力非常有限。图形主要由文本字符和线段构成，没有高级图形处理单元

2. Raster CRT（1970s - 1980s）：随着计算机显示器的普及，Raster CRT（阴极射线管）技术允许计算机以像素为基础进行图形渲染

3. 早期图形卡（1980s - 1990s）：早期的图形卡通过添加专用硬件，如帧缓冲器和图形处理器，来加速图形渲染。这些图形卡通常支持2D图形

4. 3D加速卡的兴起（1990s）：随着3D游戏的兴起，出现了首批3D加速卡，如3dfx Voodoo，它们引入了3D图形渲染的硬件支持。这些卡使用了专用的3D渲染引擎，为游戏提供了更高的性能和图形质量

5. NVIDIA的创立（1993）：NVIDIA 公司的创立标志着GPU行业的重要时刻。NVIDIA 在1990年代末和2000年代初推出了一系列创新的GPU产品，如GeForce系列，推动了GPU的发展

6. 通用GPU计算（2000s）：GPU不再仅仅用于图形渲染，它们的计算能力也得到了利用。通用GPU计算（GPGPU）允许开发人员在GPU上执行通用计算任务，如科学计算和深度学习

7. 并行处理（2010s）：GPU的核心数量迅速增加，引入了大规模并行处理能力

   NVIDIA的CUDA和AMD的OpenCL等编程模型使开发人员能够更好地利用GPU的并行性

   处理器指令和内存硬件被添加以支持通用编程语言，并创建了一个编程环境，允许使用熟悉的语言（包括C和C++）来编程GPU。这一创新使GPU成为一个完全通用的、可编程的、众核处理器

8. 深度学习加速（2010s - 至今）：GPU在深度学习领域取得了巨大的成功，因为它们可以高效地执行深度神经网络的训练和推理任务。许多深度学习框架如TensorFlow和PyTorch都支持GPU加速

9. 光线追踪和实时图形（2020s - 至今）：GPU的性能继续提升，允许实时光线追踪和更高质量的图形渲染。这推动了游戏图形的逼真度和虚拟现实体验的发展

### 显卡结构

【显卡科普】小白必看的入门显卡科普，关于显卡的原理、结构、作用 - JanePot的文章 - 知乎
https://zhuanlan.zhihu.com/p/156083352

<img src="显卡构造.png" width="70%">

GPU只是显卡 Video Card 的一个部件，显卡还有很多其他的部件

* GPU毋庸置疑是最核心的处理芯片
* 大量显存，目前好的显卡的显存基本都大于20G
* 电源供电
* 视频输出接口：HDMI、DisplayPort、DVI、VGA等
* OEM 厂商基本只会设计显卡散热（涡轮式、开放式）和外壳

显卡分类

* 集成显卡/集显：集成显卡是指集成在主板的北桥芯片的显示芯片。它不是一个独立的硬件组件
* 核心显卡/核显：核心显卡是集成在CPU内部的显卡。这是集成显卡的一种，但专指集成在处理器内部的显卡，共享系统内存资源
* 独立显卡/独显：独立显卡是一个独立的硬件组件，通常安装在主板的PCIe插槽上

### 异构计算

如何用通俗易懂的话解释异构计算？ - 辣笔小星的回答 - 知乎 <https://www.zhihu.com/question/63207620/answer/1707221853>

异构计算 Heterogeneous Computing 是一种特殊的并行分布式计算系统

* GPU包括更多的运算核心，其特别适合数据并行的计算密集型任务，比如大型矩阵运算。CPU的运算核心较少，但是其可以实现复杂的逻辑运算，因此其适合逻辑密集型任务
* CPU上的线程是重量级的，上下文切换开销大，存在大量的高级ILP技术；GPU的线程是轻量级的，没有那么多的ILP部件
* 基于CPU+GPU的异构计算平台可以优势互补，CPU负责处理逻辑复杂的串行程序，而GPU重点处理数据密集型的并行计算程序，从而发挥最大功效

https://imgtec.eetrend.com/blog/2021/100061572.html

两大派别：GPU和FPGA

## *FGMT*

### 概念

<img src="FGMT.drawio.png" width="25%">

Fine-Grained Multithreading, FGMT 的思想是在每个cycle里从不同的线程中取指令，这些指令并行的在pipeline中跑。对于每个thread而言并不存在pipeline，从每个thread自己的视角看出去自己是独享整个系统的，只是这个系统看起来是完全串行的

* 硬件上有多个完全独立的线程（PC+线程寄存器）
* 除非一个线程的一条指令在pipeline中跑完了，否则不能从这个线程中再取指令

下面是一个FGMT的实现例子，可以看到每个线程有自己的PC和General Purpose Register, GPR

<img src="FGMT例子.png" width="60%">

### GTX 285

<img src="GTX285.drawio.png">

NVIDIA GeForce GTX 285采用Tesla架构，引入了CUDA。它仍然采用FGMT调度，但CDUA的调度单位从thread上升为32个threads一组的warp，此时每一个warp共享一个PC，执行相同的执行。最多有32个warp以FGMT方式交叉调度

### Pros & Cons

* Pros
  * 线程之间是完全独立的，所以不需要处理数据和控制依赖
  * 总体上的吞吐量非常高
* Cons
  * 因为要隔一段时间才能从同一个线程中取指，单个线程的性能会受影响。如果需要特别提高某个线程的性能不能使用FGMT
  * 线程的上下文切换需要硬件逻辑支持
  * 线程会争用cache和memory
  * 如果不能有足够的线程来充满流水线的话，总体吞吐量受影响

## *Warp-based SIMD*

### 编程模型和执行模型

* 编程模型 programming model 是指程序员是如何表达代码的。比如说 Von Neumann、Dataflow、SPMD
* 执行模型 execution model 是指底层硬件是如何执行代码的。比如说OoO、SIMD (Array & Vector)、多核处理器、多线程处理器

结论：GPU使用了在SIMT执行模型上执行的SPMD编程模型

* SPMD Single Program Multiple Data 多程序多数据流编程编程模型，这是一个Nvidia的概念。SPMD即不同的处理器通过条件语句执行不同的代码段（并不意味着同一cycle执行同一指令）
* SPMD编程模型可以在 Single Instruction Multiple Thread, SIMT 机器上执行。同样SIMT也是一个Nvidia的概念，它的意思是硬件会动态地把执行相同指令的线程放到同一个warp (Nvidia)/wavefront (AMD)里去

### SIMD vs. SIMT Execution Model

SIMT和SIMD很相似，区别在于**GPU编程是利用线程，而不是SIMD指令**，其中一个warp相当于一条SIMD指令，一个线程相当于是一个数据流。也就是说编译器不会把一个没有依赖的循环向量化，而是把这个循环的每个iteration都当作一个独立的线程来执行

* SIMD：SIMD指令的单个顺序指令流，每个指令指定多个数据输入。`[VLD, VLD, VADD, VST], VLEN`
* SIMT：向量指令的多个指令流，线程会被动态分组为warp。`[LD, LD, ADD, ST], NumThreads`

SIMT有两个主要的优势

* 每一个线程可以被分开处理和指令，然后每个线程都可以使用任何标量流水线，这实际上就是一种MIMD
* 可以灵活地将不同线程分组为warp，warp里的线程执行同样的指令

### FGMT of Warps

<img src="FGMTofWarps.drawio.png" width="90%">

上图表达的意思是以warp为单位在一个SIMD pipeline中进行FGMT调度。每一个warp共享一个PC，执行相同的执行

从这里我们可以理解SIMT和SIMD的相似与不同。可以发现SIMT基本就是把一条SIMD指令来执行多个数据流换成用一个warp中的32个线程来执行多个数据流

### CUDA软件架构

<img src="CUDA软件架构.png">

CUDA的软件架构在逻辑上可以分为thread，block，gird。注意：thread，block，gird的设定是为了方便程序员进行软件设计和组织线程，是CUDA编程上的概念

* **Grid**：kernel在device上执行时实际上是启动很多线程，一个kernel所后动的所有线程称为一个网格 grid。同一个grid上的线程共享相同的全局内存空间。grid是线程结构的第一层
* **Block**：Grid又可以分为很多线程块 block，一个线程块里面包含很多线程，这些线程运行在同一个SMP中。块不能太小以至于不能隐藏其调度开销，但是也不能太大，一般是128或256个线程（32的倍数）。block是第二层
* **Warp**：一个block里的线程按顺序排成一个一维向量，每32个线程称为一个warp，是CUDA最小的调度单位。warp是第三层
* 单独的线程是第四层，线程是最小的逻辑单元

## *SPMD的分支问题*

warp里的所有线程使用的是同一个PC，而走不同的分支说明需要不同的PC，此时需要masking，将需要走某些分支的线程关闭

### Branch Divergence

实际上此时Path A和Path B用不同的PC，已经是两个Warp了

最后有一些硬件逻辑来converge两条分支

SIMD里程序员需要自己用mask来处理这种情况

### Dynamic Warp Formation/Merging

active thread: mask bit is 1

Full warp: utilization rate of threads is 100%



必须要有一个thread被删除，不能换到另一个线程，因为它们用的是同一个SP，或者说同一个SIMD车道，所以用的寄存器是相同的

## *软件 & 硬件之间的映射*

### Fermi架构

<img src="Fermi架构.drawio.png">

Fermi架构是NVIDIA GPU架构的第一个重要版本，首次引入了CUDA计算架构，将GPU用于通用计算任务。这个架构引入了流多处理器 Streaming Multiprocessor 的概念，每个多处理器包含多个CUDA核心，具有更强大的计算能力和性能。代表性产品有 Tesla C2050/C2070、GeForce GTX 480/470

Fermi架构有16个Streaming Multiprocessor, SM 流多处理器，每个SM通过L2 Cache与全局DRAM显存相连

* 每个SM有32个核心 Core，也称为 Streaming Processor, SP 或者CUDA Core。每个核心通过interconnect network和L1 Cache/共享全局内存互联。每个核心中还有Warp Scheduler、Dispatch Snit以及Special Functional Unit等硬件

  **所谓的SP就是一个SIMD车道**，里面有一个浮点单元和一个整形计算单元/ALU。一个SM因此就是一个多车道（Array + Vector Processo） 

  因为不需要处理数据和控制依赖，浮点单元和ALU都是上千级**深度流水线化**的

* Warp Scheduler：分配

* Dispatch Unit

* Interconnect Network

* 16组加载存储单元 LD/ST

* 4个特殊函数单元 SFU

### 映射

一个kernel grid会被分割成多个block，每个block会被分配给一个SM。在每个SM内部，block中的线程会被组织成多个warp，同一个block的warps只能在同一个SM上执行。但同一个SM可以容纳来自不同block甚至不同grid的若干个warp。在同一个block中的线程可以通过共享内存来通信，但不同的block的线程之间无法通信

Warp 线程束 是程序员可以操作的最基本的执行单位，这些warp将在SM内部的SPs上并行执行。具体Warp中的线程是如何使用SPs的呢？

1. 设一个SM有32个SPs。一个warp包含32个线程
2. 当这个warp被调度执行时，warp中的线程0会被分配到SP0上，线程1会被分配到SP1上，依此类推，直到线程31被分配到SP31上
3. 然后这个warp中的所有线程将同时执行相同的指令，每个线程都在自己的SP上

如果一个SM有更多的SPs，它可以同时执行多个warp；如果有更少的SPs，那么不同的warp将会在不同的时间周期被调度到SPs上执行

当一个warp在执行时，如果遇到延迟（比方说由于内存访问延迟），GPU的调度器会切换到另一个warp，这个过程称为上下文切换。这种快速的上下文切换是GPU实现高吞吐量的关键机制，因为它允许硬件在等待一个操作完成（如内存读取）时不必闲置，而是可以执行其他工作

### SP流水线

<img src="SP流水线的使用.drawio.png" width="50%">

SP 是深度流水线化的（FP、ALU操作可能需要多个cycle），这样在执行操作的第一个cycle时，其他warp的线程就可以调度过来开启它的操作。也就是说在任何给定的时刻，一个SP可能会被来自不同warp的不同线程所使用。这种设计允许GPU在处理一个线程的指令时准备和发起其他线程的指令，从而确保流水线保持忙碌状态，最大化资源利用。

这个过程称为 warp交织, warp interleaving，它确保当一些线程因为例如等待内存读取完成而暂停时，其他线程能够继续执行，从而隐藏延迟。这样，深度流水线化的SP可以持续执行操作，几乎没有空闲时间

在一个SM中，通常会有多个warp同时处于活跃状态。这些warp中的线程会根据调度器的安排在SP上执行。当一个warp遇到延迟并不能继续执行时，调度器会选择另一个准备就绪的warp继续执行，这样就可以利用SP的流水线特性，实现高效率的执行。这种动态的线程调度是GPU硬件的一个关键特性，它允许高吞吐量的并行处理，尤其是在执行大量浮点计算和图形渲染任务时充分利用了SP这个深度SIMD流水线

## *内存模型*

GPU的数据大部分都直接进入memory，因为核心太多，cache不足以支撑

### GPU物理内存结构

<img src="GPU内存结构.png">

L1 cache, L2 cache

### CUDA逻辑内存结构

下面这张图有点把软硬件混在了一块，但是可以很好地说明软件上的grid、block和thread可以获取到什么存储资源

<img src="CUDA内存模型.png">

* 每个线程有自己的一份寄存器和私有本地内存 local memory
* 每个block拥有一份共享内存 shared memory，block里面的线程都可以用它。Shared memory的作用相当于是L1缓存，它是一个scratchpad，程序员可以显式地对其操作
* 同一个grid里的所有线程（包括不同 block 的线程）都可以访问全局内存 global memory（即显存，general purpose，read/write）以及只读内存 常量内存 constant memory 和 纹理内存 texture memory。这些内存也可以被host访问，用来在host-device之间传输数据

### 可编程内存

* 可编程内存：用户可以用的代码来控制这组内存的行为
* 不可编程内存的行为用户无法决定，是由底层的微架构设计决定的。用户只能尽量去了解它们，然后利用某些规则来加速程序

CPU/GPU 内存结构中，L1和L2 cache都是不可编程（完全不可控制）的存储设备，完全由CPU自己决定如何使用它们

## *Nvidia GPU架构*

### 概览

GPU的微架构和CPU的不同，除了ISA外，还有图形函数API集合。图形函数主要用于绘制各种图形所需要的运算。比如像素、光影处理、3D坐标变换等相关运算要由GPU硬件加速来实现

<https://blog.csdn.net/tony_vip/article/details/123604308>

NVIDIA的GPU产品根据目标市场主要分成GeForce、 Tesla和Quadro三大系列区别。虽然从硬件角度来看采用的架构可以是相同的，也都支持用作通用GPU计算，但因为它们分别面向的目标市场以及产品定位的不同，这三个系列的GPU在软硬件的设计和支持上还是存在许多差异的

* Quadro是面向专业图形领域的专用显卡
* Tesla是面向数据中心的专业GPU，单价相对较高，也都很少会被用作其他用途
* GeForce显卡用于消费者级别的PC端影音娱乐，却因为出货量大，价格较低的缘故经常被当作另外两个专业产品的替代品來使用

NVIDIA的显卡的微架构有如下，按照发布时间从早到晚的顺序排列：

Tesla架构 `->` Fermi架构 `->` Kepler架构 `->` Maxwell架构 `->` Pascal架构 `->` Volta架构 `->` Turing架构 `->` Ampere架构 `->` Hooper架构







Texture Processor Cluster, TPC 是多个SM形成的小组

纹理映射单元（TMU）作为GPU的部件，它能够对二进制图像旋转、缩放、扭曲，然后将其作为纹理放置到给定3D模型的任意平面，这个过程称为纹理映射。纹理映射单元不可简单跨平台横向比较，大量的纹理映射单元是GPU性能强劲的必要非充分条件。

光栅化处理单元（ROPs）主要负责游戏中的光线和反射运算，兼顾AA、高分辨率、烟雾、火焰等效果。游戏里的抗锯齿和光影效果越厉害，对ROPs的性能要求就越高，否则可能导致帧数的急剧下降。NVIDIA的ROPs单元是和流处理器进行捆绑的，二者同比例增减。在AMD GPU中，ROPs单元和流处理器单元没有直接捆绑关系。

### Tesla

Tesla是初代G80系列使用的架构，是第一个实现统一着色器模型的微架构

### Volta架构

* Volta架构主要面向深度学习和人工智能应用，引入了张量核心（Tensor Cores），大大加速了矩阵计算，有助于深度学习模型的训练。这个架构还引入了更高带宽的HBM2内存
* 代表性产品有Tesla V100、Titan V

### Turing架构

* Turing架构继续优化了深度学习性能，引入了RT核心，用于实时光线追踪 RTX 和深度学习推理。它还引入了GDDR6内存，提供更高的内存带宽
* 代表性产品有Tesla T4、GeForce RTX 20系列（如RTX 2080）

### Ampere架构

* Ampere架构进一步提高了性能和能效，引入了第三代Tensor核心，加速了深度学习任务。它还采用了更先进的制程技术，提供更多的CUDA核心和更大的内存容量
* 代表性产品有A100、GeForce RTX 30系列（如RTX 3080）





shared memory = scratchpad in Nvidia terminology

## *术语对比*

# 存储器层次结构设计

## *存储器量化基础*

### 多核时代的来临

总峰值带宽基本上随核心个数的增大而增大。此时为了应对极高的总峰值带宽需求，采用的策略是

* 实现缓存的多端口和流水线
* 利用多级缓存，为每个核心使用独立的L1缓存，在L1使用独立的指令与数据缓存。有时也使用独立的L2缓存

比如下图是对Intel Xeon Platinum (Icelake) 执行triad microbenchmarking是的缓存使用情况，呈现明显的阶梯形



### Little定理

## *缩短命中时间*

### 小而简单的L1缓存

### 采用路预测

### 在缓存索引期间避免地址转换

## *增加缓存带宽*

### 实现缓存访问的流水线

### 采用无阻塞缓存

### 采用多组缓存

## *降低缺失代价*

### 采用多级缓存

### 关键字优先和提前重启动

### 合并写缓冲区

## *降低缺失率*

### 增大块

### 增大缓存

### 提高相联程度

### 为读取指定高于写入操作的优先级

### 采用编译器优化以降低缺失率

## *通过并行以降低缺失代价或缺失率*

### 对指令和数据进行硬件预取

### 用编译器控制预取

## *3C Benchmarking*

# 预取

### 预取流

数据数据流 prefetch data stream

由硬件控制的预取被称为硬件预取 hardware prefetch，用源语 intrinsic 可以控制软件来预取



Hardware stream prefetcher 会观察输入流是否有一定的pattern

prefecther在系统的各处都存在



prefetch到L1要小心一些，因为L1很小

过早了不好，因为可能会过早抢占资源，把本来要用的block evict出去

太晚了则减少latency的效果不明显



如果能比较准确的知道要用什么数据，就可以通过预取来消除所有的cache miss。更极端的是理论上甚至可以通过预取来完全消除cache的作用

# 存内计算

存内计算 Processing using Memory

# 近内存处理

近内存处理 Processing near Memory

# RowHammer

# 内存延迟

# SSD

### SSD固态硬盘

<img src="SSD结构.png">

* 每个闪存芯片替代了传统机械硬盘中的disk driver
* Flash translation layer 的作用和disk controller是一样的，将对逻辑块的请求翻译成对底层物理设备的访问
* 读写操作以页为单位。一个页只能在所属的整个块都被擦除之后，才能重写这一页。每一个块可以进行大约100,000次擦除，之后这个块就会损坏（其实SSD能用的时间甚至会超过一半电脑的寿命）

# 线程级并行

## *多核处理器*

### TLP & 多核处理器

TLP意味着有多个程序计数器，因此主要是通过MIMD加以开发。而为了充分利用拥有n个处理器的MIMD多处理器，通常必须拥有至少n个要执行的线程（1个任务拆成n份交给n个处理器执行）或进程（n个任务交给n个处理器分别执行），因此TLD就是多处理器。

多处理器是紧耦合处理器组成的计算机，其中的所有处理器的协调与使用由单一的处理器系统控制，通过共享地址空间来共享存储器。多处理器通过两种不同的软件模型来开发

* 并行处理模型：运行一组紧密耦合的线程，协同完成同一个任务
* 请求级并行 RLP：执行可能由一位或多位用户发起的多个相对独立的进程。当然在同一台计算机上RLP的规模是远小于在一个WSC系统上的。RLP可以由单个app或多个app开发（多重编程 multiprogramming）

指定给一个线程的计算量称为粒度 grain size。TLP与DLP的重要定性区别在于：TLP是由软件系统或程序员在较高层级确定的，这些线程由数百条乃至数百万条可以并行执行的指令组成。

### 多处理器体系结构的分类

一个挑战是如何通过有效地变成来利用单个芯片上数量不断增长的处理器，从而发挥出不断进步的硬件的能量。重写旧程序使其能够直接在并行处理器上良好的运行是困难的，计算机架构师设计了一些架构来简化编程的过程

<img src="并行CPU结构分类.drawio.png" width="60%">

* 共享内存多处理器 Shared Memory Processor：一种方法是为所有处理器提供一个共享的统一物理地址空间，使得所有处理器都可以直接取到需要的变量，从而专注于如何并行执行。处理器通过存储器中的共享变量进行通信，因此统一地址空间支持共享时必须要提供一套同步 synchronizaiton 机制

  根据多处理器所包含的存储器数量，可以将共享存储器的多处理器分为两类，处理器的数量决定了存储器的组织方式和互连策略。处理器的数目是多少可以按照时间而变化的，因此按照存储器的组织方式来分类多处理器

  **注意：SMPs指的是 Symmetric shared-memory Multiprocessors，即对称多处理器，而不是Shared Memory Processor**

  * 对称（共享存储器）多处理器 Symmetric shared-memory Multiprocessors SMPs 或者集中式（共享存储器）多处理器 centralized shared-memory multiprocessors 有少量的处理器（一般少于32个）

    因为处理器数目少，所以处理器可以共享一个集中式存储器，所有处理器都能够平等访问该处理器，这就是对称名字的来源

    **SMPs也称为统一内存访问多处理器 Uniform Memory Access, UMA**：访存延迟不依赖于是哪个处理器提出的请求。即无论哪个处理器访问存储器，存储器的访问延迟都大致相同

    <img src="SMP架构.drawio.png" width="50%">

  * 分布式共享存储器 Distributed Shared Memory, DSM

    为了支持更多的处理器，存储器必须分散在处理器之间，而不应该是集中式的；否则存储器系统无法在不大幅延长访问延迟的情况下为大量处理器提供带宽支持。这些存储器在物理上虽然是分开的，但位于同一个逻辑地址空间

    **DSM也称为非统一内存访问多处理器 Non-Uniform Memory Access, NUMA**：存储器的访问延迟各不相同，具体取决于哪个存储器访问哪个存储器

    <img src="DSM架构.png" width="70%">

    * COMA, Cache-Only Memory Architecture
    * ccNUMA, Cache-Coherent Non-Uniform Memory Access
    * nccNUMA, Non-Cache-Coherent Non-Uniform Memory Access

* 消息传递多处理器：另一种方法则是每个处理器采用独立的地址空间，此时必须进行显示的信息传递。一般在WSC中使用

## *硬件多线程*

### 硬件多线程 & 软件多线程

线程就是一个拥有自己Programm Counter和栈的执行流

* 软件多线程 software multithreading 就是在每个单核中通过CPU的动态时间片调度不同线程组所模拟出来的并行假象，实际上还是在串行执行
* 硬件多线程 hardware multithreading 是用硬件来实现的独立的执行流 + 一套独立的控制单元来执行指令流。硬件多线程是在ILP发展到一个瓶颈阶段后引入的一种新机制，以便让多个线程在同一个CPU上同时执行，以提高吞吐率

### 硬件多线程的实现方法

硬件上实现线程的一般方式是

* 寄存器和堆栈：每个线程具有自己的寄存器堆，这些寄存器包括通用寄存器、程序计数器PC、堆栈指针等。线程还有独立的堆栈空间，用于保存函数调用和局部变量
* 硬件调度：操作系统使用硬件的计时器中断等机制来决定何时切换线程。硬件提供了机制来触发上下文切换，通常在特定的时间片内或在等待I/O等操作时发生
* 同步和互斥：硬件提供原子操作和锁机制，用于实现线程之间的同步和互斥，以避免竞争条件和数据不一致
* 上下文切换 context switch： 硬件支持上下文切换，以便在多线程环境中切换执行线程。上下文切换涉及保存当前线程的寄存器状态和内存映射表，然后加载下一个线程的状态。这通常由操作系统和硬件协同完成
* Intel的超线程技术 Hyper-Threading： 一些处理器支持超线程技术，它允许一个物理核心模拟多个逻辑核心。这意味着每个物理核心可以执行多个线程，每个线程都有自己的寄存器集和执行状态。这提高了并发性，允许更多的线程在同一物理核心上运行

### 硬件层级

* Core 核心：核心是处理器芯片上的基本计算单元。一个多核处理器包含多个核心，每个核心可以执行独立的指令流。每个核心通常有自己的寄存器文件和执行单元。**一个Core里面还可以通过SMT/Hyper-threading来实现多个HWT**
* Socket 插槽：一个插槽通常包含一个物理处理器芯片（CPU）。多核处理器系统可以包含多个插槽，每个插槽上插有一个处理器芯片，每个处理器芯片上有多个核心。这意味着一个多插槽系统可以同时运行多个处理器，每个处理器有多个核心。一般一个插槽的多个多核CPU共享一个L2或L3缓存
* Node 节点：节点是一台独立的计算机，通常由多个插槽组成，这些插槽访问一个共享内存。每个节点可以具有自己的内存、存储和其他资源。在HPC领域，一个节点通常指的是一个独立的计算单元，它可以运行独立的作业或任务。节点通常通过网络连接在一起，以构建更大的并行计算集群
* Network 网络：在多核处理器系统中，网络通常指的是连接不同节点之间的通信网络。这个网络用于在节点之间传输数据和消息，以支持分布式计算、协作任务和数据共享。高性能计算集群通常使用高速网络（如InfiniBand）来实现节点之间的低延迟和高带宽通信

<img src="硬件层级.drawio.png">

注意：**从软件的视角来看每个HWT就是一个单独的CPU**

### 线程粒度

* 细粒度多线程 fine-grained multithreading 在每条指令之后切换线程，从而形成了多线程的交叉执行
* 粗粒度多线程 coarse-grained multithreading 是作为细粒度多线程的一种可选项被发明的。它仅在重大事件（比如说末级cache失效，需要IO）之后才会切换线程

## *线程亲和性*

### 灵活性对性能的影响

线程亲和性 thread affinity、线程固定 thread pinning 和 process binding 是三个可以互换的概念

在多核处理器中，任何软件概念上的线程都可以被安排到某个核上。但这种灵活性可能会对性能造成影响

* 若一个线程迁移到不同的核心，并且该核心拥有自己的缓存，那么原始缓存中的内容将丢失，会发生不必要的内存传输
* 若线程迁移，OS无法阻止将两个线程放在一个核心上，而另一个核心完全未被使用这种负载不均衡的情况。显然，此时即使线程数量等于核心数量，也会导致不完美的加速

### 亲和性策略

我们称线程/进程与核心之间的映射为亲和性 thread/process affinity，Thread affinity/thread pinning技术就是将线程固定到某个特定的核上。

亲和性通常以掩码 mask 的形式表示，即描述线程被允许在哪些位置运行的说明。mask取决于采用哪种亲和性策略

* close：适用于共享内存的线程

  ```
  Thread  Socket 0  Socket 1
  0       0
  1       1
  ```

* spread：适用于需要大的bandwidth的线程，这样可以进可以让它得到socket的bandwidth

  ```
  Thread  Socket 0  Socket 1
  0       0
  1                 1
  ```

### Fisrt-touch Policy

## *同时多线程/超线程*

### 原理

> Hyper-Threading Technology is a form of simultaneous multithreading technology  introduced by Intel, while the concept behind the technology has been patented  by Sun Microsystems.  -- wikipedia

同时多线程 SMT Simultaneous Multithreading 或者称超线程 Hyper-threading 是一种CPU的虚拟化技术，也就是说从一颗物理上的CPU可以得到多颗逻辑或者说软件层面的CPU，从而表现为1个核心里面产生了多个核心。

Intel自Pentium开始引入超标量、乱序执行、大量的寄存器及寄存器重命名、多指令解码器、预测执行等新的ILP特性。这些特性的原理是让CPU拥有大量资源，但是现实中这些硬件很少会同时运行起来，所以其实存在着资源浪费。

所以SMT的思想就是让两个线程同时不冲突的利用CPU的这些资源。比方说一条整数运算指令只会用到整数运算单元，此时浮点运算单元是空闲的，ILP也只是在同一颗CPU上的，不可能同时用到另一份硬件。而使用了SMT的话，此时刚好有另一个线程要执行一个浮点运算指令，CPU就允许属于两个不同线程的整数运算指令和浮点运算指令同时执行。

<img src="SMT_HT.jpg" width="70%">

上面是Intel提供的原理说明，可以看到在Physical processor中，黄色资源和橙色资源在任何时间片都是没有冲突的，而蓝色资源则存在冲突，所以最终通过SMT可以得到两个并行运行的线程。

### Pros & Cons

* Pros
  * 更好的利用资源：超线程允许一个物理核心模拟多个逻辑核心，从而更有效地利用了处理器的资源。这意味着更多的线程可以并行执行，提高了处理器的吞吐量
  * 更好的多任务处理：超线程可以显著提高多任务处理性能。在多任务环境中，多个线程可以在同一个核心上并行执行，而不会相互干扰
  * 更好的响应时间： 超线程有助于提高系统的响应时间，因为处理器可以更快地切换到不同的线程来处理不同的任务
  * 降低成本：超线程允许在相同的物理处理器上实现更多的逻辑核心，而无需增加硅芯片的物理核心数。这有助于降低成本
* Cons
  * 性能下降：尽管超线程提高了并发性，但并不总是带来性能提升。在某些工作负载下，超线程可能导致性能下降，因为多个线程在竞争相同的处理资源
  * 资源竞争：超线程引入了更多的资源竞争，包括缓存、执行单元等。这可能导致一些线程在等待资源时浪费了处理时间
  * 安全问题：超线程可能引入安全问题，因为多个线程可以同时运行在同一个核心上，它们可能访问共享的资源。这可能导致信息泄漏和侧信道攻击的潜在风险。

超线程引入了一些潜在的安全问题

* 信息泄漏：如果敏感数据在一个线程中处理，并且另一个线程同时运行在同一个核心上，它可能通过缓存侧信道攻击或其他方式访问敏感数据，从而导致信息泄漏
* 侧信道攻击：通过分析处理器的资源使用模式，攻击者可能能够获得有关运行在同一核心上的线程的信息，这可能导致侧信道攻击，例如缓存侧信道攻击和分支预测侧信道攻击
* 争用条件：多个线程同时运行在同一个核心上可能导致争用条件，从而可能导致数据不一致或不可预测的行为

HPC系统中基本都是默认关闭超线程

## *集中式共享存储器*

### Cache一致性

### 缓存行状态

缓存行相对于主存中的数据项的状态通常被描述为以下几种情况之一

* Scratch：缓存行不包含该项目副本
* Valid：缓存行是主内存中数据的正确拷贝
* Reserved：缓存行是该数据的唯一副本
* Dirty：缓存行已被修改，但尚未写回主内存
* Invalid：缓存线上的数据在其他处理器上也存在（它没有被保留），并且另一个进程修改了它的数据副本。一个更简单的变体是修改后的共享无效（MSI）一致性协议，在一个给定的核心上，一个缓存线可以处于以下状态
  * Modified：缓存线已经被修改，需要写到备份仓库。这个写法可以在行被驱逐时进行，也可以立即进行，取决于回写策略
  * Shared：该行至少存在于一个缓存中且未被修改
  * Invalid：该行在当前缓存中不存在，或者它存在但在另一个缓存中的副本已被修改

### 实现cache一致性的基本方案

### 监听协议

## *集中共享多处理器的性能*

## *分布式共享存储器的目录式一致性*

## *存储器连贯性模型*

# 缓存一致性




# 以DSC开发RLP、DLP

## *WSC & HPC & Data Center*

### WSC vs. Data Center

传统的Data Center和WSC最大的区别就在于其规模上，传统的Data Center一般只有数千台服务器，而一个WSC则拥有数十万台服务器。WSC从而凭借其规模优势，形成了云计算行业不断盈利，而Data Center则完全没有盈利

1. 所有权和管理
   * WSC：WSC由第三方提供商拥有和管理，客户租用云资源。客户无需购买硬件或承担硬件维护责任
   * 数据中心传统数据中心通常由组织或企业自己拥有和管理，需要购买、配置和维护硬件设
2. 客户的付费模式
   * WSC：WSC通常采用按需付费模型，客户只需为实际使用的资源付费，无需大量预先投资
   * 数据中心：传统数据中心需要前期投资，包括硬件购买、维护和运营成本。数据中心都是各家公司自己组建的，需要自己维护，最大的成本是人力费用
3. 灵活性和可伸缩性
   * WSC：WSC提供灵活的资源扩展和缩减，客户可以根据业务需求快速调整资源
   * 数据中心：扩展或收缩传统数据中心的资源通常需要更多时间和资源
4. 安全性和可用性
   * WSC：WSC通常提供一系列安全和高可用性选项，但安全性和可用性取决于客户的配置和管理
   * 数据中心：安全性和可用性完全由组织负责
5. 管理和自动化
   * WSC：WSC通常提供丰富的管理和自动化工具，以简化资源配置和监控
   * 数据中心：组织需要自行开发和维护管理工具

### WSC vs. HPC

WSC和HPC在多核处理器分类里都属于是信息传递类多核计算机

1. 用途
   * WSC：WSC是用于对外提供数据中心功能或云计算环境的大规模服务器集群，旨在支持云服务、大规模数据处理、分布式计算、虚拟化等工作负载。它们主要用于托管和运行各种云服务、网站、应用程序和存储服务，以满足广泛的企业和用户需求
   * HPC：HPC是专门设计用于解决复杂科学和工程计算问题的高性能计算机。它们通常用于模拟、模型化、数据分析和科学研究，用于处理大规模计算、模拟和模型化任务
2. 架构
   * WSC：WSC通常采用通用计算节点，具有通用处理器、存储和网络连接，以支持各种工作负载。它们可能使用虚拟化技术来运行多个虚拟机和容器
   * HPC：HPC通常采用高度定制化的硬件架构，包括专用的处理器、高速互连网络和大规模内存。它们的设计旨在最大程度地提高计算性能和并行性。HPC的架构更加紧凑，处理器和节点之间的通信要比WSC中快的多，因为HPC应用程序的独立性更强，通信也更频繁
3. 性能
   * WSC：WSC的性能度量通常以处理大量的并发请求、提供高可用性和可伸缩性为重点，而不是以计算性能为主
   * HPC：HPC的性能度量通常以浮点运算性能 FLOPS/s 为重点，旨在解决科学和工程领域的复杂计算问题。因为HPC的高度定制化，它在某一个特定任务中具有出色的计算能力，但通常不具备与仓库级计算机相同规模的存储和通用计算能力
4. 并行性 & 负荷
   * WSC：实际业务需求中往往存在着大量的RLP，因此重视RLP来完成许多独立的任务请求。WSC中服务器的利用率是不高的，大部分时间都在10%-50%之间，但浮动很大
   * HPC：编程环境强调TLP和DKP，通常会重视单项任务的完成时间。HPC集群往往需要长时间作业，以充分利用其计算性能。大负荷的计算任务会使服务器满负荷运行，持续时间甚至达到数周
5. 管理和维护
   * WSC：WSC机通常由数据中心管理员进行管理和维护，重点是确保高可用性、可伸缩性和资源分配
   * HPC：HPC通常由专门的高性能计算中心进行管理和维护，重点是最大程度地提高计算性能和解决科学问题

## *WSC的体系结构*

### 补充：Imperial Unit

* 长度单位
  * 英寸 inch：1英寸 = 25.4毫米
  * 英尺 foot：1英尺 = 12英寸，约0.3048米
  * 码 yard：3英尺 = 36英寸 = 9.144 dm  = 914.4 mm
  * 英里 mile：1英里 = 1.609344千米
* 重量单位：磅 pound：1磅等于0.45359237千克
* 体积单位
  * 加仑 gallon：1美制加仑等于3.785411784升
  * 液体盎司 fluid ounce：1液体盎司等于约0.0295735升
* 温度单位：华氏度 Fahrenheit：华氏温度用于测量温度，而不是摄氏度。水的冰点为32°F，沸点为212°F，在这个范围内刻度

### 机架标准

* IA-310标准
  * 电子工业联盟（Electronic Industries Alliance，EIA）制定了EIA-310标准，定义了机架的物理尺寸和标准化孔位。这个标准包括了机架高度、宽度、深度、前后支柱位置以及孔位的排列等信息
  * 标准机架高度通常为48U（1U等于1.75英寸或44.45毫米，1U是一个服务器可以占用的最小单元），但也有其他高度可供选择，如45U或42U
* 19英寸机架：大多数服务器机架采用19英寸的标准宽度。这个宽度在EIA-310标准中有详细规定，以确保不同厂家的设备可以放置在同一个机架内

## *WSC编程模型 - 批处理框架*

### MapReduce

WSC中最流行的批处理框架 batch processing framework 是Google开发的MapReduce及其开源孪生框架Apache Hadoop。MapReduce现在由Apache基金会管理，被包裹在Hadoop中

MapReduce依赖HDFS或GFS这种分布式文件系统向任意计算机提供文件，开发工作（包含调试）需要一个运行Hadoop或GFS的计算机。关于这两种分布式文件系统可以看 *DistributedSystem.md*

<https://www.open-open.com/lib/view/open1328763069203.html>

<https://www.twblogs.net/a/5b8dc8ab2b7177188340a30e/?lang=zh-cn>

MapReduce系统分为3个组件

* 客户端用于将用户撰写的并行处理作业提交至Master节点
* Master会自动将用户作业分解为Map任务和Reduce任务，并将任务调度到工作节点 Worker
  * `Map(key, value) -> (key, value)`：Map首先将程序员提供的函数应用于位于数万台服务器上的每个逻辑输入记录，从而产生KV对的中间结果
  * `Reduce(key, <value>) -> (key, value)`：Reduce收集对应key的分布式任务的输出的value后，并使用另外一个程序员定义的函数折叠它们（合并聚合结果）

* Worker向master请求执⾏任务，同时多个Worker组成的分布式文件系统HDFS或GFS⽤于存储输入输出数据

下面是一个用于计算大量文档中每个英语单词的出现次数的程序的伪代码

<img src="MapReduce示例.png" width="70%">

```
map(String key, String value): 
	// key: document name 
	// value: document contents
	for each word w in value: 
	EmitIntermediate(w, “1”); // Produce list of all words
	
reduce(String key, Iterator values): 
	// key: a word 
	// values: a list of counts 
	int result = 0;
	for each v in values: 
	result += ParseInt(v); // get integer from keyvalue pair 
	Emit(AsString(result)
```

### Pig

Pig是由Yahoo开发的高级数据流语言和执行框架，用于数据分析和大数据处理，现在由Apache基金会管理。Pig语言允许用户编写数据处理脚本，而Pig执行引擎将这些脚本转换为MapReduce作业，并在Hadoop集群上执行

Pig 提供的SQL-Like语言叫Pig Latin，该语言的编译器会把类SQL的数据分析请求转换为一系列经过优化处理的MapReduce运算。Pig Latin = SQL-kind queries + Distributed execution

<img src="PIG编译.png" width="60%">

Pig为复杂的海量数据并行计算提供了一个简单的操作和编程接口，使用者可以透过Python或者JavaScript编写Java，之后再重新转写

# DSA

<http://www.365pr.net/tech_view.asp?id=5602>

> Domain-Specific Architectures（DSA）是后摩尔时代持续提升处理器性能的一种技术理念，即采用可编程的专用集成电路（ASICs）去加速特定的高强度的处理器负载，比如加速图形渲染、加速AI神经网络的前向推理计算、提高巨量网络数据的吞吐等。



> **特殊应用集成电路**（英语：**A**pplication **S**pecific **I**ntegrated **C**ircuit，缩写：**ASIC**），是指依产品需求不同而[全定制](https://zh.wikipedia.org/wiki/全定制)的特殊规格[集成电路](https://zh.wikipedia.org/wiki/集成電路)，是一种有别于标准工业IC（例如[7400系列](https://zh.wikipedia.org/wiki/7400系列)或[4000系列](https://zh.wikipedia.org/wiki/4000系列)[[1\]](https://zh.wikipedia.org/wiki/特殊應用積體電路#cite_note-:0-1) ）的集成电路产品。例如，设计用来执行数字录音机或是高效能的[比特币](https://zh.wikipedia.org/wiki/比特币)[挖矿机](https://zh.wikipedia.org/wiki/挖礦_(數位貨幣))功能的IC就是ASIC。ASIC芯片通常使用金氧半导体场效应管(MOSFET)技术的[半导体工艺](https://zh.wikipedia.org/wiki/半導體製程)。-- wikipedia



TPU