# IO模型和高并发

## *I/O模型*

**I/O的本质是等+数据拷贝（内核缓冲区拷贝到应用缓冲区）**

### 同步I/O synchronous：发起IO和等待IO、执行数据拷贝的人是同一个发起者

* 阻塞模型：为了避免程序内部的数据不一致或者其他错误，程序在IO设备将数据准备好之前，会一直阻塞CPU，即程序一直处于调用的未返回状态。此时CPU是空闲的，但不能执行其他任务，CPU资源被浪费了。fd默认都是阻塞I/O

  <img src="阻塞IO.png">

  当一个应用进程被挂起后，它通常会等待某些事件的发生，比如等待I/O操作完成、等待定时器到期等。这是因为当进程被挂起时，操作系统会将进程的状态保存在内存中，包括进程的寄存器值、堆栈信息等。当进程被恢复时，操作系统会将保存的状态恢复到CPU寄存器中，并从进程被挂起的位置继续执行。因此，当某个事件发生时，操作系统需要将CPU控制权交给相应的事件处理程序，并等待该事件处理程序执行完毕后，再恢复进程的执行

  例如，当一个进程进行I/O操作时，通常需要等待I/O设备完成读写操作，然后才能继续执行。在这个过程中，操作系统会将进程挂起，然后将进程的状态保存在内存中，**并将CPU控制权交给I/O设备的驱动程序**。驱动程序会在I/O操作完成后向操作系统发出中断信号，然后操作系统会调用相应的事件处理程序来处理这个中断，从而将CPU控制权重新交给挂起的进程，使其继续执行。在这个过程中，**CPU需要等待I/O操作完成并等待操作系统的调度，才能够恢复进程的执行**

  当然了，**阻塞的概念是相对于每个程序/进程所占的CPU时间片而言的，在程序被调用的时间片内，程序就停止了调用IO的那一行了不会往下执行，它占用的CPU资源被浪费了**。并不是针对整个系统而言的，否则要是只要IO一下系统就卡住了也太可怕了

* 非阻塞模型：若IO设备还未将数据准备好之前，采取**轮询 polling**的方式反复检测和尝试读取文件，若没有准备好就直接调用返回 `EWOULDBLOCK` 错误码。因为调用后马上返回，所以CPU这段时间可以用来处理其他工作，但是仍然要定期恢复询问

  <img src="非阻塞read轮询.png">

  read 信号驱动I/O：内核将数据准备好之后，使用 `SIGIO` 信号通知程序。信号会丢失，用的比较少

* I/O多路转接/I/O多路复用 multiplexing

  **多路转接能够同时阻塞等待多个文件描述符的就绪状态**。I/O中最耗时的就是等待数据就位，所以引入了一些I/O函数，比如select可以一次等待多个文件，I/O效率显著提高；还有poll和epoll进一步提高等待的IO量

  这样进程就可以在进行多个IO的时候处理自己的事情，知道IO完毕，由select通知进程，然后CPU再寻机调度进程处理

  `select` 和 `epoll` 在单进程模式下本质上还是单进程串行的，因为它们都只能在单个进程中运行。不过，它们使用了非阻塞 IO 和多路复用技术，可以在单进程中同时处理多个 IO 事件，从而提高系统的并发处理能力。但是，在高并发场景下，单进程的处理能力可能仍然无法满足要求，这时候就需要使用多进程或多线程来进行扩展。在多线程或多进程模式下，可以将不同的 IO 事件分配给不同的线程或进程来处理，从而提高系统的并发处理能力
  
  注意多路复用既可以设置成阻塞式的，也可以设置为非阻塞的

### 异步I/O asynchronous：发起IO和等待IO、执行数据拷贝的人是不同的发起者

<img src="异步IO.png">

90%的场景里都是阻塞I/O，因为它很简单，部分会采用非阻塞和多路转接，这三种是主流的。异步I/O的逻辑非常复杂，一般都是底层的库，在公司里因为异步I/O的代码很复杂也不易维护

注意和多线程同步的区分，同步IO和多线程同步没有什么关系



## *异步：基于事件的并发*

### 多线程实现并发的问题

假设业务场景中有一组互不相关的任务需要完成，现行的主流方法有两种

* 单线程串行依次执行
* 多线程并行执行

在OS中，若可以用信号、锁等方式来控制线程，从而实现低开销的多线程并发，那当然多线程会是首选。但多线程的应用是很复杂的，正确处理并发逻辑很困难，忘加锁、各种死锁现象层出不穷

那选择单线程串行执行由如何呢？对于阻塞IO那自然是直接堵塞了整个CPU，而非阻塞IO因为没有完成完整IO，CPU要不断轮询，所以实际上也会造成效率浪费

因此对于实际情况中高并发、高吞吐量的服务器而言，更实际的是采用基于事件的并发 event-based concurrency，而不是用线程来实现并发

### 基本思想：事件循环

基于事件的并发的思想是：等待某事件发生，当它发生时检查事件的类型，然后做少量的相应工作（IO或者其他的调度）就可以完成并发了

事件循环 event loop 的基本结构是

```python
while (1):
	events = getEvents()
    for e in events:
    	processEvent(e) # event handler 事件处理程序
```

## *高并发场景*

异步I/O和多路复用都适用于高并发场景，但它们的应用场景有所不同

### 异步IO场景

异步I/O适用于需要进行大量的I/O操作，但I/O操作的时间不确定的场景。在这种情况下，如果使用阻塞I/O或非阻塞I/O，应用程序需要轮询I/O事件并等待I/O操作完成，这会占用大量的CPU资源。相比之下，异步I/O可以在I/O操作进行的同时进行其他的操作，从而避免CPU的浪费，提高系统的并发性和吞吐量。但是，异步I/O的实现比较复杂，需要适当的编程技巧和经验

1. Web服务器：在Web服务器中，客户端请求和响应的I/O操作通常需要花费较长的时间，如果使用阻塞I/O或非阻塞I/O，会导致服务器的CPU资源浪费。因此，一些Web服务器会使用异步I/O模型来处理这些I/O操作，从而提高服务器的并发性和吞吐量
2. 大规模数据处理：在大规模数据处理中，需要进行大量的I/O操作，如文件读写、网络传输等，这些I/O操作的时间往往不确定。使用异步I/O可以在I/O操作进行的同时进行其他的操作，从而提高系统的并发性和吞吐量

### 多路复用场景

多路复用适用于需要同时处理多个I/O事件的场景。在这种情况下，如果使用阻塞I/O或非阻塞I/O，应用程序需要使用多个线程或进程来同时处理多个I/O事件，这会导致系统的资源浪费和复杂性增加。相比之下，多路复用可以使用单个线程或进程同时处理多个I/O事件，从而提高系统的并发性和吞吐量。常见的多路复用模型有select、poll、epoll等，其中epoll是最常用的一种模型，具有更高的性能和可扩展性

1. 聊天程序：在聊天程序中，需要同时处理多个客户端的消息，使用多路复用可以使用单个线程或进程同时处理多个客户端的I/O事件，从而避免使用多个线程或进程增加系统复杂性
2. 网络游戏：在网络游戏中，需要实时处理多个客户端的消息，使用多路复用可以提高服务器的并发性和响应速度，从而提升游戏体验

总之，异步I/O和多路复用都可以提高系统的并发性和吞吐量，但它们的应用场景有所不同，需要根据实际情况进行选择。如果需要同时处理多个I/O事件，可以选择多路复用；如果需要进行大量的I/O操作，可以选择异步I/O

# UNIX系统IO
## *阻塞IO*

### socket就绪条件

* 读就绪
  * socket内核中，当接收缓冲区中的字节数大于等于低水位标记 `SO_RCVLOWAT` 时，可以无阻塞的读该文件描述符并返回大于0
  * socket TCP通信中，对端关闭连接，此时对该socket读，则返回0
* 写就绪
  * socket内核中，当发送缓冲区中的可用字节数（发送缓冲区的空闲位置大小）大于等于低水位标记`SO_SNDLOWAT` 时，可以无阻塞的写并返回大于0
  * socket的写操作被关闭（close或者shutdown）时对一个写操作被关闭的socket进行写操作，会触发 `SIGPIPE` 信号
* 异常就绪：socket上收到带外数据，TCP的URG标志位相关

### 为什么要使用阻塞IO？

通常情况下非阻塞I/O会比阻塞I/O更加高效，因为非阻塞I/O可以避免在等待I/O事件发生时CPU的阻塞，从而提高系统的吞吐量和性能

然而，不是所有的场景都适合使用非阻塞I/O。例如，在某些低负载的场景中，使用阻塞I/O可能更加简单和易于维护，而且在某些场景中，由于使用阻塞I/O可以避免出现竞争条件等问题，因此阻塞I/O可能更加安全和可靠

另外，虽然非阻塞I/O可以避免CPU的阻塞，但它也需要一定的轮询时间来检查I/O事件是否已经发生，这可能会导致额外的CPU开销。因此，在高并发和高负载的场景中，选择合适的I/O模型对于系统性能和吞吐量的提升至关重要

综上所述，选择适合特定场景的I/O模型是一项复杂的任务，需要综合考虑系统的需求、负载情况、可靠性等因素，以达到最佳的性能和可维护性

## *非阻塞I/O*

### `fcntl` 系统调用

```cpp
#include <unistd.h>
#include <fcntl.h>
int fcntl(int fd, int cmd, ... /* arg */ );
```

fcntl系统调用 file control 用于控制已经打开的文件。它可以对打开的文件进行一些操作，如修改文件属性，获取或设置文件状态标志等。具体实现fd的什么功能，取决于`cmd`。**文件描述符默认都是阻塞IO**

* `cmd=F_DUPFD`：复制一个现有的描述符
* `cmd=F_GETFD` 或 `cmd=F_SETFD`：获得/设置文件描述符标记
* `cmd=F_GETFL` 或 `F_SETFL`：获得/设置文件状态标记
* `cmd=F_GETOWN` 或 `F_SETOWN`：获得/设置异步I/O所有权
* `cmd=F_GETLK`、`cmd=F_SETLK` 或 `cmd=F_SETLKW`：获得/设置记录锁

### 实现非阻塞I/O，以read轮询

```cpp
bool SetNonBlock(int sock)
{
    int flag = fcntl(sock, F_GETFL);
    if (flag == -1) return false;
    int n = fcntl(sock, F_SETFL, flag | O_NONBLOCK); //将sock设置为非阻塞
    if (n == -1) return false;
    return true;
}
```

* 使用 `F_GETFL` 将当前的文件描述符的属性取出来（这是一个位图）
* 然后再使用 `F_SETFL` 将文件描述符设置回去。设置回去的同时，加上一个 `O_NONBLOCK` 参数

非阻塞的时候，总是以read出错的形式返回，告知上层数据没有就绪

```c
sing func_t = std::function<void()>; // 用一个包装器
void show() { cout << "我们正在处理其他的事情" << endl; }
int main()
{
    std::vector<func_t> funcs;
    SetNonBlock(0);
    //0号fd
    //string s;
    char buffer[1024];
    while (true)
    {
        buffer[0]=0;
        //cin >> s;
        int n = scanf("%s", buffer);

        if (n == -1)
        {
            // 设置错误码为EWOULDBLOCK
            cout << "errno: " << errno << "desc: " << strerror(errno) << endl;
        }
        else { cout << "刚刚获取的内容是" << buffer << "n: " << n << endl; }
        sleep(1);
    }
    return 0;
}
```

## *select*

<img src="非阻塞select多路复用.png">

### select 函数

系统提供 `select` 函数来实现多路复用I/O

* `select` 系统调用是用来让我们的程序监视**多个**文件描述符的状态变化的
* 程序会停在 `select` 这里等待，直到被监视的文件描述符有一个或多个发生了状态改变

```cpp
#include <sys/select.h>    
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

* 参数
  * `nfds`：要监视的最大的文件描述符+1
  
  * `readfds, writefds, exceptfds` 分别对应于需要检测的可读文件描述符的集合**位图**，可写文件描述符的集合及异常文件描述符的集合，都是输入输出型参数。输入输出用的是同一张位图表，所以会**覆盖式**地读写
    
    * 输入时：用户告诉OS，要关心哪些fd
    * 输出时：OS告诉用户，哪些fd已经就绪
    
  * 参数 `timeout` 为结构体 `timeval`，用来设置 `select()` 的等待时间
    
    ```c
    struct timeval
    {
        __time_t tv_sec;		/* Seconds.  */
        __suseconds_t tv_usec;	/* Microseconds.  */
    };
    ```
    
    * 阻塞式：`timeout=NULL`：表示 `select` 没有 `timeout`，`select` 将一直被阻塞直到某个文件描述符上发生了事件
    
    * 非阻塞式：`timeout={0, 0};`：仅检测描述符集合的状态，然后立即返回，不会等待外部事件发生
    
    * 特定的时间值：如果在指定的时间段里没有事件发生，`select` 将超时返回
    
    * 输出型参数：距离下一次timeout剩余多少时间
  
* 返回值
  * 执行成功则返回fd状态已改变的个数
  * 如果返回0代表在描述词状态改变前已超过 `timeout` 时间，没有返回
  * 当有错误发生时则返回-1，错误原因写入 `errno`，此时参数 `readfds`、`writefds`、`exceptfds` 和 `timeout` 的值变成不可预测

### 对 `fd_set` 的相关操作

`fd_set` 是一个位图，每一个bit对应一个fd是否就绪。它的大小一般是1024 bit

```cpp
typedef long int __fd_mask;
typedef struct
{
    __fd_mask fds_bits[__FD_SETSIZE / __NFDBITS];
# define __FDS_BITS(set) ((set)->fds_bits)
} fd_set;
```

不要自己去直接操作 `fd_set`，系统提供了一些接口函数来进行操作

```cpp
void FD_CLR(int fd, fd_set *set);
int  FD_ISSET(int fd, fd_set *set); 
void FD_SET(int fd, fd_set *set); //置位
void FD_ZERO(fd_set *set); //清空缓存
```

### select 编码模式

<img src="select_server.png" width="50%">

核心思路是**让select来做所有的判断和通知工作**，即告诉我们有没有socket来连接，也让socket告诉我们有没有数据到来，若到来了再去读数据。而不是自己去单独开一条线程阻塞式的去尝试read或recv可能到来的数据

* listenSock也是input IO，这里也可以用select一并等，不需要像以前阻塞的阻塞IO多线程单独Listen了
* `select` 之前要重置所有参数，之后要遍历所有的合法fd来检测事件
  * select的第一个参数nfds：随着获取的sock越来越多，添加到select里的sock也越来越多，所以select每一次轮询时nfds都会发生变化，需要对它动态更新
  * select的第2、3、4个参数：rfds/writefds/exceptfds 都是输入输出型参数，输出会直接把传入的输入覆盖掉，所以必须每一次都把合法的fd保存起来要更新位图

* 要维护第三方数组 `int _fd_array[1024]`，来保存过往的所有合法fd，方便 `select` 进行批量处理
* 一旦特定的fd事件就绪，本次读取或者写入不会被阻塞

### Pros and Cons

* Pros
  * 和多进程、多线程相比，占用资源少并且高效，因此单位时间内等的fd多了
  * 适用于有大量连接，但是只有少量是活跃的，节省资源

* Cons
  * 每一次都要进行大量的重置工作，效率比较低
  * 每一次能够检测的fd数量是有**上限**的
  * 因为每一个参数都是输入输出型的，每一次都需要内核到用户，用户到内核传递位图参数，有大量的数据拷贝
  * `select` 编码不方便，需要用户自己维护数组
  * `select` 底层需要***O(N)***同步遍历的方式，检测所有需要检测的fd，最大 `maxfd+1`

## *poll*

poll对select的改进是分离了输入输出参数，并且采用链表链接取消了能检测fd的上限

### 声明

```c
#include <poll.h>
int poll(struct pollfd *fds, nfds_t nfds, int timeout);

struct pollfd {
   int   fd;         /* file descriptor */
   short events;     /* requested events */
   short revents;    /* returned events */
};
```

* 参数

  * fds 是一个poll函数监听的结构体，里面是fd、监听的事件集合、返回的事件集合

    poll针对select的缺点做了改进，分离了输入输出型参数。其中events是给用户设定的，用户告诉OS要关心哪些事件；而revents是给OS来设定的，OS告诉用户哪些事件已经就绪了

    <img src="poll_parameters.png" width="70%">

  * nfds表示fds数组的长度

  * timeout表示poll函数的超时事件，单位是毫秒 ms。为0非阻塞模式，不为0阻塞模式

* 返回值

  * `<0` 出错
  * `=0` poll函数等待超时
  * `>0` poll由于监听的fd就绪而返回

### Pros and Cons

* Pros
  * 和select一样效率高，有大量连接，若只有少量的是活跃的可以节省资源
  * 输入输出资源分离，不需要进行大量的重置
  * poll参数级别，没有管理的fd上限

* Cons
  * poll依旧需要不少的遍历。在用户检测时间就绪，与内核检测fd就绪都需要便利。就这需要用户还是要维护第三户数组存储fd
  * poll还是需要内核到用户的拷贝

# epoll与reactor模式

## *epoll接口*

<img src="非阻塞epoll多路复用.png">

### intro

epoll是Linux下效率最高的IO时间通知机制

epoll是为处理大批量句柄而做了改进的poll，它是在Linux Kernel 2.5.44中被引进

### epoll_create

```c
 #include <sys/epoll.h>
int epoll_create(int size);
```

返回一个epoll的文件描述符。注意参数 `int size` 是被废弃的，可以随便写

### epoll_crl

```c
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
```

* 参数：它不同于 `select()` 是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型

  * epfd是 `epoll_create()` 的返回值（epoll的句柄）

  * op表示增删改动作，用三个宏来表示

    * EPOLL_CTL_ADD ：注册新的fd到epfd中
    * EPOLL_CTL_MOD ：修改已经注册的fd的监听事件
    * EPOLL_CTL_DEL ：从epfd中删除一个fd

  * fd是需要监听的fd

  * event结构体脂针是告诉内核需要监听什么

    ```c
    typedef union epoll_data {
       void        *ptr;
       int          fd;
       uint32_t     u32;
       uint64_t     u64;
    } epoll_data_t;
    
    //红黑树的一个node
    struct epoll_event {
       uint32_t     events;      /* Epoll events */
       epoll_data_t data;        /* User data variable */
    };
    ```

* 返回值

### epoll_wait

```c
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout)
```

收集在epoll监控的事件中已经就绪的事件

* 参数
  * events输出型参数是分配好的epoll_event结构体数组，epoll将会把发生的事件赋值到events数组中
  * maxevents告知内核这个events有多大，这个maxevents的值不能大于创建 `epoll_create()` 时的size
  * timeout是毫秒级的超时事件，0会立即返回非阻塞，-1是永久阻塞
* 返回已经就绪的文件描述符的个数，若返回0表示已超时，小于0表示出错

## *epoll的工作原理*

### epoll_create的时候发生了什么？

1. OS会构建eventpoll红黑树，epoll_ctl就是对这棵红黑树的增删改查
2. 构建底层回调机制 callback
3. 建立就绪队列，epoll_wait的功能就是去就绪队列里拿取就绪的事件节点

### epoll解决select和poll的两个缺陷

在select和poll里都需要自己来维护第三方数组，从而告诉OS要关心哪一些fd。epoll自己在底层维护了红黑树来记录用户要关心哪些fd

红黑树节点的key值是要关心的fd，用户只需要设置关系获取结果就可以，不用再关心任何对fd与event的管理细节

数据IO的原理：硬件会引起CPU某个引脚上的中断，然后通过对中断向量表关于该引脚的索引得到对应的执行函数（这些方法都是由驱动程序提供的），然后执行响应的动作。以数据到来要读为例，就是执行读取的动作

就绪队列解决了OS告诉用户哪些事件就绪了，上层只需要检测就绪队列就可以，因此是FIFO的队列，所以只需要***O(1)***检测队头就行了，不需要***O(N)***遍历

### callback的功能

1. 根据红黑树上节点要关心的事件，结合已经发生的事件来判断。比如说要关心EPOLLIN，然后有数据进来了，就说明就绪了
2. 自动根据fd和已经发生的事件，构建就绪节点
3. 自动将构建好的节点，插入到就绪队列中

生产者消费者模型：底层只要有fd就绪了OS会自动构建节点加入到就绪队列中，上层只需要不断的从就绪队列中将数据拿走，就完成了获取就绪事件中的任务。就绪队列是一个共享资源，epoll已经保证所有的epoll接口都是线程安全的

若底层没有就绪事件需要阻塞或轮询，所以要设置timeout，可以设置阻塞模式也可以设置非阻塞模式

## *epoll服务器*

## *reactor工作模式*

# C++

# Java的三种IO模型

Java中的BIO、NIO和AIO分别对应同步阻塞IO、同步非阻塞IO和异步IO。当然如之前所述，它们没有严格的谁替代谁的关系，只是应用场景不同。因为其简单易用性，大部分的IO仍然是阻塞式IO

下面会分别介绍Java中这三种IO应用到构建CS web的大概方法

## *BIO*

### BIO的CS框架

BIO 同步阻塞IO构建CS的方法是每当服务器有一个serverSock（或者说listenSock）用来监听client的连接请求（即发起并完成TCP3次握手）。每次新的连接到来就分一个新的线程出去处理这个client的各种任务。连接建立后**双方便阻塞式的读写**，若双方都没有读写那么这个线程资源就被阻塞住浪费了

虽然这种方式可以通过线程池和其他方式优化，但是若进来大量的高并发请求，线程池可能很快就被耗尽了

### Server

### Client

## *NIO*

### NIO的CS框架

NIO 同步非阻塞IO构建CS的方法本质上是单进程的串行执行。核心在于多路复用可以一次性

### Server

### Client

## *AIO*

### AIO的CS框架

### Server

### Client
