# Intro to BEAST Lab

## *BEAST*

https://www.lrz.de/presse/ereignisse/2020-11-06_BEAST/

> *Meet the BEAST at LRZ*
>
> *Researchers at LRZ are building the Bavarian Energy, Architecture, and Software Testbed in order to help proactively use and shape emerging technologies.*
>
> The Leibniz Supercomputing Centre (LRZ) is implementing the “Hightech Agenda” of the Bavarian State Government in its field of expertise and is launching the ambitious "Future Computing" program. Firstly, it is setting up a test environment with the latest computer technologies available on the market. Secondly, it is developing offers to train both staff members as well as the next generation of HPC scientists to exploit and explore new computer technologies and  high-performance computing (HPC) systems in collaboration with selected key scientific partners.
>
> "We want to intensively research the latest computer systems and architectures, their energy requirements, and mode of operation, without disturbing the services for our users at the production systems at LRZ," explains the  computer scientist Josef Weidendorfer, who heads the Future Computing group g at the LRZ. To his end, login and storage servers are already available in Garching, as well as two AMD Rome systems and two servers with Marvell ThunderX2 processors, each with graphics cards as accelerators. The centre will install a Cray CS500 system by mid-October, which uses the same Fujitsu A64FX processors as Fugaku, a Japanese system that is currently the world’s fastest supercomputer. Over the next few years, the test environment will be steadily expanded to include more systems and components. LRZ leadreship envisions this test environment as becoming a permanent fixture of the research work at the centre and it will also serve to evaluate new computer architectures for Bavaria's largest scientific computing center.
>
> ## Super-Tech at LRZ: Preparing for the next generation of high-performance computers
>
> "Bavarian Energy, Architecture, and Software Testbed" or  BEAST, is the name the LRZ has chosen for this innovative collection of computer and storage technologies that LRZ’s specialists are now putting through their paces.
>
> "BEAST serves as a rich environment to prepare for the next generation of supercomputers,” says Prof. Dr. Dieter Kranzlmüller, Director of LRZ. "We use it to investigate which computer architectures are suitable for larger systems and for parallelization. With the experience we gain through BEAST, we will be able to plan the successor to SuperMUC-NG and future services even better and more soundly."  In research, the amount of data that supercomputers have to handle  is currently growing. Applications such as machine learning and artificial intelligence also require new chip design, or even totally different  computer architectures.
>
> The needs posed by these emerging technologies have already begun influencing HPC systems, and in the near future, the need to help organize work or memory performance more efficiently will only grow. As a result, new ideas for computers are  needed, and HPC centres need to play an active role in research into the benefits of new technologies and architectures. In the long run, BEAST will therefore contain prototypes and the latest systems, which LRZ wants to further develop and build together with manufacturers and with key research partners. "BEAST is not a conventional LRZ service," Kranzlmüller continues. "However, joint development and co-design of new technology is pushing supercomputing and will ultimately pay off for science and society.
>
> ## Open for questions from researchers and students
>
> Although BEAST is not a service of  LRZ in the traditional sense, selected user groups will be given access to the systems. LRZ specialists will be the first users of these novel systems in order  to familiarize themselves with new computer systems and processors. Further, HPC software, such as the monitoring tool DCDB or the smart control system Wintermute, need to be adapted for  future systems. "With BEAST, we can prepare ourselves to offer modern, complex architectures and necessary software environments at service quality," says Weidendorfer. "The test environment also enables less goal-oriented and significantly more experimental research projects. We are therefore opening the test environment to selected researchers who are working on next-generation hardware". These researchers will be able to configure operating systems and hardware according to individual needs and modify them for their own applications. The LRZ will actively accompany and support this work and observe how hardware can be built and used more efficiently.
>
> ## Gather contacts and get to know innovative technology
>
> BEAST is interesting for young scientists and junior technical staff: LRZ, in collaboration with the two Munich universities, now offers an internship for computer science students interested in modern computer architectures and their energy-efficient use, allowing the next generation of HPC experts early exposure to emerging technologies that will likely play major roles in their professional careers. These internship opportunities aim to motivate students to make use of the latest computer technologies for bachelor or master theses.
>
> Participants will use BEAST to familiarize themselves with the technologies of the future and also make valuable contacts. They will regularly solve research questions and practical tasks on and with the new systems. In addition, technology companies will describe and contextualize new system designs and discuss them with young scientists. The BEAST internship will be coordinated by Dr. Karl Fürlinger (Ludwigs-Maximilians University Munich ), Dr. Weidendorfer, and the computer scientist Bengisu Elis (Technical Uuniversity of Munich).
>
> "I'm researching how to optimize communication in Graphic Processing Units (GPU),” Elis says. "With BEAST, I can compare combinations of GPU and CPU architectures from different vendors, and I can also test and improve the performance and portability of my code on different systems, BEAST offers valuable resources to increase the quality of my work, the idea that these can also contribute to improve the future systems of the LRZ is an additional incentive".

### 翻译

LRZ的研究人员正在建立Bavarian Energy, Architecture, and Software Testbed 测试平台，以积极地使用和塑造新兴技术。

LRZ正在在巴伐利亚州政府的 Hightech Agenda 下实施其专业领域的计划，并启动了雄心勃勃的 Future Computing 计划。首先，LRZ正在建立一个测试环境，其中包括市场上最新的计算机技术。其次，LRZ正在与选定的重要科学合作伙伴合作，开发培训方案，旨在培养员工和下一代HPC科学家，以开发和探索新的计算机技术和高性能计算（HPC）系统。

“我们希望深入研究最新的计算机系统和架构、它们的能源需求和运行模式，同时不干扰LRZ生产系统的用户服务。”，负责LRZ未来计算小组的计算机科学家Josef Weidendorfer解释道。为此，Garching已经提供了登录和存储服务器，以及两个AMD Rome系统和两个Marvell ThunderX2处理器的服务器，每个服务器都配备了图形卡作为加速器。该中心将于十月中旬安装一台Cray CS500系统，该系统使用与Fugaku 富岳相同的Fujitsu A64FX处理器，后者是目前全球最快的超级计算机。在未来几年内，测试环境将不断扩展，包括更多的系统和组件。LRZ的领导层将此测试环境视为中心研究工作的永久设施，并将用于评估巴伐利亚最大的科学计算中心的新计算机架构。 

**LRZ的超级技术：为下一代高性能计算做准备**

Bavarian Energy, Architecture, and Software Testbed 或 BEAST，这是LRZ为其专家们现在正在努力测试的计算机和存储技术集合选择的名字。

"BEAST是一个丰富的环境，用于为下一代超级计算机做准备，” LRZ主任Dieter Kranzlmüller教授说。 "我们用它来研究哪种计算机架构适合更大的系统和并行化。通过BEAST获得的经验，将使我们能够更好地更有力地规划SuperMUC-NG的继任者和未来的服务。" 在研究中，超级计算机必须处理的数据量目前正在增长。应用程序，如机器学习和人工智能，还需要新的芯片设计，甚至完全不同的计算机架构。

这些新兴技术提出的需求已经开始影响HPC系统，不久的将来，需要更有效地帮助组织工作或内存性能。因此，需要新的计算机构想法，而HPC中心需要在研究新技术和架构的好处方面发挥积极作用。从长远来看，BEAST将包含原型和最新的系统，LRZ希望能够进一步开发和与制造商以及重要的研究合作伙伴一起建设。 "BEAST不是LRZ的传统服务，"Kranzlmüller继续说道。 "然而，新技术的联合开发和共同设计推动了超级计算，并最终将为科学和社会付出回报。

**为研究人员和学生提供问题咨询**

尽管BEAST不是LRZ传统意义上的服务，但已经选择了一些用户组来访问这些系统。LRZ的专家将是这些新颖系统的第一批用户，以熟悉新的计算机系统和处理器。此外，HPC软件，如监控工具DCDB或智能控制系统Wintermute，需要为未来系统进行适应。"Weidendorfer表示，"借助BEAST，我们可以为自己提供现代、复杂的架构和必要的软件环境，以达到服务质量，"。"测试环境还能够进行不那么具体目标的、更为实验性的研究项目。因此，我们将测试环境开放给一些研究人员，他们正在研究下一代硬件。"这些研究人员将能够根据个人需求配置操作系统和硬件，并对其进行修改，以适应自己的应用程序。LRZ将积极陪同和支持这项工作，观察如何能更有效地构建和使用硬件。 

**建立联系和了解创新技术**

BEAST对年轻科学家和初级技术人员很有吸引力：LRZ现在与两所慕尼黑大学合作，为对现代计算机架构及其节能使用感兴趣的计算机科学学生提供实习机会，使下一代HPC专家早日接触新兴技术，这些技术可能在他们的职业生涯中扮演重要角色。这些实习机会旨在激励学生利用最新的计算机技术来撰写学士或硕士论文。 

参与者将利用BEAST熟悉未来技术，并建立宝贵的联系。他们将定期解决有关新系统的研究问题和实际任务。此外，技术公司将描述和背景化新的系统设计，并与年轻科学家讨论它们。BEAST实习将由Karl Fürlinger博士（LMU）、Weidendorfer博士以及计算机科学家Bengisu Elis（TUM）协调。 

"我正在研究如何优化图形处理单元（GPU）中的通信，"Elis说。"有了BEAST，我可以比较来自不同供应商的GPU和CPU架构的组合，还可以在不同系统上测试和改进我的代码的性能和可移植性，BEAST为提高我的工作质量提供了宝贵的资源，这些资源也可以有助于改进LRZ未来的系统，这是一个额外的激励。"

### Testbed

Testbed 测试床 指的是一个用于测试、评估和开发新硬件、软件、算法或应用程序的环境。HPC测试床通常用于以下目的：

1. **性能评估**：测试床用于评估HPC系统的性能，包括处理器、内存、存储、网络等组件的性能。这有助于研究人员和工程师了解系统的极限，并为优化工作负载做准备。
2. **新硬件和软件测试**：在HPC中，新的硬件、操作系统、编程工具或库经常被引入。测试床允许研究人员测试新技术，评估其性能，验证其可行性，并发现可能存在的问题。
3. **算法开发和测试**：研究人员和工程师使用HPC测试床来开发、优化和测试高性能算法。这包括并行计算、大规模数据处理、机器学习、数值模拟等方面的算法。
4. **应用程序开发**：在HPC环境中，应用程序的性能非常重要。测试床用于开发和测试HPC应用程序，以确保它们能够有效地利用HPC系统的硬件资源。
5. **安全性和可靠性测试**：测试床还可以用于评估HPC系统的安全性和可靠性，包括对潜在威胁的测试和系统的容错性。

HPC测试床通常由一组HPC集群、服务器、网络设备和专门的性能监测工具组成。这些测试床可能模拟大规模HPC系统的特性，允许研究人员进行实际测试和评估，而不必在生产环境中引入风险。

### Brief intro of SuperMUC-NG

## *Hardware of BEAST*

### Intel Icelake

Xeon系列的微架构演进依次为：Nehalem 2008 `->` Sandy Bridge 2011 `->` Ivy Bridge 2012 `->` Haswell 2013 `->` Broadwell 2014 `->` Skylake 2015 `->` Cascade Lake 2019 `->` Ice Lake 2019 `->` Sapphire Rapids 2023

Intel Xeon (Icelake) Platinum 8360Y: https://www.intel.cn/content/www/cn/zh/products/sku/212459/intel-xeon-platinum-8360y-processor-54m-cache-2-40-ghz/specifications.html

通过 `lscpu` 来查询一下cpu信息

```
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   52 bits physical, 57 bits virtual
CPU(s):                          144
On-line CPU(s) list:             0-143
Thread(s) per core:              2
Core(s) per socket:              36
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           106
Model name:                      Intel(R) Xeon(R) Platinum 8360Y CPU @ 2.40GHz
Stepping:                        6
Frequency boost:                 enabled
CPU MHz:                         1847.196
CPU max MHz:                     2401.0000
CPU min MHz:                     800.0000
BogoMIPS:                        4800.00
Virtualization:                  VT-x
L1d cache:                       3.4 MiB
L1i cache:                       2.3 MiB
L2 cache:                        90 MiB
L3 cache:                        108 MiB
NUMA node0 CPU(s):               0-35,72-107
NUMA node1 CPU(s):               36-71,108-143
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_ts
                                 c cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_f
                                 ault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx
                                 512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_
                                 epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
```

### AMD Milan-X

Zen架构代表了AMD Ryzen 霄龙系列处理器的微架构：AMD Epyc是AMD推出的x86架构服务器微处理器产品线，采用Zen微架构。于2017年6月发表并开始供货，取代推出已有14年历史的Opteron系列。

第一代霄龙处理器 Naples 基于Zen微架构 `->` 第二代霄龙处理器 Rome 基于Zen2微架构 `->` 第三代霄龙处理器 Milan 基于Zen3微架构 `->` 第四代霄龙处理器 Genoa, Bergamo and Siena 基于Zen4微架构

EPYC 7773X: https://www.amd.com/zh-hans/products/cpu/amd-epyc-7773x

### Marvell ThunderX2

Marvell Technology Group 是一家总部位于美国加利福尼亚州圣克拉拉的半导体公司。Marvell成立于1995年，是一家全球领先的半导体解决方案提供商，专注于开发和制造高性能处理器、控制器、存储解决方案、网络连接性产品和其他半导体设备。

Marvell ThunderX 是Marvell公司的一个服务器处理器系列，专为数据中心和云计算市场设计。ThunderX系列处理器采用ARM架构，以提供高性能、能效和多核处理能力，以满足大规模服务器和高性能计算需求。

ThunderX2 CN9980采用AArch64 ISA和Vulcan 微架构。

### Fujitsu A64FX

Fujitsu A64FX是一款高性能、面向科学计算和高性能计算的ARM架构处理器

### Nvidia V100

### AMD MI-100

## *软件*

### 编译器

GCC在每一种CPU系统上都有

介绍一下其他的编译器

* Intel oneAPI C/C++ Compiler (only on ice and milan)

  命令为 `icx/icpx`，可以使用 `man icc` 来查询手册

  Icx是由Intel开发的C++编译器。它是Intel工具套件中的一部分，通常与Intel的开发工具和库一起使用，用于编译和优化C++代码

* Cray C/C++ Compiler

  命令为 `cc/CC`，可以使用 `man craycc/man crayCC` 来查询手册



# Assignment 1

## *Vector Triad Microbenchmark*

```c++
#include <assert.h>
#include <errno.h>
#include <omp.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

// pointers for vectors
static double *restrict A, *restrict B, *restrict C, *restrict D;

bool verify(int N)
{
    for (int i = 0; i < N; ++i) {
        if (A[i] != 7.0)
            return false;
    }
    return true;
}

double triad(int N, int REP)
{
    double begin = omp_get_wtime();
#pragma omp parallel
    {
        for (int r = 0; r < REP; ++r)
#pragma omp for schedule(static) nowait
            for (int i = 0; i < N; ++i)
                A[i] = B[i] + C[i] * D[i];
    }
    return omp_get_wtime() - begin;
}

void initialize_vectors(int N)
{
#pragma omp parallel for schedule(static)
    for (int i = 0; i < N; ++i) {
        A[i] = 0.0;
        B[i] = 1.0;
        C[i] = 2.0;
        D[i] = 3.0;
    }
}

void allocate_vectors(size_t N, bool aligned)
{
    size_t CACHE_LINESIZE = sysconf(_SC_LEVEL1_DCACHE_LINESIZE);

    if (aligned) {
        A = aligned_alloc(CACHE_LINESIZE, N * sizeof(double));
        B = aligned_alloc(CACHE_LINESIZE, N * sizeof(double));
        C = aligned_alloc(CACHE_LINESIZE, N * sizeof(double));
        D = aligned_alloc(CACHE_LINESIZE, N * sizeof(double));
    } else {
        A = malloc(N * sizeof(double));
        B = malloc(N * sizeof(double));
        C = malloc(N * sizeof(double));
        D = malloc(N * sizeof(double));
    }

    if(!A || !B || !C || !D) {
        perror("malloc");
        exit(EXIT_FAILURE);
    }
}

void free_vectors(void)
{
    free(A);
    free(B);
    free(C);
    free(D);
}

int main(int argc, char *argv[])
{
    size_t N_max;
    size_t N_min = 128;

    // min. measurement time in seconds
    double min_duration = 1.0;
    bool   aligned      = true;

    if (argc < 2 || argc > 3) {
        fprintf(stderr, "Usage: %s <N> [aligned]\n", argv[0]);
        fprintf(stderr, "    N         : maximum vector length (2^N_max)\n");
        fprintf(stderr, "    aligned=0 : use unaligned memory allocation\n");
        exit(EXIT_FAILURE);
    }

    char *pEnd;
    N_max = 1ULL << strtol(argv[1], &pEnd, 10);
    if (errno || N_max > __INT_MAX__ || N_max < N_min) {
        fprintf(stderr, "N invalid / out of range\n");
        exit(EXIT_FAILURE);
    }

    if (argc == 3) {
        aligned = atoi(argv[2]);
    }

    fprintf(stderr, "N_max = %zu %s\n", N_max, aligned ? "" : "(unaligned memory)");

    printf("%s,%s,%s,%s\n", "N", "GFLOPS", "Repetitions", "Threads");

    int num_threads = omp_get_max_threads();

    for (size_t N = N_min; N <= N_max; N *= 2) {
        allocate_vectors(N, aligned);
        initialize_vectors(N);

        size_t rep = 1;
        // warm-up and set number of repetitions
        while (triad(N, rep) < min_duration / 2) {
            rep *= 2;
        }

        // actual measurement
        double time = triad(N, rep);

        assert(verify(N));
        free_vectors();

        // flop calculation
        double num_flops = 2.0 * N * rep;
        double gflops    = (num_flops / time) * 1.0e-9;

        printf("%zu,%.2f,%zu,%d\n", N, gflops, rep, num_threads);
    }
    return EXIT_SUCCESS;
}
```

### REP 的作用

* **重复执行以增加准确性**：在性能测试中，通常希望测量的操作能持续足够长的时间，以便能够准确地计算出执行时间。如果操作执行得太快，计时器的精度可能不足以提供准确的读数。通过重复执行相同的操作（在这个案例中是数组操作 `A[i] = B[i] + C[i] * D[i]`），可以延长总的执行时间，从而获得更精确的测量
* **动态确定重复次数**：在 main 函数中，有一段代码专门用来确定合适的 REP 值。代码的逻辑是这样的：首先以一个很小的重复次数开始，然后不断增加重复次数，直到 `triad` 函数执行的时间达到或超过预设的最小持续时间的一半（这里是 `min_duration / 2`）。这样做的目的是为了确保在进行实际的性能测量时，每次测量都有足够长的执行时间，从而提高准确性

### 计算 GFLOPS & Bandwidth

假设triad用到的数组数据都是double，一次数组操作 `A[i] = B[i] + C[i] * D[i]` 总共需要2次浮点数操作和4次内存访问，即 `16 Bytes/flop`

当然 `16 Bytes/flop` 只是平均值，可能会因为read miss、特殊的instruction而产生变化

所以计算bandwidth就是 `#FLOPS * 16 Bytes/flop`

## *研究一下parallel construct*

run命令：`srun OMP_NUM_THREADS=64 OMP_PROC_BIND=spread ./triad-ice1 26 1`

`#pragma omp parallel` 的作用是创建并行区域，把同样的代码分配给不同的线程

而 `#pragma omp for` 的作用则是拆分一个for循环，把里面不同的iterations分配给不同的线程（若并行化了）

### 原版

```c++
double triad(int N, int REP)
{
    double begin = omp_get_wtime();
#pragma omp parallel
    {
        for (int r = 0; r < REP; ++r)
#pragma omp for schedule(static) nowait
            for (int i = 0; i < N; ++i)
                A[i] = B[i] + C[i] * D[i];
    }
    return omp_get_wtime() - begin;
}
```

为什么第二个for后面不用加 {}？

`for (int r = 0; r < REP; ++r)` 后面紧跟着的是 `#pragma omp for schedule(static) nowait` 和内层的 `for` 循环。由于 `#pragma` 指令和内层的 `for` 循环被视为一体，因此外层 `for` 循环实际上只包含了这个复合语句。因此，从语法上来说，这种情况下外层 `for` 循环也不需要大括号。

然而，这样的代码可能会使阅读和理解变得有些困难，特别是对于不熟悉OpenMP指令的人来说。为了提高代码的清晰度，最好还是通过添加大括号明确地表示出哪些代码属于外层循环

```csv
N,        GFLOPS,  Repetitions,  Threads
128,      15.85,   33554432,     64
256,      141.42,  268435456,    64
512,      323.92,  268435456,    64
1024,     497.15,  134217728,    64
2048,     531.48,  67108864,     64
4096,     624.71,  67108864,     64
8192,     830.01,  33554432,     64
16384,    894.73,  16777216,     64
32768,    888.11,  8388608,      64
65536,    828.92,  4194304,      64
131072,   401.26,  1048576,      64
262144,   371.55,  524288,       64
524288,   374.24,  262144,       64
1048576,  371.17,  131072,       64
2097152,  281.56,  65536,        64
4194304,  40.70,   4096,         64
8388608,  19.09,   1024,         64
16777216, 16.02,   256,          64
33554432, 9.16,    128,          64
67108864, 7.98,    32,           64
```

### 同时并行化外部和内部循环

```c++
double triad(int N, int REP)
{
    double begin = omp_get_wtime();
#pragma omp parallel for schedule(static)
        for (int r = 0; r < REP; ++r) {
            for (int i = 0; i < N; ++i)
                A[i] = B[i] + C[i] * D[i];
        }
    return omp_get_wtime() - begin;
}
```

为什么不可以加 {} 和 nowait？

```csv
N,        GFLOPS,  Repetitions,  Threads
128,      1.90,    4194304,      64
256,      2.69,    4194304,      64
512,      2.94,    2097152,      64
1024,     3.09,    1048576,      64
2048,     4.13,    524288,       64
4096,     1.98,    524288,       64
8192,     4.65,    262144,       64
16384,    4.64,    131072,       64
32768,    4.89,    65536,        64
65536,    5.38,    32768,        64
131072,   6.00,    16384,        64
262144,   7.29,    8192,         64
524288,   8.32,    4096,         64
1048576,  10.00,   4096,         64
2097152,  11.59,   2048,         64
4194304,  18.78,   2048,         64
8388608,  26.72,   1024,         64
16777216, 21.14,   512,          64
33554432, 27.20,   256,          64
67108864, 26.51,   128,          64
```

### f

```c++
double triad(int N, int REP)
{
    double begin = omp_get_wtime();
#pragma omp parallel
{
        for (int r = 0; r < REP; ++r) {
            for (int i = 0; i < N; ++i)
                A[i] = B[i] + C[i] * D[i];
        }
}
    return omp_get_wtime() - begin;
}
```





```c++
double triad(int N, int REP)
{
    double begin = omp_get_wtime();
    for (int r = 0; r < REP; ++r)
#pragma omp parallel for schedule(static) 
        for (int i = 0; i < N; ++i)
            A[i] = B[i] + C[i] * D[i];
    return omp_get_wtime() - begin;
}
```

```csv
N,        GFLOPS,  Repetitions,  Threads
128,      0.02,    65536,        64
256,      0.04,    65536,        64
512,      0.08,    65536,        64
1024,     0.16,    65536,        64
2048,     0.32,    65536,        64
4096,     0.65,    65536,        64
8192,     1.30,    65536,        64
16384,    2.60,    65536,        64
32768,    5.17,    65536,        64
65536,    10.33,   65536,        64
131072,   20.35,   65536,        64
262144,   39.16,   65536,        64
524288,   71.85,   65536,        64
1048576,  124.87,  32768,        64
2097152,  189.80,  32768,        64
4194304,  36.94,   4096,         64
8388608,  19.31,   1024,         64
16777216, 15.35,   256,          64
33554432, 14.05,   128,          64
67108864, 14.18,   64,           64
```



```c++
double triad(int N, int REP)
{
    double begin = omp_get_wtime();
    for (int r = 0; r < REP; ++r)
#pragma omp parallel
{
        for (int i = 0; i < N; ++i)
            A[i] = B[i] + C[i] * D[i];
}
    return omp_get_wtime() - begin;
}
```





## *Experiments and Measurements*

### Sequential Performance

### Parallel Performance





Cycling time：什么时候可以进行下一次的寻址



Read的延迟很大、电荷会泄漏，需要不断刷新



50% miss rate -> filter



L1和L2的管理算法是不同的



可以只利用time locality或只利用spatial locality，或者两者都利用



Tag 是一种Metadata

是并行还是顺序的获取Tag和Data



# Assignment 2

### GPU监控

Check GPU utilization `nvidia-smi -l 1` each second update 或者 `watch -n 1 nvidia-smi`

* GPU：本机中的GPU编号（有多块显卡的时候，从0开始编号）图上GPU的编号是：0
* Fan：风扇转速（0%-100%），N/A表示没有风扇
* Name：GPU类型，图上GPU的类型是：Tesla T4
* Temp：GPU的温度（GPU温度过高会导致GPU的频率下降）
* Perf：GPU的性能状态，从P0（最大性能）到P12（最小性能），图上是：P0
* Persistence-M：持续模式的状态，持续模式虽然耗能大，但是在新的GPU应用启动时花费的时间更少，图上显示的是：off
* Pwr：Usager/Cap：能耗表示，Usage：用了多少，Cap总共多少
* Bus-Id：GPU总线相关显示，domain：bus：device.function
* Disp.A：Display Active ，表示GPU的显示是否初始化
* Memory-Usage：显存使用率
* Volatile GPU-Util：GPU使用率
* Uncorr. ECC：关于ECC的东西，是否开启错误检查和纠正技术，0/disabled,1/enabled
* Compute M：计算模式，0/DEFAULT,1/EXCLUSIVE_PROCESS,2/PROHIBITED
* Processes：显示每个进程占用的显存使用率、进程号、占用的哪个GPU 显存占用和GPU占用是两个不一样的东西，显卡是由GPU和显存等组成的，显存和GPU的关系有点类似于内存和CPU的关系。跑caffe代码的时候显存占得少，GPU占得多；跑TensorFlow代码的时候，显存占得多，GPU占得少

# Assignment 3

## *性能分析相关概念*

1. 事件（Events）：事件是在处理器中发生的特定操作或情况，如缓存命中、缓存不命中、分支预测错误、指令执行等。这些事件可以用于分析程序或工作负载的性能特征。
2. **性能计数器（Performance Counters）**：性能计数器是硬件寄存器，用于记录特定事件的数量。每个性能计数器通常与一个特定的事件相关联。通过读取这些计数器的值，在程序运行期间或之后，可以分析性能并评估处理器的行为。
3. **寄存器（Registers）**：在CPU或系统芯片中，有一组特殊寄存器用于存储性能事件和计数器的配置信息以及计数器的当前值。这些寄存器通常由操作系统、性能分析工具或用户空间程序来配置和访问。
4. **性能监测工具（Performance Monitoring Tools）**：性能监测工具是用于配置、启动性能计数器并分析计数器数据的软件工具。这些工具可以提供详细的性能分析信息，帮助开发人员和系统管理员识别性能瓶颈和优化机会。





## *Perf*

http://www.yunweipai.com/43686.html

### Perf event

Perf event 是perf工具的基础，代表了一个特定的性能度量。事件可以是内核、硬件或用户级应用程序产生的。这些事件可以用于监控、统计和剖析各种性能指标

* Hardware Event由Power Management Unit, PMU部件产生，在特定的条件下探测性能事件是否发生以及发生的次数。比如cache命中
* Software Event是内核产生的事件，分布在各个功能模块中，统计和操作系统相关性能事件。比如进程切换，tick数等
* Tracepoint Event是内核中静态tracepoint所触发的事件，这些tracepoint用来判断程序运行期间内核的行为细节，比如slab分配器的分配次数等

### perf record

`perf record` 命令用于收集指定事件的性能数据，并将其保存在文件中以便后续分析。默认数据将保存在名为 perf.data 的文件中

```shell
$ perf record [options] [command]
```

* -e 或 –event：指定要记录的事件类型，例如cache-misses, instructions等
* -p 或 –pid：指定要监控的进程ID
* -t 或 –tid：指定要监控的线程ID
* -a 或 –all-cpus：监控所有CPU，而不仅仅是当前CPU
* -C 或 –cpu：指定要监控的CPU列表
* -f 或 –overwrite：以覆盖模式记录事件
* -c 或 –count：设置每个事件的采样周期
* -r 或 –real-time：设置实时优先级
* -o 或 –output：指定要将数据写入的文件
* -g 或 –call-graph：指定调用图记录方法，例如dwarf或fp（帧指针）
* –switch-events：记录上下文切换事件
* –no-buffering：禁用数据缓冲
* –dry-run：显示要执行的操作，但不实际执行

### perf report

perf report 命令从 perf.data 文件中读取性能数据，并以多种格式展示分析结果。用户可以根据需要自定义报告的输出格式

```shell
$ perf report [options]
```

* -i 或 –input：指定要读取的输入文件，默认为 perf.data
* -F 或 –fields：指定要显示的字段，例如：comm, dso, symbol 等
* –sort：指定排序顺序，例如：dso,symbol 或 symbol,dso
* –show-total-period：显示每个符号的总周期数
* -T 或 –threads：显示线程相关数据
* -m 或 –modules：显示模块（共享库）相关数据
* -k 或 –vmlinux：指定内核符号表文件（vmlinux）的路径
* -f 或 –force：强制解析文件，即使它看起来无效或损坏
* -c 或 –comms：指定要显示的命令（进程）列表
* –dsos：指定要显示的动态共享对象（DSO）列表
* -s 或 –symbols：指定要显示的符号（函数）列表
* –percent-limit：仅显示超过指定百分比的项
* -P 或 –pretty：指定输出格式，如raw、normal等
* –stdio：以文本模式显示报告（而非 TUI 模式）
* –tui：以 TUI 模式显示报告（默认方式）
* –gtk：以 GTK 模式显示报告
* -g 或 –call-graph：显示调用图数据
* –no-children：仅显示独立样本，不显示调用子函数的样本
* –no-demangle：禁用 C++ 符号解析
* –demangle：指定 C++ 符号解析方式，如：no, normal, smart 等
* –filter：指定过滤器，如：–filter ‘dso(/lib*)’
* –max-stack：指定栈帧的最大数量

## *likwid*

https://github.com/RRZE-HPC/likwid/wiki/

likwid, like I knew what I am doing. 是一组命令行性能分析系列工具的集合，用于针对多线程程序的指标分析

* likwid-topology：显示线程和缓存拓扑
* likwid-perfctr：在Intel、ARM、Power处理器上测量硬件性能计数寄存器
* likwid-features：显示并切换Intel Core 2处理器上的硬件预取控制位
* likwid-pin：在不修改代码的情况下为多线程应用程序设置绑定（支持pthread、Intel OpenMP和gcc OpenMP）
* likwid-bench：允许快速原型化线程汇编内核的基准测试框架
* likwid-mpirun：脚本，可简化MPI和MPI/多线程混合应用程序的绑定设置
* likwid-perfscope：likwid-perfctr时间轴模式的前端。允许实时绘制性能指标
* likwid-powermeter：用于访问Intel处理器上的RAPL计数器并查询Turbo模式步骤的工具
* likwid-memsweeper：用于清理ccNUMA内存域并强制从缓存中驱逐脏缓存行的工具
* likwid-setFrequencies：用于设置特定处理器频率的工具

### Performance Group

Performance group 指的是一组计算机中测量用的性能计数寄存器

```
$ likwid-perfctr -a
    Group name	Description
--------------------------------------------------------------------------------
           UPI	UPI data traffic 用于测量处理器之间的数据通信流量，特别是用于UPI的数据流量。
          DATA	Load to store ratio 用于测量加载和存储操作之间的比率，有助于了解内存访问模式。
        ENERGY	Power and Energy consumption 用于测量系统的功耗和能源消耗情况，帮助评估能源效率。
        BRANCH	Branch prediction miss rate/ratio 用于测量分支预测错误的频率，这对于评估分支预测性能很有帮助。
           TMA	Top down cycle allocation 用于测量处理器周期的分配情况，以了解性能瓶颈。
           MEM	Memory bandwidth in MBytes/s 用于测量内存带宽，即从主存读取和写入数据的速度。
        MEM_DP	Overview of arithmetic and main memory performance 提供有关算术运算和主存性能的总体概述。
            L2	L2 cache bandwidth in MBytes/s 用于测量L2缓存的带宽，即L2缓存的读取和写入速度。
  CYCLE_STALLS	Cycle Activities (Stalls) 用于测量处理器周期内不同类型的停顿（stalls）活动。
        MEM_SP	Overview of arithmetic and main memory performance 提供有关算术运算和主存性能的总体概述，针对单精度操作。
            L3	L3 cache bandwidth in MBytes/s 用于测量L3缓存的带宽，即L3缓存的读取和写入速度。
        DIVIDE	Divide unit information 提供有关除法单元性能和信息的计数。
   MEM_FREERUN	Memory bandwidth in MBytes/s 用于测量内存带宽，与之前的 "MEM" 类似。
CYCLE_ACTIVITY	Cycle Activities 用于测量不同类型的处理器周期活动。
     FLOPS_AVX	Packed AVX MFLOP/s 用于测量AVX（Advanced Vector Extensions）指令集的每秒浮点操作（FLOP）。
       L2CACHE	L2 cache miss rate/ratio 用于测量L2缓存的缺失率或比率。
      FLOPS_DP	Double Precision MFLOP/s 用于测量双精度浮点操作的每秒执行次数。
         CLOCK	Power and Energy consumption 用于测量处理器时钟频率、功耗和能源消耗。
      FLOPS_SP	Single Precision MFLOP/s 用于测量单精度浮点操作的每秒执行次数。
```
