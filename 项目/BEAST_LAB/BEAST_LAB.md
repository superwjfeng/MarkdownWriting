# Intro to BEAST Lab

## *BEAST*

https://www.lrz.de/presse/ereignisse/2020-11-06_BEAST/

> *Meet the BEAST at LRZ*
>
> *Researchers at LRZ are building the Bavarian Energy, Architecture, and Software Testbed in order to help proactively use and shape emerging technologies.*
>
> The Leibniz Supercomputing Centre (LRZ) is implementing the “Hightech Agenda” of the Bavarian State Government in its field of expertise and is launching the ambitious "Future Computing" program. Firstly, it is setting up a test environment with the latest computer technologies available on the market. Secondly, it is developing offers to train both staff members as well as the next generation of HPC scientists to exploit and explore new computer technologies and  high-performance computing (HPC) systems in collaboration with selected key scientific partners.
>
> "We want to intensively research the latest computer systems and architectures, their energy requirements, and mode of operation, without disturbing the services for our users at the production systems at LRZ," explains the  computer scientist Josef Weidendorfer, who heads the Future Computing group g at the LRZ. To his end, login and storage servers are already available in Garching, as well as two AMD Rome systems and two servers with Marvell ThunderX2 processors, each with graphics cards as accelerators. The centre will install a Cray CS500 system by mid-October, which uses the same Fujitsu A64FX processors as Fugaku, a Japanese system that is currently the world’s fastest supercomputer. Over the next few years, the test environment will be steadily expanded to include more systems and components. LRZ leadreship envisions this test environment as becoming a permanent fixture of the research work at the centre and it will also serve to evaluate new computer architectures for Bavaria's largest scientific computing center.
>
> ## Super-Tech at LRZ: Preparing for the next generation of high-performance computers
>
> "Bavarian Energy, Architecture, and Software Testbed" or  BEAST, is the name the LRZ has chosen for this innovative collection of computer and storage technologies that LRZ’s specialists are now putting through their paces.
>
> "BEAST serves as a rich environment to prepare for the next generation of supercomputers,” says Prof. Dr. Dieter Kranzlmüller, Director of LRZ. "We use it to investigate which computer architectures are suitable for larger systems and for parallelization. With the experience we gain through BEAST, we will be able to plan the successor to SuperMUC-NG and future services even better and more soundly."  In research, the amount of data that supercomputers have to handle  is currently growing. Applications such as machine learning and artificial intelligence also require new chip design, or even totally different  computer architectures.
>
> The needs posed by these emerging technologies have already begun influencing HPC systems, and in the near future, the need to help organize work or memory performance more efficiently will only grow. As a result, new ideas for computers are  needed, and HPC centres need to play an active role in research into the benefits of new technologies and architectures. In the long run, BEAST will therefore contain prototypes and the latest systems, which LRZ wants to further develop and build together with manufacturers and with key research partners. "BEAST is not a conventional LRZ service," Kranzlmüller continues. "However, joint development and co-design of new technology is pushing supercomputing and will ultimately pay off for science and society.
>
> ## Open for questions from researchers and students
>
> Although BEAST is not a service of  LRZ in the traditional sense, selected user groups will be given access to the systems. LRZ specialists will be the first users of these novel systems in order  to familiarize themselves with new computer systems and processors. Further, HPC software, such as the monitoring tool DCDB or the smart control system Wintermute, need to be adapted for  future systems. "With BEAST, we can prepare ourselves to offer modern, complex architectures and necessary software environments at service quality," says Weidendorfer. "The test environment also enables less goal-oriented and significantly more experimental research projects. We are therefore opening the test environment to selected researchers who are working on next-generation hardware". These researchers will be able to configure operating systems and hardware according to individual needs and modify them for their own applications. The LRZ will actively accompany and support this work and observe how hardware can be built and used more efficiently.
>
> ## Gather contacts and get to know innovative technology
>
> BEAST is interesting for young scientists and junior technical staff: LRZ, in collaboration with the two Munich universities, now offers an internship for computer science students interested in modern computer architectures and their energy-efficient use, allowing the next generation of HPC experts early exposure to emerging technologies that will likely play major roles in their professional careers. These internship opportunities aim to motivate students to make use of the latest computer technologies for bachelor or master theses.
>
> Participants will use BEAST to familiarize themselves with the technologies of the future and also make valuable contacts. They will regularly solve research questions and practical tasks on and with the new systems. In addition, technology companies will describe and contextualize new system designs and discuss them with young scientists. The BEAST internship will be coordinated by Dr. Karl Fürlinger (Ludwigs-Maximilians University Munich ), Dr. Weidendorfer, and the computer scientist Bengisu Elis (Technical Uuniversity of Munich).
>
> "I'm researching how to optimize communication in Graphic Processing Units (GPU),” Elis says. "With BEAST, I can compare combinations of GPU and CPU architectures from different vendors, and I can also test and improve the performance and portability of my code on different systems, BEAST offers valuable resources to increase the quality of my work, the idea that these can also contribute to improve the future systems of the LRZ is an additional incentive".

### 翻译

LRZ的研究人员正在建立Bavarian Energy, Architecture, and Software Testbed 测试平台，以积极地使用和塑造新兴技术。

LRZ正在在巴伐利亚州政府的 Hightech Agenda 下实施其专业领域的计划，并启动了雄心勃勃的 Future Computing 计划。首先，LRZ正在建立一个测试环境，其中包括市场上最新的计算机技术。其次，LRZ正在与选定的重要科学合作伙伴合作，开发培训方案，旨在培养员工和下一代HPC科学家，以开发和探索新的计算机技术和高性能计算（HPC）系统。

“我们希望深入研究最新的计算机系统和架构、它们的能源需求和运行模式，同时不干扰LRZ生产系统的用户服务。”，负责LRZ未来计算小组的计算机科学家Josef Weidendorfer解释道。为此，Garching已经提供了登录和存储服务器，以及两个AMD Rome系统和两个Marvell ThunderX2处理器的服务器，每个服务器都配备了图形卡作为加速器。该中心将于十月中旬安装一台Cray CS500系统，该系统使用与Fugaku 富岳相同的Fujitsu A64FX处理器，后者是目前全球最快的超级计算机。在未来几年内，测试环境将不断扩展，包括更多的系统和组件。LRZ的领导层将此测试环境视为中心研究工作的永久设施，并将用于评估巴伐利亚最大的科学计算中心的新计算机架构。 

**LRZ的超级技术：为下一代高性能计算做准备**

Bavarian Energy, Architecture, and Software Testbed 或 BEAST，这是LRZ为其专家们现在正在努力测试的计算机和存储技术集合选择的名字。

"BEAST是一个丰富的环境，用于为下一代超级计算机做准备，” LRZ主任Dieter Kranzlmüller教授说。 "我们用它来研究哪种计算机架构适合更大的系统和并行化。通过BEAST获得的经验，将使我们能够更好地更有力地规划SuperMUC-NG的继任者和未来的服务。" 在研究中，超级计算机必须处理的数据量目前正在增长。应用程序，如机器学习和人工智能，还需要新的芯片设计，甚至完全不同的计算机架构。

这些新兴技术提出的需求已经开始影响HPC系统，不久的将来，需要更有效地帮助组织工作或内存性能。因此，需要新的计算机构想法，而HPC中心需要在研究新技术和架构的好处方面发挥积极作用。从长远来看，BEAST将包含原型和最新的系统，LRZ希望能够进一步开发和与制造商以及重要的研究合作伙伴一起建设。 "BEAST不是LRZ的传统服务，"Kranzlmüller继续说道。 "然而，新技术的联合开发和共同设计推动了超级计算，并最终将为科学和社会付出回报。

**为研究人员和学生提供问题咨询**

尽管BEAST不是LRZ传统意义上的服务，但已经选择了一些用户组来访问这些系统。LRZ的专家将是这些新颖系统的第一批用户，以熟悉新的计算机系统和处理器。此外，HPC软件，如监控工具DCDB或智能控制系统Wintermute，需要为未来系统进行适应。"Weidendorfer表示，"借助BEAST，我们可以为自己提供现代、复杂的架构和必要的软件环境，以达到服务质量，"。"测试环境还能够进行不那么具体目标的、更为实验性的研究项目。因此，我们将测试环境开放给一些研究人员，他们正在研究下一代硬件。"这些研究人员将能够根据个人需求配置操作系统和硬件，并对其进行修改，以适应自己的应用程序。LRZ将积极陪同和支持这项工作，观察如何能更有效地构建和使用硬件。 

**建立联系和了解创新技术**

BEAST对年轻科学家和初级技术人员很有吸引力：LRZ现在与两所慕尼黑大学合作，为对现代计算机架构及其节能使用感兴趣的计算机科学学生提供实习机会，使下一代HPC专家早日接触新兴技术，这些技术可能在他们的职业生涯中扮演重要角色。这些实习机会旨在激励学生利用最新的计算机技术来撰写学士或硕士论文。 

参与者将利用BEAST熟悉未来技术，并建立宝贵的联系。他们将定期解决有关新系统的研究问题和实际任务。此外，技术公司将描述和背景化新的系统设计，并与年轻科学家讨论它们。BEAST实习将由Karl Fürlinger博士（LMU）、Weidendorfer博士以及计算机科学家Bengisu Elis（TUM）协调。 

"我正在研究如何优化图形处理单元（GPU）中的通信，"Elis说。"有了BEAST，我可以比较来自不同供应商的GPU和CPU架构的组合，还可以在不同系统上测试和改进我的代码的性能和可移植性，BEAST为提高我的工作质量提供了宝贵的资源，这些资源也可以有助于改进LRZ未来的系统，这是一个额外的激励。"

### Testbed

Testbed 测试床 指的是一个用于测试、评估和开发新硬件、软件、算法或应用程序的环境。HPC测试床通常用于以下目的：

1. **性能评估**：测试床用于评估HPC系统的性能，包括处理器、内存、存储、网络等组件的性能。这有助于研究人员和工程师了解系统的极限，并为优化工作负载做准备。
2. **新硬件和软件测试**：在HPC中，新的硬件、操作系统、编程工具或库经常被引入。测试床允许研究人员测试新技术，评估其性能，验证其可行性，并发现可能存在的问题。
3. **算法开发和测试**：研究人员和工程师使用HPC测试床来开发、优化和测试高性能算法。这包括并行计算、大规模数据处理、机器学习、数值模拟等方面的算法。
4. **应用程序开发**：在HPC环境中，应用程序的性能非常重要。测试床用于开发和测试HPC应用程序，以确保它们能够有效地利用HPC系统的硬件资源。
5. **安全性和可靠性测试**：测试床还可以用于评估HPC系统的安全性和可靠性，包括对潜在威胁的测试和系统的容错性。

HPC测试床通常由一组HPC集群、服务器、网络设备和专门的性能监测工具组成。这些测试床可能模拟大规模HPC系统的特性，允许研究人员进行实际测试和评估，而不必在生产环境中引入风险。

### Brief intro of SuperMUC-NG

## *Hardware of BEAST*

### Intel Icelake

Xeon系列的微架构演进依次为：Nehalem 2008 `->` Sandy Bridge 2011 `->` Ivy Bridge 2012 `->` Haswell 2013 `->` Broadwell 2014 `->` Skylake 2015 `->` Cascade Lake 2019 `->` Ice Lake 2019 `->` Sapphire Rapids 2023

Intel Xeon (Icelake) Platinum 8360Y: https://www.intel.cn/content/www/cn/zh/products/sku/212459/intel-xeon-platinum-8360y-processor-54m-cache-2-40-ghz/specifications.html

通过 `lscpu` 来查询一下cpu信息

```
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   52 bits physical, 57 bits virtual
CPU(s):                          144
On-line CPU(s) list:             0-143
Thread(s) per core:              2
Core(s) per socket:              36
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           106
Model name:                      Intel(R) Xeon(R) Platinum 8360Y CPU @ 2.40GHz
Stepping:                        6
Frequency boost:                 enabled
CPU MHz:                         1847.196
CPU max MHz:                     2401.0000
CPU min MHz:                     800.0000
BogoMIPS:                        4800.00
Virtualization:                  VT-x
L1d cache:                       3.4 MiB
L1i cache:                       2.3 MiB
L2 cache:                        90 MiB
L3 cache:                        108 MiB
NUMA node0 CPU(s):               0-35,72-107
NUMA node1 CPU(s):               36-71,108-143
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable
Vulnerability Retbleed:          Not affected
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_ts
                                 c cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_f
                                 ault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx
                                 512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_
                                 epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities
```

### AMD Milan-X

Zen架构代表了AMD Ryzen 霄龙系列处理器的微架构：AMD Epyc是AMD推出的x86架构服务器微处理器产品线，采用Zen微架构。于2017年6月发表并开始供货，取代推出已有14年历史的Opteron系列。

第一代霄龙处理器 Naples 基于Zen微架构 `->` 第二代霄龙处理器 Rome 基于Zen2微架构 `->` 第三代霄龙处理器 Milan 基于Zen3微架构 `->` 第四代霄龙处理器 Genoa, Bergamo and Siena 基于Zen4微架构

EPYC 7773X: https://www.amd.com/zh-hans/products/cpu/amd-epyc-7773x

### Marvell ThunderX2

Marvell Technology Group 是一家总部位于美国加利福尼亚州圣克拉拉的半导体公司。Marvell成立于1995年，是一家全球领先的半导体解决方案提供商，专注于开发和制造高性能处理器、控制器、存储解决方案、网络连接性产品和其他半导体设备。

Marvell ThunderX 是Marvell公司的一个服务器处理器系列，专为数据中心和云计算市场设计。ThunderX系列处理器采用ARM架构，以提供高性能、能效和多核处理能力，以满足大规模服务器和高性能计算需求。

ThunderX2 CN9980采用AArch64 ISA和Vulcan 微架构。

### Fujitsu A64FX

Fujitsu A64FX是一款高性能、面向科学计算和高性能计算的ARM架构处理器

### Nvidia V100

### AMD MI-100

## *软件*

### 编译器

GCC在每一种CPU系统上都有

介绍一下其他的编译器

* Intel oneAPI C/C++ Compiler (only on ice and milan)

  命令为 `icx/icpx`，可以使用 `man icc` 来查询手册

  Icx是由Intel开发的C++编译器。它是Intel工具套件中的一部分，通常与Intel的开发工具和库一起使用，用于编译和优化C++代码

* Cray C/C++ Compiler

  命令为 `cc/CC`，可以使用 `man craycc/man crayCC` 来查询手册

### gcc 优化等级

<img src="gcc优化等级.png" width="50%">

1. **-O0**：没有优化。此等级生成最简单、最容易调试的代码，但性能通常较低。
2. **-O1**：启用基本的优化。这一级别启用一些简单的优化，如函数内联和一些代码移动。它可以提高性能，同时保留了较好的调试能力。
3. **-O2**：启用更多的优化。此等级会应用更多的代码转换，包括循环展开和更强大的优化。这通常会提高代码的性能，但会增加编译时间。
4. **-O3**：启用高级优化。它启用了大多数常用的优化，包括函数内联、循环展开、自动矢量化等。这可以显著提高生成的代码的性能，但仍然保留了对浮点精度和符号运算的一定程度的保守性。因此，它适用于大多数情况下，可以在不牺牲太多数值精度的情况下提高性能。
5. **-Ofast**：极高级别的优化。此等级启用了所有常见的优化，同时允许牺牲一些数值精度以提高性能。它适用于那些对数值精度要求不高的高性能应用。
6. **-Os**：优化代码大小。这一级别旨在减小生成的可执行文件的大小，而不是提高性能。它会删除一些不必要的代码和数据，适合于资源有限的环境。
7. **-Og**：适用于调试的优化。这一级别会进行一些优化，同时保留了较好的调试能力。它是为了在调试期间获得较好的性能和调试能力的平衡。
8. **-O**：默认优化等级。这一级别通常等同于 `-O1` 或 `-O2`，具体取决于编译器版本和配置。

一些参数的意义如下

* `-flto`

  在GCC编译器中，选项 `-flto` 表示 "Link Time Optimization"，即链接时优化。它是一种编译器优化技术，它将编译阶段的优化延伸到链接阶段，以进一步提高生成的可执行文件的性能。

  使用 `-flto` 选项，编译器将在编译时生成中间表示（IR），然后将这些中间表示保存在目标文件中。在链接时，编译器会再次优化这些中间表示，并生成最终的可执行文件。这使得编译器能够进行全局的优化，跨足够多的源文件，从而产生更高效的代码。

* `march=native`

  `march=native` 是GCC编译器的一个选项，用于优化生成的机器代码以最大限度地利用当前主机的CPU架构。这个选项告诉编译器使用当前主机的本机（native）CPU架构，以便生成特定于该CPU的指令集的代码。

  使用 `-march=native` 时，GCC会检测当前主机的CPU架构，并根据检测结果生成与该架构最兼容的机器代码。这可以提高程序的性能，因为生成的代码会更好地利用当前CPU的特性和指令集扩展。

  注意：使用 `-march=native` 选项可能会导致生成的代码在其他CPU架构上不兼容，因为它会针对当前主机的CPU进行优化。因此如果计划在多个不同CPU架构的计算机上运行相同的二进制程序，应谨慎使用这个选项。

  这个选项通常用于在特定主机上编译和运行程序，以获得最佳性能。**如果要生成可移植的代码，不建议使用 `-march=native`**，而应选择适当的目标架构标志，例如 **`-march=core2`**、**`-march=corei7`** 等。这将生成适用于特定CPU架构的代码，而不仅仅是当前主机的本机架构。

### Modules Environment

Module environment 是一个专门管理环境变量的工具，一般应用于软件或运行库等设备有多个版本，且需要分别配置这些环境变量

```shell
$ module list                  # list loaded modules
$ module avail                 # list all compilers / packages available
$ module load <package-name>   # load a module
$ module unload <package-name> # unload a module
$ module switch <package-name> # switch module ( unload and load )
```

# SLURM

## *任务调度工具*

### 什么是任务调度工具

任务调度工具 Job Scheduling Tool 是用于在HPC集群或分布式计算环境中管理和调度作业的软件工具。它们用于有效地分配计算资源，如处理器、内存和存储，以满足不同用户或应用程序的需求。任务调度工具的主要目标是提高计算资源的利用率，减少等待时间，确保公平的资源分配，以及简化管理和监控任务的执行。

以下是一些常见的HPC任务调度工具：

* Simple Linux Utility for Resource Management, SLURM 极简Linux资源管理工具：SLURM是一种广泛使用的任务调度工具，特别用于Linux环境。它支持作业管理、资源分配、节点管理、负载平衡等功能，并被许多HPC集群采用
* Torque/Maui：Torque是一个开源的批处理系统，与Maui调度器一起使用。它提供了作业管理和节点管理功能，而Maui负责资源分配和作业调度
* Grid Engine：Grid Engine是一种用于管理和调度计算任务的开源工具。它支持作业队列管理、资源分配、负载平衡和可视化监控
* PBS, Portable Batch System：PBS是一种任务调度工具的通用标准，有多个实现，包括OpenPBS、PBS Professional等。它支持各种资源管理和作业调度功能
* LSF, Load Sharing Facility：LSF是IBM开发的任务调度工具，用于大型HPC集群和超算。它支持复杂的作业调度和资源管理需求
* Moab：Moab是Maui调度器的商业版本，提供高级的资源管理和调度功能，特别适用于大规模HPC环境
* Slurm-Workload Manager：Slurm是一个开源的HPC任务调度工具，与SLURM配套使用。它提供了资源管理、作业调度和监控功能

### SLURM简介

https://slurm.schedmd.com/overview.html

BEAST中使用的是SLURM

> **slurm 任务调度工具**（前身为**极简Linux资源管理工具**，英文：**Simple Linux Utility for Resource Management**，取首字母，简写为**SLURM**），或 **Slurm**，是一个用于 Linux 和 Unix 内核系统的[自由开源](https://zh.wikipedia.org/wiki/自由开源)的任务调度工具，被世界范围内的[超级计算机](https://zh.wikipedia.org/wiki/超级计算机)和[计算机群](https://zh.wikipedia.org/wiki/计算机集群)广泛采用。它提供了三个关键功能。第一，为用户分配一定时间的专享或非专享的资源(计算机节点)，以供用户执行工作。第二，它提供了一个框架，用于启动、执行、监测在节点上运行着的任务(通常是并行的任务，例如 [MPI](https://zh.wikipedia.org/wiki/訊息傳遞介面))，第三，为任务队列合理地分配资源。
>
> 大约60％的[500强](https://zh.wikipedia.org/wiki/TOP500)超级计算机上都运行着Slurm，包括2016年前世界上最快的计算机[天河-2](https://zh.wikipedia.org/wiki/天河二号)。
>
> Slurm使用基于Hilbert曲线调度或肥胖 网络拓扑结构的[最适算法](https://zh.wikipedia.org/wiki/曲線擬合)，以便优化并行计算机中的任务分配。-- wikipedia

Slurm 是一个开源、容错性强、高度可扩展的集群管理和作业调度系统，适用于大型和小型 Linux 集群。Slurm 在运行时不需要对内核进行修改，而且相对自包含。作为一个集群工作负载管理器，Slurm 具有三个关键功能

* 它为用户分配对资源（计算节点）的独占和/或非独占访问权限，以供一段时间内执行工作
* 它提供了一个启动、执行和监控工作（通常是并行作业）的框架，这些工作在分配的节点集上执行
* 它通过管理待处理工作队列来仲裁资源的争用。可以使用可选插件进行会计、高级预约、团体调度（并行作业的时间共享）、后台调度、拓扑优化的资源选择、按用户或银行账户的资源限制以及复杂的多因素作业优先级算法

## *SLURM的结构*

### SLURM架构

https://blog.csdn.net/qq_31910613/article/details/102916309

<img src="SLURM架构.png">

* 控制器守护进程 controller daemons
  * 集中式管理器 slurmctld是SLURM资源管理系统的中枢服务，它用于监视资源和工作
    * 在发生故障的情况下，可能还有一个可选的备份管理器 secondary slurmctld 来承担这些职责
    * 每个计算服务器（节点）都有一个 slurmd 守护进程，可以类比为远程 shell：它等待工作、执行工作、返回状态，然后等待更多工作
    * slurmd 守护进程提供了容错性强的分层通信
  * 可选的但是推荐运行的 slurmdbd（Slurm 数据库守护进程）：可用于记录单个数据库中管理的多个 Slurm 集群的记账信息
  * 可选的 slurmrestd（Slurm REST API 守护进程），可用于通过其 REST API 与 Slurm 进行交互。
* 用户命令：下面会具体介绍命令
* 节点监控进程 slurmd
  * slurmd 运行在每个计算节点上，负责收集节点上的资源状态并向控制进程报告。slurmd 接收来自控制进程与用户命令的请求，进行作业步任务加载、作业取消等操作
  * slurmstepd 作业管理进程是由slurmd在加载作业步任务或批处理作业时派生出来的。slurmstepd进程管理本节点上一个作业步的所有任务，负责计算任务启动、标准IO转发、信号传递、记账信息收集以及并行环境初始化等功能

### SLURM对Job的划分

<img src="SLURM实体.png">

上图是由 Slurm 守护程序管理的实体以及对它们的划分

* 节点 nodes：Slurm 中的计算资源，就是整个上图
* 分区 partitions：用于将节点分组成逻辑集合。可以将分区视为作业队列，每个队列具有各种约束条件，如作业大小限制、作业时间限制、允许使用的用户等
* 作业 jobs：分配给用户的资源，以供在指定时间内使用。作业按优先级顺序分配节点，直到该分区内的资源（节点、处理器、内存等）耗尽。一旦作业分配了一组节点，用户就能够以分配内的任何配置形式启动并行工作
* 作业步 job steps：它们是作业内的一组（可能是并行的）任务

可以启动一个单一作业步骤，利用分配给作业的所有节点，或者可以独立使用分配的一部分资源启动多个作业步骤。Slurm 提供了对分配给作业的处理器资源的资源管理，以便可以同时提交多个作业步骤并排队，直到作业的分配内有可用资源为止

## *SLURM的使用*

### 三种提交模式

https://www.bkunyun.com/help/docs/cloudE18

* `srun`

  ```shell
  srun [OPTIONS...] executable [args...]
  srun -p gpu -w g[1100-1101] -N 2 -n 80 -t 20 A.exe # 示例
  ```

  srun 属于交互式提交作业，有屏幕输出，但容易收到网络波动影响，断网或关闭窗口会导致作业中断。一般仅在调试程序时使用此方式提交作业。下面是一些参数的意义

  * -N 3：指定节点数为3
  * -n 12：指定进程数为12
  * -c 12：指定每个进程（任务）使用的CPU核 为12，一般运行OpenMP等多线程程序时需，普通MPI程序不需要指定
  * --gres=gpu:4：指定每台机器使用4张GPU卡
  * -p gpu：指定提交作业到GPU队列
  * -w g[100-101]：指定提交作业到 g100、g101 节点
  * -x g[100, 106]：排除 g100、g106 节点
  * --o out.log：指定标准输出到 out.log 文件
  * -e err.log：指定重定向错误输出到 err.log 文件
  * -J JOBNAME：指定作业名为 JOBNAME
  * -t 20：限制运行 20 分钟

* `sbatch`

  Slurm 支持利用 sbatch 命令采用批处理方式运行作业，sbatch 命令在脚本正确传递给作业调度系统后立即退出，同时获取到一个作业号。作业等所需资源满足后开始运行

* `salloc`

  salloc 命令用于申请节点资源

### 其他命令

* 查看状态
  * sinfo 用于报告系统状态
  * sacct 用于获取有关正在运行或已完成的作业和作业步骤的信息
  * sview 命令以图形方式报告系统和作业状态，包括网络拓扑
  * squeue 用于报告作业状态
* 作业控制
  * srun、salloc、sbatch
  * scancel 用于终止排队或运行中的作业
* scontrol 管理工具，可用于监视和/或修改集群的配置和状态信息
* sacctmgr 管理数据库的管理工具。它可用于识别集群、有效用户、有效银行账户等。对于所有功能都提供了 API

## *LRZ集群的SLURM节点*

### 节点分类

### 在集群上运行并行程序

# Assignment 1

## *Vector Triad Microbenchmark*

```c++
#include <assert.h>
#include <errno.h>
#include <omp.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

// pointers for vectors
static double *restrict A, *restrict B, *restrict C, *restrict D;

bool verify(int N)
{
    for (int i = 0; i < N; ++i) {
        if (A[i] != 7.0)
            return false;
    }
    return true;
}

double triad(int N, int REP)
{
    double begin = omp_get_wtime();
#pragma omp parallel
    {
        for (int r = 0; r < REP; ++r)
#pragma omp for schedule(static) nowait
            for (int i = 0; i < N; ++i)
                A[i] = B[i] + C[i] * D[i];
    }
    return omp_get_wtime() - begin;
}

void initialize_vectors(int N)
{
#pragma omp parallel for schedule(static)
    for (int i = 0; i < N; ++i) {
        A[i] = 0.0;
        B[i] = 1.0;
        C[i] = 2.0;
        D[i] = 3.0;
    }
}

void allocate_vectors(size_t N, bool aligned)
{
    size_t CACHE_LINESIZE = sysconf(_SC_LEVEL1_DCACHE_LINESIZE);

    if (aligned) {
        A = aligned_alloc(CACHE_LINESIZE, N * sizeof(double));
        B = aligned_alloc(CACHE_LINESIZE, N * sizeof(double));
        C = aligned_alloc(CACHE_LINESIZE, N * sizeof(double));
        D = aligned_alloc(CACHE_LINESIZE, N * sizeof(double));
    } else {
        A = malloc(N * sizeof(double));
        B = malloc(N * sizeof(double));
        C = malloc(N * sizeof(double));
        D = malloc(N * sizeof(double));
    }

    if(!A || !B || !C || !D) {
        perror("malloc");
        exit(EXIT_FAILURE);
    }
}

void free_vectors(void)
{
    free(A);
    free(B);
    free(C);
    free(D);
}

int main(int argc, char *argv[])
{
    size_t N_max;
    size_t N_min = 128;

    // min. measurement time in seconds
    double min_duration = 1.0;
    bool   aligned      = true;

    if (argc < 2 || argc > 3) {
        fprintf(stderr, "Usage: %s <N> [aligned]\n", argv[0]);
        fprintf(stderr, "    N         : maximum vector length (2^N_max)\n");
        fprintf(stderr, "    aligned=0 : use unaligned memory allocation\n");
        exit(EXIT_FAILURE);
    }

    char *pEnd;
    N_max = 1ULL << strtol(argv[1], &pEnd, 10);
    if (errno || N_max > __INT_MAX__ || N_max < N_min) {
        fprintf(stderr, "N invalid / out of range\n");
        exit(EXIT_FAILURE);
    }

    if (argc == 3) {
        aligned = atoi(argv[2]);
    }

    fprintf(stderr, "N_max = %zu %s\n", N_max, aligned ? "" : "(unaligned memory)");

    printf("%s,%s,%s,%s\n", "N", "GFLOPS", "Repetitions", "Threads");

    int num_threads = omp_get_max_threads();

    for (size_t N = N_min; N <= N_max; N *= 2) {
        allocate_vectors(N, aligned);
        initialize_vectors(N);

        size_t rep = 1;
        // warm-up and set number of repetitions
        while (triad(N, rep) < min_duration / 2) {
            rep *= 2;
        }

        // actual measurement
        double time = triad(N, rep);

        assert(verify(N));
        free_vectors();

        // flop calculation
        double num_flops = 2.0 * N * rep;
        double gflops    = (num_flops / time) * 1.0e-9;

        printf("%zu,%.2f,%zu,%d\n", N, gflops, rep, num_threads);
    }
    return EXIT_SUCCESS;
}
```

### REP 的作用

* **重复执行以增加准确性**：在性能测试中，通常希望测量的操作能持续足够长的时间，以便能够准确地计算出执行时间。如果操作执行得太快，计时器的精度可能不足以提供准确的读数。通过重复执行相同的操作（在这个案例中是数组操作 `A[i] = B[i] + C[i] * D[i]`），可以延长总的执行时间，从而获得更精确的测量
* **动态确定重复次数**：在 main 函数中，有一段代码专门用来确定合适的 REP 值。代码的逻辑是这样的：首先以一个很小的重复次数开始，然后不断增加重复次数，直到 `triad` 函数执行的时间达到或超过预设的最小持续时间的一半（这里是 `min_duration / 2`）。这样做的目的是为了确保在进行实际的性能测量时，每次测量都有足够长的执行时间，从而提高准确性

### 计算 GFLOPS & Bandwidth

假设triad用到的数组数据都是double，一次数组操作 `A[i] = B[i] + C[i] * D[i]` 总共需要2次浮点数操作和4次内存访问，即 `16 Bytes/flop`

当然 `16 Bytes/flop` 只是平均值，可能会因为read miss、特殊的instruction而产生变化

所以计算bandwidth就是 `#FLOPS * 16 Bytes/flop`

## *研究一下parallel construct*

run命令：`srun OMP_NUM_THREADS=64 OMP_PROC_BIND=spread ./triad-ice1 26 1`

`#pragma omp parallel` 的作用是创建并行区域，把同样的代码分配给不同的线程

而 `#pragma omp for` 的作用则是拆分一个for循环，把里面不同的iterations分配给不同的线程（若并行化了）

### 原版

```c++
double triad(int N, int REP)
{
    double begin = omp_get_wtime();
#pragma omp parallel
    {
        for (int r = 0; r < REP; ++r)
#pragma omp for schedule(static) nowait
            for (int i = 0; i < N; ++i)
                A[i] = B[i] + C[i] * D[i];
    }
    return omp_get_wtime() - begin;
}
```

为什么第二个for后面不用加 {}？

`for (int r = 0; r < REP; ++r)` 后面紧跟着的是 `#pragma omp for schedule(static) nowait` 和内层的 `for` 循环。由于 `#pragma` 指令和内层的 `for` 循环被视为一体，因此外层 `for` 循环实际上只包含了这个复合语句。因此，从语法上来说，这种情况下外层 `for` 循环也不需要大括号。

然而，这样的代码可能会使阅读和理解变得有些困难，特别是对于不熟悉OpenMP指令的人来说。为了提高代码的清晰度，最好还是通过添加大括号明确地表示出哪些代码属于外层循环

```csv
N,        GFLOPS,  Repetitions,  Threads
128,      15.85,   33554432,     64
256,      141.42,  268435456,    64
512,      323.92,  268435456,    64
1024,     497.15,  134217728,    64
2048,     531.48,  67108864,     64
4096,     624.71,  67108864,     64
8192,     830.01,  33554432,     64
16384,    894.73,  16777216,     64
32768,    888.11,  8388608,      64
65536,    828.92,  4194304,      64
131072,   401.26,  1048576,      64
262144,   371.55,  524288,       64
524288,   374.24,  262144,       64
1048576,  371.17,  131072,       64
2097152,  281.56,  65536,        64
4194304,  40.70,   4096,         64
8388608,  19.09,   1024,         64
16777216, 16.02,   256,          64
33554432, 9.16,    128,          64
67108864, 7.98,    32,           64
```

### 同时并行化外部和内部循环

```c++
double triad(int N, int REP)
{
    double begin = omp_get_wtime();
#pragma omp parallel for schedule(static)
        for (int r = 0; r < REP; ++r) {
            for (int i = 0; i < N; ++i)
                A[i] = B[i] + C[i] * D[i];
        }
    return omp_get_wtime() - begin;
}
```

为什么不可以加 {} 和 nowait？

```csv
N,        GFLOPS,  Repetitions,  Threads
128,      1.90,    4194304,      64
256,      2.69,    4194304,      64
512,      2.94,    2097152,      64
1024,     3.09,    1048576,      64
2048,     4.13,    524288,       64
4096,     1.98,    524288,       64
8192,     4.65,    262144,       64
16384,    4.64,    131072,       64
32768,    4.89,    65536,        64
65536,    5.38,    32768,        64
131072,   6.00,    16384,        64
262144,   7.29,    8192,         64
524288,   8.32,    4096,         64
1048576,  10.00,   4096,         64
2097152,  11.59,   2048,         64
4194304,  18.78,   2048,         64
8388608,  26.72,   1024,         64
16777216, 21.14,   512,          64
33554432, 27.20,   256,          64
67108864, 26.51,   128,          64
```

### f

```c++
double triad(int N, int REP)
{
    double begin = omp_get_wtime();
#pragma omp parallel
{
        for (int r = 0; r < REP; ++r) {
            for (int i = 0; i < N; ++i)
                A[i] = B[i] + C[i] * D[i];
        }
}
    return omp_get_wtime() - begin;
}
```





```c++
double triad(int N, int REP)
{
    double begin = omp_get_wtime();
    for (int r = 0; r < REP; ++r)
#pragma omp parallel for schedule(static) 
        for (int i = 0; i < N; ++i)
            A[i] = B[i] + C[i] * D[i];
    return omp_get_wtime() - begin;
}
```

```csv
N,        GFLOPS,  Repetitions,  Threads
128,      0.02,    65536,        64
256,      0.04,    65536,        64
512,      0.08,    65536,        64
1024,     0.16,    65536,        64
2048,     0.32,    65536,        64
4096,     0.65,    65536,        64
8192,     1.30,    65536,        64
16384,    2.60,    65536,        64
32768,    5.17,    65536,        64
65536,    10.33,   65536,        64
131072,   20.35,   65536,        64
262144,   39.16,   65536,        64
524288,   71.85,   65536,        64
1048576,  124.87,  32768,        64
2097152,  189.80,  32768,        64
4194304,  36.94,   4096,         64
8388608,  19.31,   1024,         64
16777216, 15.35,   256,          64
33554432, 14.05,   128,          64
67108864, 14.18,   64,           64
```



```c++
double triad(int N, int REP)
{
    double begin = omp_get_wtime();
    for (int r = 0; r < REP; ++r)
#pragma omp parallel
{
        for (int i = 0; i < N; ++i)
            A[i] = B[i] + C[i] * D[i];
}
    return omp_get_wtime() - begin;
}
```





## *Experiments and Measurements*

### Sequential Performance

### Parallel Performance





Cycling time：什么时候可以进行下一次的寻址



Read的延迟很大、电荷会泄漏，需要不断刷新



50% miss rate -> filter



L1和L2的管理算法是不同的



可以只利用time locality或只利用spatial locality，或者两者都利用



Tag 是一种Metadata

是并行还是顺序的获取Tag和Data



# Assignment 2

### GPU监控

Check GPU utilization `nvidia-smi -l 1` each second update 或者 `watch -n 1 nvidia-smi`

* GPU：本机中的GPU编号（有多块显卡的时候，从0开始编号）图上GPU的编号是：0
* Fan：风扇转速（0%-100%），N/A表示没有风扇
* Name：GPU类型，图上GPU的类型是：Tesla T4
* Temp：GPU的温度（GPU温度过高会导致GPU的频率下降）
* Perf：GPU的性能状态，从P0（最大性能）到P12（最小性能），图上是：P0
* Persistence-M：持续模式的状态，持续模式虽然耗能大，但是在新的GPU应用启动时花费的时间更少，图上显示的是：off
* Pwr：Usager/Cap：能耗表示，Usage：用了多少，Cap总共多少
* Bus-Id：GPU总线相关显示，domain：bus：device.function
* Disp.A：Display Active ，表示GPU的显示是否初始化
* Memory-Usage：显存使用率
* Volatile GPU-Util：GPU使用率
* Uncorr. ECC：关于ECC的东西，是否开启错误检查和纠正技术，0/disabled,1/enabled
* Compute M：计算模式，0/DEFAULT,1/EXCLUSIVE_PROCESS,2/PROHIBITED
* Processes：显示每个进程占用的显存使用率、进程号、占用的哪个GPU 显存占用和GPU占用是两个不一样的东西，显卡是由GPU和显存等组成的，显存和GPU的关系有点类似于内存和CPU的关系。跑caffe代码的时候显存占得少，GPU占得多；跑TensorFlow代码的时候，显存占得多，GPU占得少

# Assignment 3

## *性能分析相关概念*

1. 事件（Events）：事件是在处理器中发生的特定操作或情况，如缓存命中、缓存不命中、分支预测错误、指令执行等。这些事件可以用于分析程序或工作负载的性能特征。
2. **性能计数器（Performance Counters）**：性能计数器是硬件寄存器，用于记录特定事件的数量。每个性能计数器通常与一个特定的事件相关联。通过读取这些计数器的值，在程序运行期间或之后，可以分析性能并评估处理器的行为。
3. **寄存器（Registers）**：在CPU或系统芯片中，有一组特殊寄存器用于存储性能事件和计数器的配置信息以及计数器的当前值。这些寄存器通常由操作系统、性能分析工具或用户空间程序来配置和访问。
4. **性能监测工具（Performance Monitoring Tools）**：性能监测工具是用于配置、启动性能计数器并分析计数器数据的软件工具。这些工具可以提供详细的性能分析信息，帮助开发人员和系统管理员识别性能瓶颈和优化机会。





## *Perf*

http://www.yunweipai.com/43686.html

### Perf event

Perf event 是perf工具的基础，代表了一个特定的性能度量。事件可以是内核、硬件或用户级应用程序产生的。这些事件可以用于监控、统计和剖析各种性能指标

* Hardware Event由Power Management Unit, PMU部件产生，在特定的条件下探测性能事件是否发生以及发生的次数。比如cache命中
* Software Event是内核产生的事件，分布在各个功能模块中，统计和操作系统相关性能事件。比如进程切换，tick数等
* Tracepoint Event是内核中静态tracepoint所触发的事件，这些tracepoint用来判断程序运行期间内核的行为细节，比如slab分配器的分配次数等

### perf record

`perf record` 命令用于收集指定事件的性能数据，并将其保存在文件中以便后续分析。默认数据将保存在名为 perf.data 的文件中

```shell
$ perf record [options] [command]
```

* -e 或 –event：指定要记录的事件类型，例如cache-misses, instructions等
* -p 或 –pid：指定要监控的进程ID
* -t 或 –tid：指定要监控的线程ID
* -a 或 –all-cpus：监控所有CPU，而不仅仅是当前CPU
* -C 或 –cpu：指定要监控的CPU列表
* -f 或 –overwrite：以覆盖模式记录事件
* -c 或 –count：设置每个事件的采样周期
* -r 或 –real-time：设置实时优先级
* -o 或 –output：指定要将数据写入的文件
* -g 或 –call-graph：指定调用图记录方法，例如dwarf或fp（帧指针）
* –switch-events：记录上下文切换事件
* –no-buffering：禁用数据缓冲
* –dry-run：显示要执行的操作，但不实际执行

### perf report

perf report 命令从 perf.data 文件中读取性能数据，并以多种格式展示分析结果。用户可以根据需要自定义报告的输出格式

```shell
$ perf report [options]
```

* -i 或 –input：指定要读取的输入文件，默认为 perf.data
* -F 或 –fields：指定要显示的字段，例如：comm, dso, symbol 等
* –sort：指定排序顺序，例如：dso,symbol 或 symbol,dso
* –show-total-period：显示每个符号的总周期数
* -T 或 –threads：显示线程相关数据
* -m 或 –modules：显示模块（共享库）相关数据
* -k 或 –vmlinux：指定内核符号表文件（vmlinux）的路径
* -f 或 –force：强制解析文件，即使它看起来无效或损坏
* -c 或 –comms：指定要显示的命令（进程）列表
* –dsos：指定要显示的动态共享对象（DSO）列表
* -s 或 –symbols：指定要显示的符号（函数）列表
* –percent-limit：仅显示超过指定百分比的项
* -P 或 –pretty：指定输出格式，如raw、normal等
* –stdio：以文本模式显示报告（而非 TUI 模式）
* –tui：以 TUI 模式显示报告（默认方式）
* –gtk：以 GTK 模式显示报告
* -g 或 –call-graph：显示调用图数据
* –no-children：仅显示独立样本，不显示调用子函数的样本
* –no-demangle：禁用 C++ 符号解析
* –demangle：指定 C++ 符号解析方式，如：no, normal, smart 等
* –filter：指定过滤器，如：–filter ‘dso(/lib*)’
* –max-stack：指定栈帧的最大数量

## *likwid*

https://github.com/RRZE-HPC/likwid/wiki/

likwid, like I knew what I am doing. 是一组命令行性能分析系列工具的集合，用于针对多线程程序的指标分析

* likwid-topology：显示线程和缓存拓扑
* likwid-perfctr：在Intel、ARM、Power处理器上测量硬件性能计数寄存器
* likwid-features：显示并切换Intel Core 2处理器上的硬件预取控制位
* likwid-pin：在不修改代码的情况下为多线程应用程序设置绑定（支持pthread、Intel OpenMP和gcc OpenMP）
* likwid-bench：允许快速原型化线程汇编内核的基准测试框架
* likwid-mpirun：脚本，可简化MPI和MPI/多线程混合应用程序的绑定设置
* likwid-perfscope：likwid-perfctr时间轴模式的前端。允许实时绘制性能指标
* likwid-powermeter：用于访问Intel处理器上的RAPL计数器并查询Turbo模式步骤的工具
* likwid-memsweeper：用于清理ccNUMA内存域并强制从缓存中驱逐脏缓存行的工具
* likwid-setFrequencies：用于设置特定处理器频率的工具

### Performance Group

Performance group 指的是一组计算机中测量用的性能计数寄存器

```
$ likwid-perfctr -a
    Group name	Description
--------------------------------------------------------------------------------
           UPI	UPI data traffic 用于测量处理器之间的数据通信流量，特别是用于UPI的数据流量。
          DATA	Load to store ratio 用于测量加载和存储操作之间的比率，有助于了解内存访问模式。
        ENERGY	Power and Energy consumption 用于测量系统的功耗和能源消耗情况，帮助评估能源效率。
        BRANCH	Branch prediction miss rate/ratio 用于测量分支预测错误的频率，这对于评估分支预测性能很有帮助。
           TMA	Top down cycle allocation 用于测量处理器周期的分配情况，以了解性能瓶颈。
           MEM	Memory bandwidth in MBytes/s 用于测量内存带宽，即从主存读取和写入数据的速度。
        MEM_DP	Overview of arithmetic and main memory performance 提供有关算术运算和主存性能的总体概述。
            L2	L2 cache bandwidth in MBytes/s 用于测量L2缓存的带宽，即L2缓存的读取和写入速度。
  CYCLE_STALLS	Cycle Activities (Stalls) 用于测量处理器周期内不同类型的停顿（stalls）活动。
        MEM_SP	Overview of arithmetic and main memory performance 提供有关算术运算和主存性能的总体概述，针对单精度操作。
            L3	L3 cache bandwidth in MBytes/s 用于测量L3缓存的带宽，即L3缓存的读取和写入速度。
        DIVIDE	Divide unit information 提供有关除法单元性能和信息的计数。
   MEM_FREERUN	Memory bandwidth in MBytes/s 用于测量内存带宽，与之前的 "MEM" 类似。
CYCLE_ACTIVITY	Cycle Activities 用于测量不同类型的处理器周期活动。
     FLOPS_AVX	Packed AVX MFLOP/s 用于测量AVX（Advanced Vector Extensions）指令集的每秒浮点操作（FLOP）。
       L2CACHE	L2 cache miss rate/ratio 用于测量L2缓存的缺失率或比率。
      FLOPS_DP	Double Precision MFLOP/s 用于测量双精度浮点操作的每秒执行次数。
         CLOCK	Power and Energy consumption 用于测量处理器时钟频率、功耗和能源消耗。
      FLOPS_SP	Single Precision MFLOP/s 用于测量单精度浮点操作的每秒执行次数。
```
