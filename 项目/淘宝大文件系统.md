为什么不能存小文件？

* 大规模的小文件存取，磁头需要频繁的训导和换到，因此在读取上容易带来较长的延时

  目录寻址查找Inode Number -> Inode table寻址，获取inode信息 -> 块寻址及读取数据

* 频繁的新增删除操作会导致磁盘碎片，降低磁盘利用率和IO读写效率

* Inode抢占了大量缓存空间，实际降低了真正需要的数据在缓冲中的命中率

设计思路



* 以block文件的形式存放数据文件，一般64M一个block，理论上使用128、256、512MB都行，但是淘宝大文件系统推荐使用64MB。以下简称为“块”，每个块都有唯一的一个整数编号，块在使用之前所用到的存储空间都会预先分配和初始化
* 每一个块由一个索引文件、一个主块文件和若干个扩展块组成，“小文件”主要存放在主块中，扩展块主要用来存放溢出的数据
* 每个索引文件存放对应的块信息和“小文件”索引信息，索引文件会在服务启动时映射（`mmap`）到内存，从而极大的提高文件检索速度。“小文件”索引信息采用在索引文件中的数据结构哈希链表来实现
* 每个文件有对应的文件编号，文件编号从1开始编号，依次递增，同时作为哈希查找算法的Key来定位“小文件”在主块和扩展块中的偏移量。文件编号+块编号按某种算法可得到“小文件”对应的文件名

```c
struct BlockInfo {
  uint32_t block_id_;             //块编号   1 ......2^32-1  TFS = NameServer + DataServer
  int32_t version_;               //块当前版本号
  int32_t file_count_;            //当前已保存文件总数
  int32_t size_;                  //当前已保存文件数据总大小
  int32_t del_file_count_;        //已删除的文件数量
  int32_t del_size_;              //已删除的文件数据总大小
  uint32_t seq_no_;               //下一个可分配的文件编号  1 ...2^64-1    
}
```

`del_file_count_` 和 `del_size_`：删除的时候是不会立刻删除的，因为需要大量IO。当积累已删除文件到一定指标的时候需要在某个时间段进行集中处理





```c
struct RawMeta {
    uint65_t field_;
	struct {
        int32_t inner_offset_;     //文件在块内部的偏移量
        int32_t size_;             //文件大小
    } location_;
};
```





小文件

```c
struct MetaInfo {
     uint64_t fileid_;               //文件编号
     RawMeta raw_meta_;              //文件元数据
     int32_t next_meta_offset_;      //当前哈希链下一个节点在索引文件中的偏移量
} 
```





可重用链表：已经删除的块的metainfo连接起来 